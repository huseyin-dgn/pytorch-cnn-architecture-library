{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ad7d71",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "-----\n",
    "\n",
    "\n",
    "**Batch Renorm şu: BatchNorm’un “küçük batch yüzünden saçmalamasını” azaltmak için çıkan bir yamadır.**\n",
    "\n",
    "\n",
    "## 1) Sorun: BatchNorm küçük batch’te niye bozuluyor?\n",
    "\n",
    "BN eğitimde şunu yapıyor:\n",
    "\n",
    "* Batch’ten ortalama (μ_B) ve varyans (σ_B²) hesaplıyor\n",
    "\n",
    "* Sonra her şeyi ona göre normalize ediyor\n",
    "\n",
    "* Ama batch küçükse (mesela B=2):\n",
    "\n",
    "* μ_B ve σ_B çok gürültülü olur\n",
    "\n",
    "* Her iterasyonda başka başka değerler çıkar\n",
    "\n",
    "Modelin içindeki aktivasyon dağılımı zıplaya zıplaya gider\n",
    "**→ eğitim kararsız, performans düşer**\n",
    "\n",
    "Ayrıca:\n",
    "\n",
    "* Train’de batch stats kullanıyorsun\n",
    "\n",
    "* Eval’da running stats (μ_R, σ_R) kullanıyorsun\n",
    "**→ train–eval farkı büyür**\n",
    "\n",
    ">### Batch Renorm ne yapıyor?\n",
    "\n",
    "Batch Renorm diyor ki:\n",
    "\n",
    "**“Tamam batch istatistiğini kullan ama running istatistiğe göre düzelt.**\n",
    "\n",
    "* Batch çok sapıtırsa da kırbaçla, sınırlı tut (clamp).”\n",
    "\n",
    "Yani eğitimde 2 kaynak var:\n",
    "\n",
    "* Batch stats: μ_B, σ_B (anlık, gürültülü)\n",
    "\n",
    "* Running stats: μ_R, σ_R (birikmiş, daha stabil)\n",
    "\n",
    "Batch Renorm bu ikisini birleştiriyor.\n",
    "\n",
    "\n",
    ">### r ve d dediğimiz şey ne?\n",
    "\n",
    "Batch Renorm iki düzeltme katsayısı hesaplıyor:\n",
    "\n",
    "* r: “ölçek düzeltmesi”\n",
    "\n",
    "* d: “kaydırma düzeltmesi”\n",
    "\n",
    "Sezgi:\n",
    "\n",
    "* Batch’in std’si running’den fazla farklıysa → r ile düzelt\n",
    "\n",
    "* Batch’in mean’i running’den fazla farklıysa → d ile düzelt\n",
    "\n",
    "Ve bunları clamp ediyor:\n",
    "\n",
    "* r çok uçmasın\n",
    "\n",
    "* d çok uçmasın\n",
    "\n",
    "**Batch Renorm = BatchNorm + (running istatistiklere göre ölçek/kaydırma düzeltmesi) + clamp**\n",
    "\n",
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128470ce",
   "metadata": {},
   "source": [
    "\n",
    "# Batch Renormalization (Batch Renorm) — Baştan Sona (Tek GPU / Küçük Batch Odaklı)\n",
    "\n",
    "Bu notebook, **Batch Renormalization** konusunu **temelden ileri seviyeye** kadar anlatır ve PyTorch’ta kullanılabilir bir implementasyon verir.\n",
    "\n",
    "- BatchNorm’un küçük batch’te neden zorlandığı  \n",
    "- Batch Renorm’un fikri (**r**, **d** düzeltmeleri)  \n",
    "- Train/Eval uyumsuzluğunu nasıl azalttığı  \n",
    "- Hangi senaryolarda mantıklı / mantıksız  \n",
    "- PyTorch’ta **BatchRenorm2d** implementasyonu (drop‑in replacement)  \n",
    "- Pratik checklist ve tipik hatalar  \n",
    "- Mini demo: küçük batch’te BN vs BatchRenorm sezgisi  \n",
    "\n",
    "> Not: Bu dosya tek GPU / tek makine kullanımına uygundur. SyncBN gibi “multi‑GPU şart” değildir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d28ce",
   "metadata": {},
   "source": [
    "\n",
    "## 1) BatchNorm’u hatırlayalım (problem nerede başlıyor?)\n",
    "\n",
    "Conv için **BatchNorm2d** genelde kanallar üzerinde çalışır ve istatistikleri **(B, H, W)** eksenlerinde toplar:\n",
    "\n",
    "- Batch mean:  \\(\\mu_B\\)  \n",
    "- Batch var:   \\(\\sigma_B^2\\)\n",
    "\n",
    "Normalize:\n",
    "\\[\n",
    "\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "\\]\n",
    "Affine:\n",
    "\\[\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "\\]\n",
    "\n",
    "### Train vs Eval farkı\n",
    "- **Train:** \\(\\mu_B, \\sigma_B\\) mini‑batch’ten gelir.\n",
    "- **Eval:** “running” istatistikler kullanılır: \\(\\mu_R, \\sigma_R\\)\n",
    "\n",
    "### BN’in temel zayıflığı\n",
    "Per‑batch istatistikleri güvenilir değilse (batch küçükse):\n",
    "- \\(\\mu_B, \\sigma_B\\) gürültülü olur\n",
    "- Train sırasında normalize edilen dağılım “zıplar”\n",
    "- Eval’da running istatistikle farklı normalize edilir ⇒ **train‑eval mismatch** artar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a9fa8",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Batch Renormalization nedir? (ana fikir)\n",
    "\n",
    "Batch Renorm (Ioffe, 2017) şunu hedefler:\n",
    "\n",
    "> “Train sırasında batch istatistikleri ile running istatistikleri arasında **kontrollü bir köprü** kur.”\n",
    "\n",
    "Yani train sırasında tamamen \\(\\mu_B, \\sigma_B\\) ile gitmek yerine,\n",
    "running (\\(\\mu_R, \\sigma_R\\)) istatistiklerini referans alır ve batch’e bir düzeltme uygular.\n",
    "\n",
    "### 2.1. r ve d düzeltmeleri\n",
    "\\[\n",
    "r = \\frac{\\sigma_B}{\\sigma_R}\n",
    "\\]\n",
    "\\[\n",
    "d = \\frac{\\mu_B - \\mu_R}{\\sigma_R}\n",
    "\\]\n",
    "\n",
    "Sonra normalize edilmiş aktivasyonu şu şekilde düzeltir:\n",
    "\\[\n",
    "\\hat{x}_{BR} = r \\cdot \\hat{x}_{BN} + d\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "- \\(\\hat{x}_{BN}\\) = normal BN ile normalize edilmiş değer\n",
    "- r, d = batch istatistiğini running istatistiğe “yaklaştıran” düzeltmeler\n",
    "\n",
    "### 2.2. Neden clamp var?\n",
    "r ve d sınırsız olursa küçük batch’te düzeltme patlayabilir. Bu yüzden:\n",
    "\n",
    "\\[\n",
    "r \\in [1/r_{max}, r_{max}], \\quad d \\in [-d_{max}, d_{max}]\n",
    "\\]\n",
    "\n",
    "rmax ve dmax genelde **eğitim boyunca yavaş yavaş artırılır** (schedule).\n",
    "Başta sistem daha “BN gibi” davranır, zamanla renorm etkisi artar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1ed7b",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Batch Renorm neyi düzeltir?\n",
    "\n",
    "### 3.1. Küçük batch kararsızlığı\n",
    "Batch küçükken BN’in istatistiği gürültülüdür.\n",
    "Batch Renorm, gürültüyü tamamen yok etmez ama running istatistikleri referans aldığı için\n",
    "normalize edilen dağılımı daha “tutarlı” hale getirir.\n",
    "\n",
    "### 3.2. Train‑Eval mismatch\n",
    "BN’de train normalize = batch stats, eval normalize = running stats.\n",
    "Küçük batch’te bu fark büyür. Batch Renorm, train sırasında da running’e bağlı bir düzeltme kullandığı için\n",
    "train ve eval davranışını birbirine yaklaştırır.\n",
    "\n",
    "### 3.3. “BN’yi çöpe atma” değil\n",
    "Batch Renorm hâlâ batch istatistiğini kullanır ama onu kontrollü biçimde running’e bağlar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f9b8b",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Ne zaman kullanmalı? (karar çerçevesi)\n",
    "\n",
    "### Kullan ✅\n",
    "- Tek GPU / küçük batch (1–8 gibi)\n",
    "- CNN tabanlı model (Conv ağırlıklı)\n",
    "- BN’den vazgeçmek istemiyorsun ama GN’ye geçmek de istemiyorsun\n",
    "- Finetune’da BN kararsızlığı yaşıyorsun\n",
    "\n",
    "### Kullanma / dikkatli ol ⚠️\n",
    "- Batch zaten büyükse: çoğu zaman gereksiz\n",
    "- Çok kısa eğitim: rmax/dmax schedule’ı oturmadan eğitim biter\n",
    "\n",
    "### Alternatifler\n",
    "- **GroupNorm:** batch bağımsız, çok stabil\n",
    "- **BN freeze:** pretrained backbone finetune’da sık kullanılır\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d40f7",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Batch Renorm parametreleri (pratik)\n",
    "\n",
    "Tipik parametreler:\n",
    "- `eps`: sayısal stabilite\n",
    "- `momentum`: running stats güncellemesi\n",
    "- `rmax`, `dmax`: clamp limitleri\n",
    "- `warmup_steps` veya schedule: rmax/dmax zamanla artar\n",
    "\n",
    "Örnek schedule fikri:\n",
    "- Başlangıç: rmax=1.0, dmax=0.0  (tam BN gibi)\n",
    "- Zamanla: rmax → 3.0, dmax → 5.0 gibi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e3e77",
   "metadata": {},
   "source": [
    "\n",
    "## 6) PyTorch’ta BatchRenorm2d implementasyonu\n",
    "\n",
    "PyTorch’un core `nn` içinde BatchRenorm yok. O yüzden “drop‑in replacement” bir modül yazacağız.\n",
    "\n",
    "Hedef:\n",
    "- `nn.BatchNorm2d` yerine kullanılabilsin\n",
    "- `train()` modunda batch + running stats üzerinden r,d hesaplasın\n",
    "- `eval()` modunda normal BN gibi running stats kullansın\n",
    "- Running mean/var’ı momentum ile güncellesin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e01a038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16, 16])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BatchRenorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-5,\n",
    "        momentum: float = 0.1,\n",
    "        affine: bool = True,\n",
    "        track_running_stats: bool = True,\n",
    "        rmax: float = 3.0,\n",
    "        dmax: float = 5.0,\n",
    "        warmup_steps: int = 5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        self.rmax_target = float(rmax)\n",
    "        self.dmax_target = float(dmax)\n",
    "        self.warmup_steps = int(warmup_steps)\n",
    "\n",
    "        if self.affine:\n",
    "            self.weight = nn.Parameter(torch.ones(num_features))\n",
    "            self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var', torch.ones(num_features))\n",
    "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "        else:\n",
    "            self.register_buffer('running_mean', None)\n",
    "            self.register_buffer('running_var', None)\n",
    "            self.register_buffer('num_batches_tracked', None)\n",
    "\n",
    "    def _current_clamp(self):\n",
    "        if not self.track_running_stats:\n",
    "            return self.rmax_target, self.dmax_target\n",
    "\n",
    "        t = int(self.num_batches_tracked.item())\n",
    "        alpha = 1.0 if self.warmup_steps <= 0 else min(1.0, t / self.warmup_steps)\n",
    "\n",
    "        rmax = 1.0 + alpha * (self.rmax_target - 1.0)\n",
    "        dmax = 0.0 + alpha * (self.dmax_target - 0.0)\n",
    "        return rmax, dmax\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() != 4:\n",
    "            raise ValueError('BatchRenorm2d expects input of shape (N, C, H, W)')\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.num_batches_tracked += 1\n",
    "\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=(0, 2, 3))\n",
    "            var = x.var(dim=(0, 2, 3), unbiased=False)\n",
    "            std = torch.sqrt(var + self.eps)\n",
    "\n",
    "            if self.track_running_stats:\n",
    "                r_std = torch.sqrt(self.running_var + self.eps)\n",
    "\n",
    "                r = (std / r_std).detach()\n",
    "                d = ((mean - self.running_mean) / r_std).detach()\n",
    "\n",
    "                rmax, dmax = self._current_clamp()\n",
    "                r = torch.clamp(r, 1.0 / rmax, rmax)\n",
    "                d = torch.clamp(d, -dmax, dmax)\n",
    "            else:\n",
    "                r = 1.0\n",
    "                d = 0.0\n",
    "\n",
    "            x_hat = (x - mean[None, :, None, None]) / std[None, :, None, None]\n",
    "\n",
    "            if self.track_running_stats:\n",
    "                x_hat = x_hat * r[None, :, None, None] + d[None, :, None, None]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "                    self.running_var  = (1 - self.momentum) * self.running_var  + self.momentum * var\n",
    "\n",
    "        else:\n",
    "            if not self.track_running_stats:\n",
    "                raise RuntimeError('Eval mode requires track_running_stats=True')\n",
    "            x_hat = (x - self.running_mean[None, :, None, None]) / torch.sqrt(\n",
    "                self.running_var[None, :, None, None] + self.eps\n",
    "            )\n",
    "\n",
    "        if self.affine:\n",
    "            x_hat = x_hat * self.weight[None, :, None, None] + self.bias[None, :, None, None]\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "m = BatchRenorm2d(8)\n",
    "x = torch.randn(4, 8, 16, 16)\n",
    "y = m(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bd130",
   "metadata": {},
   "source": [
    "\n",
    "### 6.1. r ve d neden `detach()`?\n",
    "r ve d’yi `detach()` yapmanın amacı:\n",
    "- Renorm düzeltmelerinin gradyan yolunu “garipleştirmesini” istememek\n",
    "- Daha stabil optimizasyon\n",
    "\n",
    "Pratikte bu genelde daha güvenli davranır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43d3ca",
   "metadata": {},
   "source": [
    "\n",
    "## 7) BN katmanlarını BatchRenorm’a dönüştürme (model içinde)\n",
    "\n",
    "Elinde BN kullanan bir CNN varsa, modülü dolaşıp `BatchNorm2d` → `BatchRenorm2d` çevirebilirsin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d7df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Önce:\n",
      " MiniCNN(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Sonra:\n",
      " MiniCNN(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchRenorm2d()\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchRenorm2d()\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def convert_bn_to_batchrenorm(module: nn.Module, rmax=3.0, dmax=5.0, warmup_steps=5000):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.BatchNorm2d):\n",
    "            brn = BatchRenorm2d(\n",
    "                num_features=child.num_features,\n",
    "                eps=child.eps,\n",
    "                momentum=child.momentum if child.momentum is not None else 0.1,\n",
    "                affine=child.affine,\n",
    "                track_running_stats=child.track_running_stats,\n",
    "                rmax=rmax,\n",
    "                dmax=dmax,\n",
    "                warmup_steps=warmup_steps,\n",
    "            )\n",
    "\n",
    "            if child.affine:\n",
    "                with torch.no_grad():\n",
    "                    brn.weight.copy_(child.weight)\n",
    "                    brn.bias.copy_(child.bias)\n",
    "\n",
    "            if child.track_running_stats:\n",
    "                with torch.no_grad():\n",
    "                    brn.running_mean.copy_(child.running_mean)\n",
    "                    brn.running_var.copy_(child.running_var)\n",
    "                    brn.num_batches_tracked.copy_(child.num_batches_tracked)\n",
    "\n",
    "            setattr(module, name, brn)\n",
    "        else:\n",
    "            convert_bn_to_batchrenorm(child, rmax=rmax, dmax=dmax, warmup_steps=warmup_steps)\n",
    "    return module\n",
    "\n",
    "class MiniCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MiniCNN()\n",
    "print(\"Önce:\\n\", model)\n",
    "model = convert_bn_to_batchrenorm(model, rmax=3.0, dmax=5.0, warmup_steps=2000)\n",
    "print(\"\\nSonra:\\n\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b5bbd",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Mini demo: Küçük batch’te BN vs BatchRenorm sezgisi\n",
    "\n",
    "Bu demo tam eğitim değil; **küçük batch istatistiğinin oynaklığını** ve\n",
    "Batch Renorm’un running’e bağlanmasının etkisini sezdirir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe00fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN  küçük batch: {'mean': 1.1059455573558807e-09, 'std': 0.9999949932098389, 'min': -4.318045139312744, 'max': 4.028398036956787}\n",
      "BN  büyük batch: {'mean': 1.8044374883174896e-09, 'std': 0.9999949932098389, 'min': -4.584938049316406, 'max': 4.676060676574707}\n",
      "BRN küçük batch: {'mean': -0.011972633190453053, 'std': 1.002089023590088, 'min': -4.343570709228516, 'max': 4.0954813957214355}\n",
      "BRN büyük batch: {'mean': 9.5534254796803e-05, 'std': 1.001241683959961, 'min': -4.5844340324401855, 'max': 4.662753105163574}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "x_small = torch.randn(2, 16, 32, 32)    # küçük batch\n",
    "x_big   = torch.randn(32, 16, 32, 32)   # büyük batch\n",
    "\n",
    "bn = nn.BatchNorm2d(16).train()\n",
    "brn = BatchRenorm2d(16, rmax=3.0, dmax=5.0, warmup_steps=0).train()\n",
    "\n",
    "for _ in range(20):\n",
    "    _ = brn(torch.randn(8, 16, 32, 32))\n",
    "\n",
    "y_bn_small = bn(x_small)\n",
    "y_bn_big   = bn(x_big)\n",
    "\n",
    "y_brn_small = brn(x_small)\n",
    "y_brn_big   = brn(x_big)\n",
    "\n",
    "def summarize(t):\n",
    "    return {\n",
    "        'mean': float(t.mean()),\n",
    "        'std': float(t.std(unbiased=False)),\n",
    "        'min': float(t.min()),\n",
    "        'max': float(t.max()),\n",
    "    }\n",
    "\n",
    "print('BN  küçük batch:', summarize(y_bn_small))\n",
    "print('BN  büyük batch:', summarize(y_bn_big))\n",
    "print('BRN küçük batch:', summarize(y_brn_small))\n",
    "print('BRN büyük batch:', summarize(y_brn_big))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f3841",
   "metadata": {},
   "source": [
    "\n",
    "## 9) İleri seviye pratikler (kısa)\n",
    "\n",
    "- `warmup_steps` ile renorm’u yavaş açmak genelde iyi fikir.\n",
    "- Pretrained finetune’da BN freeze bazen daha iyi olabilir.\n",
    "- Detection/segmentation’da GN popüler; BatchRenorm “BN tabanlı pipeline”ı minimal değiştirmek için iyi seçenek.\n",
    "\n",
    "### Hızlı kullanım reçetesi\n",
    "```python\n",
    "model = convert_bn_to_batchrenorm(model, rmax=3.0, dmax=5.0, warmup_steps=5000)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
