{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "17afa1bc",
      "metadata": {},
      "source": [
        "----\n",
        "-----\n",
        "-----\n",
        "\n",
        "## **Mean = 0, Std = 1 olacak şekilde yeniden ölçeklemek**\n",
        "\n",
        "**LayerNorm, mean ve std’yi minimize eden bir yapı değil; her forward adımında mean=0, std=1 olacak şekilde aktivasyonları yeniden ölçekleyen deterministik bir normalizasyon katmanıdır.**\n",
        "\n",
        "LN:\n",
        "\n",
        "* her örneği kendi içinde normalize eder\n",
        "\n",
        "* batch gürültüsünü yok sayar\n",
        "\n",
        "* gradyan akışını stabilize eder\n",
        "\n",
        "Ama:\n",
        "\n",
        "* “mean’i sıfıra çekmeye çalışıyor” diye bir hedef yok\n",
        "\n",
        "* her forward pass’te zorla sıfır yapıyor\n",
        "\n",
        "-----\n",
        "-----\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4) Normalization Katmanları — Layer Normalization (LayerNorm)\n",
        "\n",
        "Bu defter, **Layer Normalization (LN)** kavramını **sıfırdan ileri seviyeye** taşır:\n",
        "- LN nedir, hangi probleme cevap verir?\n",
        "- Matematiksel tanım (forward) ve pratik nüanslar\n",
        "- CNN (object detection / segmentation) bağlamında nerede mantıklı?\n",
        "- PyTorch implementasyonları: `nn.LayerNorm`, **channels-first (NCHW)** için pratik çözümler\n",
        "- BN/GN/IN ile karşılaştırma (kısa ama net)\n",
        "\n",
        "> Not: Bu defter *uygulama odaklıdır*. Aşırı teori yerine, doğru sezgi + doğru kod + doğru kullanım senaryosu hedeflenir.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 0) Normalization kavramı: Neyi normalize ediyoruz?\n",
        "\n",
        "Bir sinir ağında \"normalization\" genel olarak şu iki şeyi hedefler:\n",
        "\n",
        "1. **Optimizasyonu stabilize etmek**  \n",
        "   Aktivasyonların ölçeği/dağılımı aşırı oynadığında, gradyanlar patlayabilir/sönebilir ve eğitim zorlaşır.\n",
        "\n",
        "2. **Hiperparametre hassasiyetini azaltmak**  \n",
        "   Öğrenme oranı (LR), başlangıç ağırlıkları, batch boyutu gibi değişkenlere toleransı artırır.\n",
        "\n",
        "Normalization ailesi, bunu farklı \"eksenlerde\" yapar:\n",
        "- Batch üzerinde mi? (BN)\n",
        "- Kanal grupları üzerinde mi? (GN)\n",
        "- Örnek başına mı? (IN)\n",
        "- Katman başına mı? (LN)\n",
        "\n",
        "LayerNorm bu ailenin önemli bir üyesidir çünkü **batch boyutuna bağımlılığı yoktur**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1) Layer Normalization (LN) nedir?\n",
        "\n",
        "**Layer Normalization**, bir örnek (sample) içinde, belirli bir katmandaki aktivasyonları normalize eder.\n",
        "\n",
        "En basit haliyle:\n",
        "- Her örnek için ayrı ayrı,\n",
        "- Belirli boyut seti üzerinde (genelde feature dimension),\n",
        "- Ortalama (`μ`) ve standart sapma (`σ`) hesaplanır,\n",
        "- Aktivasyonlar normalize edilir,\n",
        "- Ardından **öğrenilebilir** ölçek (`γ`) ve kaydırma (`β`) uygulanır.\n",
        "\n",
        "Bu sayede ağ, \"normalize edilmiş\" uzayda stabil şekilde öğrenirken,\n",
        "gerekli olduğunda `γ` ve `β` ile tekrar uygun ölçeğe dönebilecek esnekliği korur.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1) Nereden çıkmıştır? Neden ihtiyaç doğdu?\n",
        "\n",
        "Batch Normalization (BN) CNN'lerde çok güçlü olsa da şu iki ana problem öne çıkar:\n",
        "\n",
        "1. **Batch bağımlılığı**  \n",
        "   BN istatistikleri batch üzerinden hesaplar. Batch küçükse (özellikle detection/segmentation), istatistikler gürültülü olur.\n",
        "\n",
        "2. **Training / inference davranışı ayrımı**  \n",
        "   BN'de eğitimde batch istatistiği, inference'ta running mean/var kullanılır.\n",
        "   Dağılım kayarsa veya fine-tune senaryolarında bu karışıklık sorun yaratabilir.\n",
        "\n",
        "LayerNorm, özellikle **RNN/Transformer gibi** sequence modellerinde batch boyutu değişken ve küçük olabildiği için,\n",
        "**batch'e bağımlı olmadan** stabilizasyon sağlamak amacıyla geliştirilmiştir.\n",
        "Daha sonra CNN dünyasında da (özellikle small-batch rejimi) anlamlı kullanım alanları bulmuştur.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2) Matematik (temel): LN forward denklemi\n",
        "\n",
        "Bir örnek için normalize edilen boyutları `S` ile gösterelim (ör: embedding dimension).\n",
        "\n",
        "- Ortalama:\n",
        "\\[\n",
        "\\mu = \\frac{1}{|S|}\\sum_{i\\in S} x_i\n",
        "\\]\n",
        "\n",
        "- Varyans:\n",
        "\\[\n",
        "\\sigma^2 = \\frac{1}{|S|}\\sum_{i\\in S} (x_i-\\mu)^2\n",
        "\\]\n",
        "\n",
        "- Normalize:\n",
        "\\[\n",
        "\\hat{x}_i = \\frac{x_i-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
        "\\]\n",
        "\n",
        "- Öğrenilebilir ölçek/kaydırma:\n",
        "\\[\n",
        "y_i = \\gamma_i\\hat{x}_i + \\beta_i\n",
        "\\]\n",
        "\n",
        "Burada:\n",
        "- `ε` sayısal stabilite içindir (sıfıra bölmeyi önler),\n",
        "- `γ` ve `β` parametreleri şekil olarak normalize edilen boyutlarla uyumludur.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1) LN hangi eksenlerde çalışır?\n",
        "\n",
        "Bu kritik bir noktadır: LN \"layer\" kelimesi yüzünden bazen yanlış anlaşılır.\n",
        "\n",
        "- **MLP / Transformer**: genelde `(..., d_model)` üzerinde normalize edilir → yani **son boyut**.\n",
        "- **CNN**: NCHW formatında (batch, channel, height, width) tipik LN kullanımları:\n",
        "  - **channels-last** (NHWC) + `LayerNorm(C)` (çok yaygın)\n",
        "  - NCHW için özel bir katman ile **kanal ekseninde** normalize etme (ConvNeXt tarzı)\n",
        "\n",
        "PyTorch `nn.LayerNorm` varsayılan olarak **son boyutları** normalize eder.\n",
        "Bu yüzden CNN'de NCHW ile doğrudan kullanmak istediğinde genelde iki seçenek vardır:\n",
        "1. NCHW → NHWC (channels-last) formatına geçip `LayerNorm(C)` uygulamak\n",
        "2. NCHW için özel `LayerNorm2d` yazarak C ekseni üzerinde normalize etmek\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x205ef529d30>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3) PyTorch ile LN: en temel kullanım\n",
        "\n",
        "`nn.LayerNorm(normalized_shape)`:\n",
        "\n",
        "- `normalized_shape`: normalize edilecek son boyut(lar)ın şekli\n",
        "  - Örn: `[d_model]` veya `d_model`\n",
        "  - Örn: `[C, H, W]` gibi çoklu boyutlar da verilebilir (tensor düzeni uygunsa)\n",
        "\n",
        "Önemli: `LayerNorm` **training / inference** arasında aynı davranır.\n",
        "BN'deki gibi running stats ayrımı yoktur.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x mean/std (sample0): -0.03851224109530449 0.546144425868988\n",
            "y mean/std (sample0): 0.0 0.9999833106994629\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2, 5)          # (batch=2, features=5)\n",
        "ln = nn.LayerNorm(5)\n",
        "\n",
        "y = ln(x)\n",
        "print(\"x mean/std (sample0):\", x[0].mean().item(), x[0].std(unbiased=False).item())\n",
        "print(\"y mean/std (sample0):\", y[0].mean().item(), y[0].std(unbiased=False).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beklenen:\n",
        "- Her örnek için (satır başına) ortalama ~0, std ~1 (γ=1, β=0 iken).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4) CNN tarafı: NCHW ile LN nasıl yapılır?\n",
        "\n",
        "NCHW: `(N, C, H, W)`\n",
        "\n",
        "`nn.LayerNorm` ile `(C,H,W)` üzerinde normalize etmek mümkündür ama:\n",
        "- `H,W` sabit olmalı (input boyutu değişirse shape tutmaz),\n",
        "- Detection/segmentation gibi dinamik boyutlarda bu pratik değildir.\n",
        "\n",
        "Bu yüzden CNN'de iki yaygın pratik yaklaşım:\n",
        "1) **Channels-last** (NHWC) kullanıp `LayerNorm(C)` yapmak  \n",
        "2) **C ekseninde** normalize eden bir `LayerNorm2d` yazmak (ConvNeXt benzeri)\n",
        "\n",
        "Aşağıdaki implementasyon NCHW tensor üzerinde kanal ekseninde LN yapar:\n",
        "- Her piksel konumu için C üzerinden normalize eder (per-position).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-position mean (sample0, h0,w0): 0.0\n",
            "Per-position std  (sample0, h0,w0): 0.9999761581420898\n"
          ]
        }
      ],
      "source": [
        "class LayerNorm2d(nn.Module):\n",
        "    def __init__(self, num_channels: int, eps: float = 1e-5, affine: bool = True):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        if affine:\n",
        "            self.gamma = nn.Parameter(torch.ones(num_channels))\n",
        "            self.beta = nn.Parameter(torch.zeros(num_channels))\n",
        "        else:\n",
        "            self.register_parameter('gamma', None)\n",
        "            self.register_parameter('beta', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=1, keepdim=True)  # over C\n",
        "        var = (x - mean).pow(2).mean(dim=1, keepdim=True)\n",
        "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        if self.affine:\n",
        "            return x_hat * self.gamma.view(1, -1, 1, 1) + self.beta.view(1, -1, 1, 1)\n",
        "        return x_hat\n",
        "\n",
        "# hızlı kontrol\n",
        "x_img = torch.randn(2, 3, 4, 4)\n",
        "ln2d = LayerNorm2d(3)\n",
        "y_img = ln2d(x_img)\n",
        "print(\"Per-position mean (sample0, h0,w0):\", y_img[0, :, 0, 0].mean().item())\n",
        "print(\"Per-position std  (sample0, h0,w0):\", y_img[0, :, 0, 0].std(unbiased=False).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5) LN vs BN: detection/segmentation için kritik farklar\n",
        "\n",
        "### Batch bağımlılığı\n",
        "- BN: batch istatistikleri → batch küçükse gürültü\n",
        "- LN: örnek içi istatistik → batch'ten bağımsız\n",
        "\n",
        "### Training/Inference davranışı\n",
        "- BN: train/infer farklı (running stats)\n",
        "- LN: aynı (deterministik)\n",
        "\n",
        "### CNN performansı (pratik gözlem)\n",
        "- Klasik classification CNN (büyük batch): BN çoğu zaman daha iyi/kolay\n",
        "- Detection/segmentation (small batch): GN/LN/IN gibi batch bağımsız normlar avantajlı olabilir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6) Mini deney: Küçük batch'te BN vs LN davranışı (hızlı)\n",
        "\n",
        "Bu deney:\n",
        "- Küçük batch rejiminde BN ve LN-benzeri yaklaşımın farklı davranabileceğini gösterir.\n",
        "- Gerçek dataset yerine `torchvision.datasets.FakeData` ile hızlı demo yapılır.\n",
        "- Bu bir benchmark değildir; sezgi kazandırma amaçlıdır.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size= 2 | BN loss=2.4088 | LN-like loss=2.3993\n",
            "Batch size=16 | BN loss=2.3126 | LN-like loss=2.3742\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = datasets.FakeData(size=512, image_size=(3, 32, 32), num_classes=10, transform=transform)\n",
        "\n",
        "def make_loader(batch_size):\n",
        "    return DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "class SmallCNN_BN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.fc = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.net(x).flatten(1)\n",
        "        return self.fc(x)\n",
        "\n",
        "class SmallCNN_LN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            LayerNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            LayerNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.fc = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.net(x).flatten(1)\n",
        "        return self.fc(x)\n",
        "\n",
        "def one_epoch(model, loader, lr=1e-2, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    total_loss = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "for bs in [2, 16]:\n",
        "    loader = make_loader(bs)\n",
        "    bn_model = SmallCNN_BN()\n",
        "    ln_model = SmallCNN_LN()\n",
        "    bn_loss = one_epoch(bn_model, loader)\n",
        "    ln_loss = one_epoch(ln_model, loader)\n",
        "    print(f\"Batch size={bs:>2} | BN loss={bn_loss:.4f} | LN-like loss={ln_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7) CNN'de LayerNorm ne zaman mantıklı?\n",
        "\n",
        "### Mantıklı olduğu yerler\n",
        "- **Batch size küçük** (detection/segmentation eğitimi)\n",
        "- SyncBN istemiyorsun veya distributed senaryo yok\n",
        "- Mimari zaten **channels-last** optimizasyonuna uygunsa\n",
        "- Backbone'da BN yerine batch bağımsız norm denemek istiyorsun\n",
        "\n",
        "### Dikkat gerektiren yerler\n",
        "- Klasik classification CNN (büyük batch) → BN çoğu zaman daha iyi/kolay\n",
        "- NCHW ile LN'yi yanlış eksende uygulamak → performans/kararlılık kaybı\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8) LN için pratik “doğru kullanım” checklist'i ✅\n",
        "\n",
        "1) **Hangi eksenlerde normalize ettiğini net yaz**  \n",
        "2) **Affine (γ,β) açık mı?** (genelde açık)  \n",
        "3) **eps** sayısal stabilite (karma hassasiyette bazen kritik)  \n",
        "4) CNN formatı: **NHWC mi NCHW mi?**  \n",
        "5) Detection/segmentation için gerçek metriklerle ölç: **mAP / IoU / Dice**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9) Deneme/ödev fikirleri (ileri seviye)\n",
        "\n",
        "1) Aynı backbone üzerinde BN → GN → LN değiştir ve küçük batch (2/4) rejiminde mAP/IoU farkını kıyasla.  \n",
        "2) LN'yi sadece neck'te kullan (FPN/PAN) ve etkisini ölç.  \n",
        "3) `LayerNorm2d` yerine `channels-last + nn.LayerNorm(C)` ile hız/VRAM kıyasla.  \n",
        "4) Mixed precision (fp16) eğitimde eps değerini değiştirip stabiliteyi gözle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 10) Özet\n",
        "\n",
        "- **LayerNorm**, batch'e bağımlı olmadan normalize eder.\n",
        "- **Training / inference** davranışı aynıdır (BN gibi running stats yok).\n",
        "- CNN'de kullanırken **eksen seçimi** kritik:\n",
        "  - Transformer/MLP: son boyut\n",
        "  - CNN: channels-last LN(C) veya NCHW için LN-benzeri katman (ör. `LayerNorm2d`)\n",
        "- Detection/segmentation gibi **small-batch** senaryolarda LN/GN ailesi sık tercih edilir.\n",
        "\n",
        "Bir sonraki defter: **Group Normalization (GN)** — LN ile farkı ve small-batch detection/segmentation’da neden çok popüler olduğu.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
