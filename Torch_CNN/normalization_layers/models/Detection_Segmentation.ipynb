{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed58cc2",
   "metadata": {},
   "source": [
    "\n",
    "# Detection / Segmentation’da “Saçmalama” (Kararsız Eğitim) Nasıl Önlenir?\n",
    "## Normalization + Stabilizasyon Paketi (Tek GPU / Küçük Batch Odaklı)\n",
    "\n",
    "Bu notebook’un amacı: **tek GPU** ve **küçük batch (1–4–8)** ile detection/segmentation eğitirken\n",
    "yaşanan “loss zıplaması / mAP-IOU oturmaması / training patlaması” gibi problemleri **mühendislik mantığıyla** açıklamak\n",
    "ve **pratik çözümleri** kod şablonlarıyla vermek.\n",
    "\n",
    "> Kısaca: “Saçmalama” çoğunlukla **istatistik gürültüsü + ölçek dalgalanması + agresif optimizasyon** birleşimidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75233dc6",
   "metadata": {},
   "source": [
    "\n",
    "## 1) “Saçmalama” dediğimiz şey tam olarak ne?\n",
    "\n",
    "Detection/segmentation eğitiminde tipik belirtiler:\n",
    "- Loss’un aşırı dalgalanması (step’ten step’e büyük zıplama)\n",
    "- mAP / IoU’nun uzun süre oturmaması veya “geri gitmesi”\n",
    "- Grad norm’un patlaması, NaN/Inf\n",
    "- Aynı seed ile bile çok farklı sonuçlar (yüksek varyans)\n",
    "\n",
    "Bu belirtiler çoğu zaman “model kötü” değil; **eğitim dinamikleri kararsız** demek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9aac3",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Kök nedenler (en sık görülen 3 kaynak)\n",
    "\n",
    "### 2.1. Küçük batch istatistiği (BN problemi)\n",
    "- Detection/segmentation’da input büyük → VRAM yetmez → batch küçük\n",
    "- BatchNorm train’de mean/var’ı batch’ten alır\n",
    "- Batch küçük olunca mean/var gürültülü olur → aktivasyon ölçeği zıplar\n",
    "\n",
    "### 2.2. Feature map ölçeği (weight/activation scale drift)\n",
    "- Derin conv bloklarında aktivasyon ölçeği katman katman kayabilir\n",
    "- Özellikle residual + ağır augment + mixed precision kombinasyonunda dalgalanma artar\n",
    "\n",
    "### 2.3. Optimizasyon agresifliği (LR, schedule, warmup)\n",
    "- Küçük batch + yüksek LR = çok sık patlama\n",
    "- Warmup yoksa ilk epoch’larda ölçek oturmadan parametreler sert güncellenir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8a64c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) “Sağlıklı kontrol paketi” \n",
    "\n",
    "Tek GPU + küçük batch için en sağlam yaklaşım sırası:\n",
    "\n",
    "1) **Normalization seçimini düzelt**\n",
    "   - Default: **GroupNorm**\n",
    "   - Alternatif: **Frozen BatchNorm** (pretrained backbone finetune)\n",
    "\n",
    "2) **Ölçek stabilizasyonu ekle (opsiyonel ama güçlü)**\n",
    "   - **Weight Standardization (WS)** özellikle GN ile iyi gider\n",
    "\n",
    "3) **Optimizasyon emniyet kemerleri**\n",
    "   - LR düşür, warmup ekle\n",
    "   - AMP kullan (VRAM kazan → batch büyüt)\n",
    "   - Gradient clipping (özellikle transformer head / ağır loss’larda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b686ab0",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Normalization tarafı: BN neden bozuluyor?\n",
    "\n",
    "BatchNorm (Conv için) kabaca kanallar bazında (B,H,W) üzerinden istatistik alır.\n",
    "\n",
    "- Train: \\(\\mu_B, \\sigma_B\\)\n",
    "- Eval: running \\(\\mu_R, \\sigma_R\\)\n",
    "\n",
    "Küçük batch’te problem:\n",
    "- \\(\\mu_B, \\sigma_B\\) çok oynar\n",
    "- Train’de normalize edilen dağılım step step değişir\n",
    "- Eval’a geçtiğinde running stats farklı olduğundan **train-eval mismatch** artar\n",
    "\n",
    "Bu mismatch, detection/segmentation gibi “ince ayar” gerektiren görevlerde daha acı hissedilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d8063",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Çözüm #1: GroupNorm (GN) — Tek GPU’da default\n",
    "\n",
    "### 5.1. GN fikri\n",
    "GN batch’e bakmaz. Kanalları gruplara ayırır ve her örnek içinde normalize eder.\n",
    "- Batch boyutu = 1 bile olsa stabil çalışır ✅\n",
    "- Train-eval mismatch gibi bir problem yok (istatistik sample içi)\n",
    "\n",
    "### 5.2. Nerede kullanılır?\n",
    "- Detection ve segmentation pipeline’larında çok yaygın\n",
    "- Özellikle küçük batch, büyük çözünürlük senaryosunda birinci tercih\n",
    "\n",
    "### 5.3. Pratik kural\n",
    "- `num_groups` genelde 32 seçilir (kanal sayısı buna bölünmeli)\n",
    "- Kanal azsa 16/8 gibi seç\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c244b978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 32, 32])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# GN örnek: C=64 kanallı bir blok için\n",
    "gn = nn.GroupNorm(num_groups=32, num_channels=64)\n",
    "x = torch.randn(2, 64, 32, 32)\n",
    "y = gn(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5bae7",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Çözüm #2: Frozen BatchNorm (BN freeze) — Pretrained finetune kurtarıcısı\n",
    "\n",
    "### 6.1. Mantık\n",
    "Pretrained backbone (ImageNet vb.) ile gelen BN running stats çoğu zaman “iyi”dir.\n",
    "Küçük batch ile finetune yaparken bu stats’leri güncellersen:\n",
    "- gürültülü batch stats running’e karışır\n",
    "- modelin ölçeği bozulur\n",
    "\n",
    "**Frozen BN**:\n",
    "- BN katmanlarını eval benzeri sabit tutar (running mean/var güncellenmez)\n",
    "- affine parametre (gamma/beta) istersen train edilebilir\n",
    "\n",
    "### 6.2. Ne zaman iyi?\n",
    "- Backbone pretrained\n",
    "- Batch küçük\n",
    "- GN’ye çevirmek istemiyorsun (pipeline karmaşıklığı)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fb008d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallBNNet(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basit BN freeze: BN modüllerini eval'a alıp stats güncellemesini kapatmak\n",
    "def freeze_batchnorm_stats(model: nn.Module):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()               # running stats kullan\n",
    "            m.requires_grad_(True) # affine train et (istersen)\n",
    "    return model\n",
    "\n",
    "# Örnek kullanım:\n",
    "class SmallBNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "m = SmallBNNet()\n",
    "m = freeze_batchnorm_stats(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37760b8",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Çözüm #3: Weight Standardization (WS) — GN ile iyi kombin\n",
    "\n",
    "### 7.1. WS nedir?\n",
    "WS, Conv ağırlıklarını (per-out-channel) standartlaştırır:\n",
    "- Her output channel için weight mean’ini çıkarır\n",
    "- std’ye böler\n",
    "\n",
    "Bu, activations’ın ölçeğini daha stabil tutar. GN ile birlikte kullanımı çok yaygındır (özellikle bazı segmentation/backbone çalışmalarında).\n",
    "\n",
    "### 7.2. WS neyi çözmez?\n",
    "- BN’in batch istatistiği sorununu tek başına çözmez\n",
    "- Ama GN/LN gibi batch bağımsız normlarla stabiliteyi artırır\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b313455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WSConv2d(nn.Conv2d):\n",
    "    \"\"\"Weight Standardization uygulanmış Conv2d\"\"\"\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        w_mean = w.mean(dim=(1,2,3), keepdim=True)\n",
    "        w = w - w_mean\n",
    "        w_std = w.flatten(1).std(dim=1, keepdim=True).view(-1,1,1,1) + 1e-5\n",
    "        w = w / w_std\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "class WS_GN_Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, groups=32):\n",
    "        super().__init__()\n",
    "        self.conv = WSConv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.gn = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.act(self.gn(self.conv(x)))\n",
    "\n",
    "blk = WS_GN_Block(3, 64, groups=32)\n",
    "y = blk(torch.randn(2,3,64,64))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b81475",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Optimizasyon emniyet kemerleri (çoğu kişinin kaçırdığı yer)\n",
    "\n",
    "### 8.1. Learning rate (LR) küçült\n",
    "Küçük batch’te gradient daha gürültülüdür → yüksek LR dalgalanmayı büyütür.\n",
    "\n",
    "Pratik:\n",
    "- Aynı modeli küçük batch ile eğitirken LR’ı düşür\n",
    "- Warmup ekle\n",
    "\n",
    "### 8.2. Warmup\n",
    "İlk N iterasyonda LR’ı yavaş artır.\n",
    "Bu, özellikle pretrained + yeni head senaryosunda patlamayı azaltır.\n",
    "\n",
    "### 8.3. AMP (mixed precision)\n",
    "AMP çoğu zaman:\n",
    "- VRAM kazandırır → batch’i büyütebilirsin (en temiz stabilite artışı)\n",
    "- throughput artırır\n",
    "\n",
    "### 8.4. Gradient clipping (opsiyonel)\n",
    "Özellikle transformer head / ağır loss kombinasyonunda:\n",
    "- grad norm’un patlamasını engeller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b156f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient clipping örnek şablon\n",
    "def train_step(model, opt, loss_fn, x, y, max_norm=1.0):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    out = model(x)\n",
    "    loss = loss_fn(out, y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "    opt.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f95a68",
   "metadata": {},
   "source": [
    "\n",
    "## 9) “Biz hangisini seçeceğiz?” Karar tablosu (tek GPU)\n",
    "\n",
    "### A) Yeni bir detection/segmentation modeli yazıyorsak\n",
    "✅ **GroupNorm** (default)  \n",
    "Opsiyonel: **WS + GN**\n",
    "\n",
    "### B) Pretrained backbone ile finetune yapıyorsak (çok yaygın)\n",
    "✅ **Frozen BN** veya **BN→GN dönüşümü**  \n",
    "- Eğer pipeline BN’e bağımlıysa: Frozen BN çok pratik\n",
    "- Eğer daha stabil istiyorsan: GN (veya WS+GN) daha temiz\n",
    "\n",
    "### C) Batch 1 ise\n",
    "✅ GN / WS+GN  \n",
    "⚠️ BN tavsiye edilmez (frozen değilse)\n",
    "\n",
    "> BatchRenorm bu dünyada genelde “niş/ara çözüm”. GN varken çoğu zaman tercih edilmez.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8e624",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Mini demo: Küçük batch’te BN vs GN istatistik farkı (sezgisel)\n",
    "\n",
    "Amaç:\n",
    "- BN’in batch küçülünce ne kadar oynaklaşabileceğini görmek\n",
    "- GN’in batch’ten bağımsız olduğuna sezgi kazanmak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN  batch=1 mean/std: (-3.812601789832115e-09, 0.9999950528144836)\n",
      "BN  batch=8 mean/std: (-5.587935447692871e-09, 0.9999949932098389)\n",
      "GN  batch=1 mean/std: (-8.731149137020111e-11, 0.9999949932098389)\n",
      "GN  batch=8 mean/std: (-2.342858351767063e-09, 0.9999949932098389)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "bn = nn.BatchNorm2d(16).train()\n",
    "gn = nn.GroupNorm(8, 16).train()\n",
    "\n",
    "x1 = torch.randn(1, 16, 32, 32)  # batch=1\n",
    "x8 = torch.randn(8, 16, 32, 32)  # batch=8\n",
    "\n",
    "y_bn_1 = bn(x1)\n",
    "y_bn_8 = bn(x8)\n",
    "\n",
    "y_gn_1 = gn(x1)\n",
    "y_gn_8 = gn(x8)\n",
    "\n",
    "def summ(t):\n",
    "    return float(t.mean()), float(t.std(unbiased=False))\n",
    "\n",
    "print(\"BN  batch=1 mean/std:\", summ(y_bn_1))\n",
    "print(\"BN  batch=8 mean/std:\", summ(y_bn_8))\n",
    "print(\"GN  batch=1 mean/std:\", summ(y_gn_1))\n",
    "print(\"GN  batch=8 mean/std:\", summ(y_gn_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce904e1",
   "metadata": {},
   "source": [
    "\n",
    "**Yorum:**  \n",
    "- BN çıktısı batch’e göre değişmeye daha yatkın (çünkü istatistiği batch’ten alıyor).  \n",
    "- GN’de batch boyutu değişse de “normalize mantığı” sample içidir; daha stabil davranır.\n",
    "\n",
    "> Bu demo “tam eğitim performansı” göstermez; sadece mekanizmayı sezdirir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74787c4f",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Pratik dönüşüm fonksiyonları (BN → GN)\n",
    "\n",
    "Elimizde BN’li bir model varsa, BN’leri GN’e çevirmek için modül dolaşan bir fonksiyon kullanabiliriz.\n",
    "Aşağıdaki fonksiyon `BatchNorm2d` gördüğü yerde onu `GroupNorm` ile değiştirir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a627a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def convert_bn_to_gn(model: nn.Module, num_groups: int = 32):\n",
    "    for name, m in model.named_children():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            C = m.num_features\n",
    "            g = min(num_groups, C)\n",
    "            while C % g != 0 and g > 1:\n",
    "                g -= 1\n",
    "            setattr(model, name, nn.GroupNorm(g, C))\n",
    "        else:\n",
    "            convert_bn_to_gn(m, num_groups=num_groups)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d267df1",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Sonuç: Sağlıklı eğitim için “3 kilit”\n",
    "\n",
    "1) **Norm seçimi:** GN (veya pretrained finetune ise Frozen BN)  \n",
    "2) **Ölçek stabilizasyonu:** WS + GN (opsiyonel ama güçlü)  \n",
    "3) **Optimizasyon emniyeti:** LR + warmup + AMP (+ clipping)\n",
    "\n",
    "Bunları yaptığımızda “saçmalama”nın %80–90’ı gider.\n",
    "Kalan kısım genelde dataset/augment/loss ayarıdır.;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
