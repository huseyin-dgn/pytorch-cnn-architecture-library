{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696badf3",
   "metadata": {},
   "source": [
    "\n",
    "# Sync Batch Normalization (SyncBatchNorm) â€” BaÅŸtan Sona\n",
    "\n",
    "> Bu notebook, **Sync Batch Normalization** kavramÄ±nÄ± **temelden ileri dÃ¼zeye** kadar anlatÄ±r:  \n",
    "> - Neden ihtiyaÃ§ duyulur?  \n",
    "> - **BatchNorm / LayerNorm / GroupNorm** ile farklarÄ±  \n",
    "> - Ã‡oklu GPU/Ã§oklu sÃ¼reÃ§ (DDP) senaryolarÄ±nda davranÄ±ÅŸ  \n",
    "> - PyTorchâ€™ta doÄŸru kurulum, dÃ¶nÃ¼ÅŸÃ¼m (`convert_sync_batchnorm`)  \n",
    "> - Performans/maliyet, pratik tuzaklar, â€œne zaman kullanmalÄ±?â€ karar Ã§erÃ§evesi  \n",
    "> - CNN/Detection/Segmentation Ã¶rnek kullanÄ±m kalÄ±plarÄ±  \n",
    "\n",
    "---\n",
    "\n",
    "## Ä°Ã§indekiler\n",
    "1. Batch Normalization temeli (kÄ±sa ama saÄŸlam)  \n",
    "2. SyncBatchNorm nedir? Ã‡alÄ±ÅŸma mantÄ±ÄŸÄ±  \n",
    "3. Ne zaman gerekli olur? (kÃ¼Ã§Ã¼k batch / Ã§oklu GPU)  \n",
    "4. DiÄŸer normalization tÃ¼rleriyle karÅŸÄ±laÅŸtÄ±rma (BN, LN, GN, IN)  \n",
    "5. PyTorchâ€™ta SyncBatchNorm:  \n",
    "   - DDP ile doÄŸru kullanÄ±m  \n",
    "   - Modeli SyncBNâ€™e Ã§evirme  \n",
    "   - SÄ±k hatalar (DataParallel, tek GPU, eval modu vb.)  \n",
    "6. Performans ve iletiÅŸim maliyeti (All-Reduce)  \n",
    "7. Uygulama alanlarÄ± (Detection/Segmentation/Video/CV)  \n",
    "8. Karar rehberi + pratik checklist  \n",
    "9. Mini Ã¶rnek: BN vs SyncBN (kod ÅŸablonlarÄ±)\n",
    "\n",
    "> Not: Kodlar â€œÃ§alÄ±ÅŸtÄ±rÄ±labilir ÅŸablonâ€ olarak verilir. Ã‡oklu GPU gerektiren hÃ¼creler, tek GPU/CPU ortamÄ±nda **bilgilendirici** amaÃ§lÄ±dÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b2147",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Batch Normalization (BN) Temeli\n",
    "\n",
    "### 1.1. BN neyi Ã§Ã¶zer?\n",
    "Derin aÄŸlarda katmanlarÄ±n aktivasyon daÄŸÄ±lÄ±mlarÄ± eÄŸitim boyunca kayar (internal covariate shift olarak popÃ¼lerleÅŸti). BN:\n",
    "- AktivasyonlarÄ± **mini-batch** istatistikleriyle normalize eder.\n",
    "- Daha stabil gradyan, daha yÃ¼ksek Ã¶ÄŸrenme oranÄ±, daha hÄ±zlÄ± yakÄ±nsama saÄŸlar.\n",
    "- DÃ¼zenlileÅŸtirici (regularizer) etkisi de vardÄ±r (mini-batch gÃ¼rÃ¼ltÃ¼sÃ¼).\n",
    "\n",
    "### 1.2. BN matematiÄŸi (Ã¶zet)\n",
    "Bir kanala (feature channel) ait aktivasyonlar iÃ§in, mini-batch Ã¼zerinde:\n",
    "- Ortalama:  \\(\\mu_B = \\frac{1}{m}\\sum_{i=1}^{m} x_i\\)\n",
    "- Varyans:  \\(\\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_i-\\mu_B)^2\\)\n",
    "\n",
    "Normalize:\n",
    "\\[\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "\\]\n",
    "\n",
    "Sonra Ã¶ÄŸrenilebilir affine parametrelerle Ã¶lÃ§ek/kaydÄ±r:\n",
    "\\[\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "\\]\n",
    "\n",
    "### 1.3. Train vs Eval farkÄ±\n",
    "- **Train:** \\(\\mu_B, \\sigma_B^2\\) mini-batchâ€™ten hesaplanÄ±r.\n",
    "- **Eval:** eÄŸitim boyunca biriken **running_mean** ve **running_var** kullanÄ±lÄ±r.\n",
    "\n",
    "**Kritik nokta:** BNâ€™in kalitesi, Ã¶zellikle **batch istatistiklerinin gÃ¼venilirliÄŸine** baÄŸlÄ±dÄ±r. Batch Ã§ok kÃ¼Ã§Ã¼kse istatistikler â€œgÃ¼rÃ¼ltÃ¼lÃ¼â€ olur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0712add",
   "metadata": {},
   "source": [
    "\n",
    "## 2) SyncBatchNorm Nedir?\n",
    "\n",
    "### 2.1. Problem: Ã‡oklu GPU + kÃ¼Ã§Ã¼k per-GPU batch\n",
    "Distributed Data Parallel (DDP) eÄŸitimde tipik olarak:\n",
    "- Her GPU bir â€œprocessâ€ Ã§alÄ±ÅŸtÄ±rÄ±r.\n",
    "- Her process kendi mini-batchâ€™ini iÅŸler (Ã¶r. GPU baÅŸÄ±na batch=2).\n",
    "- BN istatistikleri **lokal mini-batchâ€™ten** hesaplanÄ±r.\n",
    "\n",
    "EÄŸer GPU baÅŸÄ±na batch kÃ¼Ã§Ã¼kse (1â€“4 gibi), BN istatistikleri kararsÄ±zlaÅŸÄ±r â‡’ performans dÃ¼ÅŸer.\n",
    "\n",
    "### 2.2. Ã‡Ã¶zÃ¼m: SyncBatchNorm\n",
    "**SyncBatchNorm**, BN istatistiklerini **tÃ¼m sÃ¼reÃ§ler/GPUâ€™lar arasÄ±nda senkronize eder**.\n",
    "\n",
    "Yani, her GPU kendi aktivasyonlarÄ±ndan kÄ±smi istatistik Ã§Ä±karÄ±r, sonra:\n",
    "- TÃ¼m GPUâ€™lar arasÄ±nda **all-reduce** ile toplanÄ±r.\n",
    "- Global ortalama/varyans hesaplanÄ±r.\n",
    "- Her GPU aynÄ± \\(\\mu\\) ve \\(\\sigma^2\\) ile normalize eder.\n",
    "\n",
    "Bu sayede â€œglobal batchâ€ sanki tek GPUâ€™da daha bÃ¼yÃ¼k batch ile yapÄ±lmÄ±ÅŸ gibi davranÄ±r.\n",
    "\n",
    "### 2.3. Basit sezgi\n",
    "- Normal BN: â€œHer GPU kendi kÃ¼Ã§Ã¼k Ã¶rneÄŸine bakÄ±p karar veriyor.â€  \n",
    "- SyncBN: â€œTÃ¼m GPUâ€™lar konuÅŸup ortak karar veriyor.â€ âœ…\n",
    "\n",
    "Ama konuÅŸmanÄ±n maliyeti var: **iletiÅŸim (communication) overhead**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eeaef9",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Ne Zaman Gerekli Olur?\n",
    "\n",
    "### 3.1. Tipik ihtiyaÃ§ senaryolarÄ±\n",
    "- **Detection / Segmentation**: yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k â†’ GPU belleÄŸi sÄ±nÄ±rÄ± â†’ batch kÃ¼Ã§Ã¼lÃ¼r.\n",
    "- **3D / Video**: tensÃ¶rler bÃ¼yÃ¼k â†’ batch kÃ¼Ã§Ã¼lÃ¼r.\n",
    "- BÃ¼yÃ¼k backbone + aÄŸÄ±r head (Mask R-CNN, RetinaNet, U-Net varyantlarÄ±, vb.)\n",
    "- â€œGlobal batchâ€ hedefleniyor ama â€œper-GPU batchâ€ kÃ¼Ã§Ã¼k kalÄ±yor.\n",
    "\n",
    "### 3.2. Ne zaman gereksiz ya da zararlÄ± olabilir?\n",
    "- **Per-GPU batch zaten yeterince bÃ¼yÃ¼kse** (kabaca 16+ gibi; model ve veri tÃ¼rÃ¼ne gÃ¶re deÄŸiÅŸir): SyncBN Ã§oÄŸu zaman **fazladan overhead**.\n",
    "- **Tek GPU** eÄŸitimde: SyncBNâ€™in anlamÄ± yok (senkronize edilecek baÅŸka sÃ¼reÃ§ yok).\n",
    "- Ã‡ok yÃ¼ksek iletiÅŸim maliyeti olan sistemlerde (yavaÅŸ interconnect): throughput dÃ¼ÅŸebilir.\n",
    "\n",
    "### 3.3. â€œKÃ¼Ã§Ã¼k batchâ€ iÃ§in alternatifler\n",
    "SyncBN tek Ã§Ã¶zÃ¼m deÄŸil:\n",
    "- **GroupNorm (GN):** batch boyutundan baÄŸÄ±msÄ±z, Ã¶zellikle detectionâ€™da sÄ±k tercih edilir.\n",
    "- **LayerNorm (LN):** transformerâ€™larda default.\n",
    "- **Batch size bÃ¼yÃ¼tme:** gradient accumulation ile efektif batch artar ama BN istatistiÄŸi hÃ¢lÃ¢ â€œmicro-batchâ€ Ã¼zerinden hesaplanÄ±r (yani BN sorununu doÄŸrudan Ã§Ã¶zmez).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe888d05",
   "metadata": {},
   "source": [
    "\n",
    "## 4) DiÄŸer Normalization TÃ¼rleriyle FarklarÄ±\n",
    "\n",
    "AÅŸaÄŸÄ±da kÄ±sa ama â€œkarar vermeye yarayanâ€ bir karÅŸÄ±laÅŸtÄ±rma var.\n",
    "\n",
    "### 4.1. BatchNorm (BN)\n",
    "- Normalize ekseni: batch + (H,W) (Conv iÃ§in kanal bazlÄ±)\n",
    "- **Batch boyutuna duyarlÄ±**\n",
    "- CNNâ€™lerde Ã§ok baÅŸarÄ±lÄ± (Ã¶zellikle sÄ±nÄ±flandÄ±rma)\n",
    "\n",
    "### 4.2. SyncBatchNorm (SyncBN)\n",
    "- BNâ€™in aynÄ±sÄ± ama batch istatistiklerini **tÃ¼m GPUâ€™lardan** toplar\n",
    "- DDP Ã§oklu GPU senaryolarÄ±nda kÃ¼Ã§Ã¼k batch sorununu azaltÄ±r\n",
    "- **Ä°letiÅŸim maliyeti** var\n",
    "\n",
    "### 4.3. LayerNorm (LN)\n",
    "- Normalize ekseni: Ã¶rnek iÃ§indeki feature boyutu (token/kanal)\n",
    "- Batch boyutundan baÄŸÄ±msÄ±z\n",
    "- Transformer mimarilerinde standart\n",
    "\n",
    "### 4.4. GroupNorm (GN)\n",
    "- KanallarÄ± gruplara bÃ¶lÃ¼p normalize eder\n",
    "- Batch boyutundan baÄŸÄ±msÄ±z\n",
    "- Detection/Segmentationâ€™da Ã§ok kullanÄ±lÄ±r (Ã¶zellikle kÃ¼Ã§Ã¼k batch)\n",
    "\n",
    "### 4.5. InstanceNorm (IN)\n",
    "- Her Ã¶rneÄŸi (ve kanalÄ±) kendi iÃ§inde normalize eder\n",
    "- Stil transfer gibi iÅŸlerde yaygÄ±n\n",
    "\n",
    "> Ã–zet karar:  \n",
    "> - CNN + **kÃ¼Ã§Ã¼k per-GPU batch** + **Ã§oklu GPU** â‡’ SyncBN mantÄ±klÄ±.  \n",
    "> - CNN + kÃ¼Ã§Ã¼k batch ama iletiÅŸim pahalÄ± â‡’ GN daha stabil olabilir.  \n",
    "> - Transformer â‡’ LN Ã§oÄŸu zaman zaten yeterli.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4201d0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Norm</th>\n",
       "      <th>Normalize ekseni</th>\n",
       "      <th>Batch'e duyarlÄ±?</th>\n",
       "      <th>Ä°letiÅŸim maliyeti</th>\n",
       "      <th>Tipik kullanÄ±m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BatchNorm</td>\n",
       "      <td>Batch + (H,W)</td>\n",
       "      <td>Evet</td>\n",
       "      <td>Yok</td>\n",
       "      <td>CNN sÄ±nÄ±flandÄ±rma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SyncBatchNorm</td>\n",
       "      <td>Global Batch + (H,W)</td>\n",
       "      <td>Evet (global)</td>\n",
       "      <td>Var</td>\n",
       "      <td>Detection/Segmentation (DDP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Feature/Channel</td>\n",
       "      <td>HayÄ±r</td>\n",
       "      <td>Yok</td>\n",
       "      <td>Transformer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GroupNorm</td>\n",
       "      <td>Channel groups</td>\n",
       "      <td>HayÄ±r</td>\n",
       "      <td>Yok</td>\n",
       "      <td>Detection/Segmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>InstanceNorm</td>\n",
       "      <td>Per-sample, per-channel</td>\n",
       "      <td>HayÄ±r</td>\n",
       "      <td>Yok</td>\n",
       "      <td>Style transfer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Norm         Normalize ekseni Batch'e duyarlÄ±? Ä°letiÅŸim maliyeti  \\\n",
       "0      BatchNorm            Batch + (H,W)             Evet               Yok   \n",
       "1  SyncBatchNorm     Global Batch + (H,W)    Evet (global)               Var   \n",
       "2      LayerNorm          Feature/Channel            HayÄ±r               Yok   \n",
       "3      GroupNorm           Channel groups            HayÄ±r               Yok   \n",
       "4   InstanceNorm  Per-sample, per-channel            HayÄ±r               Yok   \n",
       "\n",
       "                 Tipik kullanÄ±m  \n",
       "0             CNN sÄ±nÄ±flandÄ±rma  \n",
       "1  Detection/Segmentation (DDP)  \n",
       "2                   Transformer  \n",
       "3        Detection/Segmentation  \n",
       "4                Style transfer  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KÄ±sa bir tabloyu yazdÄ±rmak iÃ§in: (not: bu hÃ¼cre opsiyonel)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    [\"BatchNorm\", \"Batch + (H,W)\", \"Evet\", \"Yok\", \"CNN sÄ±nÄ±flandÄ±rma\"],\n",
    "    [\"SyncBatchNorm\", \"Global Batch + (H,W)\", \"Evet (global)\", \"Var\", \"Detection/Segmentation (DDP)\"],\n",
    "    [\"LayerNorm\", \"Feature/Channel\", \"HayÄ±r\", \"Yok\", \"Transformer\"],\n",
    "    [\"GroupNorm\", \"Channel groups\", \"HayÄ±r\", \"Yok\", \"Detection/Segmentation\"],\n",
    "    [\"InstanceNorm\", \"Per-sample, per-channel\", \"HayÄ±r\", \"Yok\", \"Style transfer\"],\n",
    "], columns=[\"Norm\", \"Normalize ekseni\", \"Batch'e duyarlÄ±?\", \"Ä°letiÅŸim maliyeti\", \"Tipik kullanÄ±m\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595719f",
   "metadata": {},
   "source": [
    "\n",
    "## 5) PyTorchâ€™ta SyncBatchNorm\n",
    "\n",
    "### 5.1. AltÄ±n kural: SyncBN **DDP ile** anlamlÄ±dÄ±r\n",
    "\n",
    "**DDP = aynÄ± modelin birden fazla kopyasÄ±nÄ±, birden fazla GPUâ€™da, senkronize ÅŸekilde eÄŸitme mekanizmasÄ±dÄ±r.Model tekmiÅŸ gibi davranÄ±r ama iÅŸ yÃ¼kÃ¼ GPUâ€™lara bÃ¶lÃ¼nmÃ¼ÅŸtÃ¼r ğŸ”ğŸ”¥,**\n",
    "\n",
    "PyTorchâ€™ta SyncBatchNorm Ã§oÄŸunlukla ÅŸu senaryoda doÄŸru Ã§alÄ±ÅŸÄ±r:\n",
    "- `torch.nn.parallel.DistributedDataParallel (DDP)`\n",
    "- Her GPU = ayrÄ± process\n",
    "- NCCL backend (GPU iÃ§in)\n",
    "\n",
    "**DataParallel** (tek process, Ã§ok GPU) genelde Ã¶nerilmez; SyncBN iÃ§in de doÄŸru/optimum yol deÄŸildir.\n",
    "\n",
    "### 5.2. Modeli SyncBNâ€™e Ã§evirme\n",
    "PyTorchâ€™ta yaygÄ±n pratik:\n",
    "1) Modeli normal BN ile kur (nn.BatchNorm2d vs.)  \n",
    "2) DDPâ€™ye sarmalamadan Ã¶nce:  \n",
    "   `model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)`  \n",
    "3) Sonra DDP ile wrap et.\n",
    "\n",
    "### 5.3. Train/Eval davranÄ±ÅŸÄ±\n",
    "- Train: istatistikler sync edilir.\n",
    "- Eval: running stats kullanÄ±lÄ±r (senkronizasyon yok).\n",
    "\n",
    "### 5.4. Dikkat: BatchNorm â€œrunning statsâ€\n",
    "SyncBN de BN gibi `running_mean`, `running_var` tutar. EÄŸitim sÄ±rasÄ±nda global istatistiklerle gÃ¼ncellenir.\n",
    "\n",
    "### 5.5. SÄ±k tuzaklar\n",
    "- **DDP olmadan SyncBN kullanmak:** pratikte fayda yok/yanlÄ±ÅŸ beklenti.\n",
    "- **Batch Ã§ok kÃ¼Ã§Ã¼k + BN (sync yok):** mAP/IoU/accuracy dÃ¼ÅŸebilir.\n",
    "- `model.train()` / `model.eval()` modlarÄ±nÄ± yanlÄ±ÅŸ yÃ¶netmek.\n",
    "- `find_unused_parameters=True` gibi DDP ayarlarÄ± ile gereksiz overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6d7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã–nce: TinyCNN(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Sonra: TinyCNN(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# PyTorch'ta convert_sync_batchnorm Ã¶rneÄŸi (ÅŸablon)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = TinyCNN()\n",
    "print(\"Ã–nce:\", model)\n",
    "\n",
    "model_sync = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "print(\"\\nSonra:\", model_sync)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af6ecf",
   "metadata": {},
   "source": [
    "\n",
    "## 6) DDP ile â€œDoÄŸruâ€ EÄŸitim Ä°skele (Skeleton)\n",
    "\n",
    "AÅŸaÄŸÄ±daki iskelet, SyncBN kullanÄ±mÄ± iÃ§in en temiz yoldur:\n",
    "\n",
    "1. `torchrun --nproc_per_node=<GPU_SAYISI> train.py ...`\n",
    "2. `init_process_group(backend=\"nccl\")`\n",
    "3. `torch.cuda.set_device(local_rank)`\n",
    "4. Modeli GPUâ€™ya taÅŸÄ±\n",
    "5. `convert_sync_batchnorm`\n",
    "6. DDP ile sar: `DDP(model, device_ids=[local_rank])`\n",
    "7. DistributedSampler kullan\n",
    "8. EÄŸitim dÃ¶ngÃ¼sÃ¼\n",
    "\n",
    "> AÅŸaÄŸÄ±daki kod hÃ¼cresi â€œtek notebook iÃ§indeâ€ Ã¶rnek iskelet veriyor.  \n",
    "> Bunu genelde `train.py` ÅŸeklinde scriptâ€™e taÅŸÄ±mak daha doÄŸru (DDP notebook iÃ§inde uÄŸraÅŸtÄ±rÄ±r).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174fcdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import os\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.distributed as dist\n",
      "from torch.nn.parallel import DistributedDataParallel as DDP\n",
      "from torch.utils.data import DataLoader, DistributedSampler\n",
      "from torchvision import datasets, transforms\n",
      "\n",
      "def setup():\n",
      "    dist.init_process_group(backend=\"nccl\")\n",
      "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
      "    torch.cuda.set_device(local_rank)\n",
      "    return local_rank\n",
      "\n",
      "def cleanup():\n",
      "    dist.destroy_process_group()\n",
      "\n",
      "class Model(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.net = nn.Sequential(\n",
      "            nn.Conv2d(3, 32, 3, padding=1),\n",
      "            nn.BatchNorm2d(32),\n",
      "            nn.ReLU(),\n",
      "            nn.Conv2d(32, 64, 3, padding=1),\n",
      "            nn.BatchNorm2d(64),\n",
      "            nn.ReLU(),\n",
      "            nn.AdaptiveAvgPool2d(1),\n",
      "            nn.Flatten(),\n",
      "            nn.Linear(64, 10),\n",
      "        )\n",
      "    def forward(self, x): return self.net(x)\n",
      "\n",
      "def main():\n",
      "    local_rank = setup()\n",
      "\n",
      "    # dataset\n",
      "    tfm = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
      "    ds = datasets.FakeData(size=2000, image_size=(3,64,64), num_classes=10, transform=tfm)\n",
      "    sampler = DistributedSampler(ds, shuffle=True)\n",
      "    dl\n",
      "...\n",
      " dl:\n",
      "            x = x.cuda(local_rank, non_blocking=True)\n",
      "            y = y.cuda(local_rank, non_blocking=True)\n",
      "            opt.zero_grad(set_to_none=True)\n",
      "            logits = model(x)\n",
      "            loss = loss_fn(logits, y)\n",
      "            loss.backward()\n",
      "            opt.step()\n",
      "\n",
      "        if local_rank == 0:\n",
      "            print(f\"epoch {epoch} done\")\n",
      "\n",
      "    cleanup()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DDP + SyncBN eÄŸitim iskeleti (script ÅŸablonu)\n",
    "# Bunu tipik olarak train.py iÃ§ine koyarsÄ±n. Notebook iÃ§inde \"doÄŸrudan Ã§alÄ±ÅŸtÄ±rma\" hedeflenmemiÅŸtir.\n",
    "\n",
    "ddp_syncbn_template = r'''\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def setup():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return local_rank\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def main():\n",
    "    local_rank = setup()\n",
    "\n",
    "    # dataset\n",
    "    tfm = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "    ds = datasets.FakeData(size=2000, image_size=(3,64,64), num_classes=10, transform=tfm)\n",
    "    sampler = DistributedSampler(ds, shuffle=True)\n",
    "    dl = DataLoader(ds, batch_size=8, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = Model().cuda(local_rank)\n",
    "\n",
    "    # 1) SyncBN dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (DDP'den Ã–NCE)\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "\n",
    "    # 2) DDP wrap\n",
    "    model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        sampler.set_epoch(epoch)\n",
    "        for x, y in dl:\n",
    "            x = x.cuda(local_rank, non_blocking=True)\n",
    "            y = y.cuda(local_rank, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if local_rank == 0:\n",
    "            print(f\"epoch {epoch} done\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(ddp_syncbn_template[:1200] + \"\\n...\\n\" + ddp_syncbn_template[-400:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e065dd0",
   "metadata": {},
   "source": [
    "\n",
    "### Bu script nasÄ±l Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r?\n",
    "Ã–rneÄŸin 2 GPU iÃ§in:\n",
    "```bash\n",
    "torchrun --nproc_per_node=2 train.py\n",
    "```\n",
    "- `torchrun` DDP sÃ¼reÃ§lerini baÅŸlatÄ±r.\n",
    "- `LOCAL_RANK` gibi env deÄŸiÅŸkenleri otomatik gelir.\n",
    "\n",
    "> EÄŸer tek GPU/CPU ortamÄ±ndaysan, SyncBNâ€™e geÃ§mek **anlamsÄ±z** olur. Normal BN veya GN seÃ§.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb712b0d",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Performans ve Ä°letiÅŸim Maliyeti\n",
    "\n",
    "SyncBNâ€™in bedeli: **All-Reduce** iletiÅŸimi.\n",
    "\n",
    "### 7.1. Neden yavaÅŸlatÄ±r?\n",
    "Her SyncBN katmanÄ±, forward aÅŸamasÄ±nda (istatistik iÃ§in) ve bazen backward aÅŸamasÄ±nda iletiÅŸim gerektirir.\n",
    "- Modelde Ã§ok BN varsa (ResNet gibi), iletiÅŸim sayÄ±sÄ± artar.\n",
    "- Interconnect yavaÅŸsa (PCIe vs NVLink vs InfiniBand), fark bÃ¼yÃ¼r.\n",
    "\n",
    "### 7.2. Pratik optimizasyonlar\n",
    "- SyncBNâ€™i sadece kritik bloklarda kullanmak (her yerde ÅŸart deÄŸil)\n",
    "- Alternatif: GN (communication yok)\n",
    "- Daha bÃ¼yÃ¼k per-GPU batch mÃ¼mkÃ¼nse yapmak\n",
    "- Mixed precision (AMP) computeâ€™Ä± hÄ±zlandÄ±rÄ±r ama iletiÅŸim overheadâ€™i aynÄ± kalabilir\n",
    "\n",
    "### 7.3. â€œDoÄŸruâ€ beklenti\n",
    "SyncBN genelde:\n",
    "- **Stabilite/kalite** kazandÄ±rÄ±r (Ã¶zellikle kÃ¼Ã§Ã¼k per-GPU batchâ€™te)\n",
    "- Ama **throughput** dÃ¼ÅŸebilir\n",
    "\n",
    "Bu yÃ¼zden hedef metrik (mAP, IoU, accuracy) vs â€œtime-to-trainâ€ dengesi kurulur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced5ffa",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Uygulama AlanlarÄ± ve Tipik KullanÄ±m\n",
    "\n",
    "### 8.1. Object Detection\n",
    "- YOLO, Faster R-CNN, RetinaNet, DETR dÄ±ÅŸÄ±ndaki klasik CNN headâ€™li modeller\n",
    "- High-res input â†’ kÃ¼Ã§Ã¼k batch â†’ BN bozulur\n",
    "- SyncBN veya GN tercih edilir\n",
    "\n",
    "### 8.2. Semantic / Instance Segmentation\n",
    "- U-Net tÃ¼revleri, DeepLab, Mask R-CNN\n",
    "- Batch kÃ¼Ã§Ã¼lÃ¼r, SyncBN yaygÄ±n\n",
    "\n",
    "### 8.3. Video / 3D\n",
    "- 3D Convâ€™lar bellek yer â†’ batch kÃ¼Ã§Ã¼lÃ¼r\n",
    "- SyncBN ile stabilizasyon\n",
    "\n",
    "### 8.4. BÃ¼yÃ¼k Ã¶lÃ§ekli sÄ±nÄ±flandÄ±rma\n",
    "EÄŸer global batch zaten bÃ¼yÃ¼kse SyncBN ÅŸart deÄŸil.\n",
    "Ama â€œÃ§ok GPU + per-GPU batch kÃ¼Ã§Ã¼kâ€ ise (Ã¶r. dev model, bÃ¼yÃ¼k gÃ¶rsel) yine anlamlÄ± olur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880feddc",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Karar Rehberi (Cheat Sheet)\n",
    "\n",
    "### SyncBatchNorm kullan âœ…\n",
    "- DDP (Ã§oklu GPU / Ã§oklu process) kullanÄ±yorsun\n",
    "- Per-GPU batch kÃ¼Ã§Ã¼k (1â€“4â€“8 gibi)\n",
    "- CNN tabanlÄ± model (Conv aÄŸÄ±rlÄ±klÄ±)\n",
    "- Detection/Segmentation gibi BN hassas gÃ¶revler\n",
    "\n",
    "### SyncBatchNorm kullanma âŒ (Ã§oÄŸu durumda)\n",
    "- Tek GPU / tek process\n",
    "- Per-GPU batch zaten bÃ¼yÃ¼k (gÃ¶reli olarak)\n",
    "- Transformer aÄŸÄ±rlÄ±klÄ± mimari (LN kullan)\n",
    "- Ä°letiÅŸim Ã§ok pahalÄ± ve GN ile benzer kalite alabiliyorsun\n",
    "\n",
    "### Alternatif Ã¶neriler\n",
    "- Per-GPU batch kÃ¼Ã§Ã¼k + SyncBN pahalÄ± â‡’ **GroupNorm**\n",
    "- Batch bÃ¼yÃ¼tmek istiyorsun â‡’ gradient accumulation (ama BN sorununu tam Ã§Ã¶zmez)\n",
    "- Stabilite istiyorsun â‡’ LR schedule / warmup / EMA / uygun augment stratejileri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee84974c",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Mini Demonstrasyon: BN istatistiklerinin â€œbatch kÃ¼Ã§Ã¼lÃ¼nceâ€ sapmasÄ±\n",
    "\n",
    "AÅŸaÄŸÄ±daki Ã¶rnekte tek GPUâ€™da Ã§alÄ±ÅŸÄ±rken bile ÅŸunu gÃ¶sterebiliriz:\n",
    "- Batch kÃ¼Ã§Ã¼ldÃ¼kÃ§e (B=2 vs B=32) batch istatistikleri daha gÃ¼rÃ¼ltÃ¼lÃ¼ olur.\n",
    "- Bu doÄŸrudan SyncBNâ€™i Ã§alÄ±ÅŸtÄ±rmak deÄŸil ama **neden** SyncBNâ€™e ihtiyaÃ§ duyulduÄŸunu sezdirir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3081a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B=2  mean (ilk 5 kanal): tensor([ 0.0049, -0.0272, -0.0208, -0.0453, -0.0016])\n",
      "B=32 mean (ilk 5 kanal): tensor([ 0.0013, -0.0023, -0.0025,  0.0018, -0.0067])\n",
      "B=2  var  (ilk 5 kanal): tensor([1.0341, 1.0294, 0.9736, 0.9417, 0.9718])\n",
      "B=32 var  (ilk 5 kanal): tensor([0.9924, 1.0024, 1.0140, 0.9981, 0.9955])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# AynÄ± layer, farklÄ± batch boyutlarÄ±yla Ã§Ä±kan batch mean/var Ã¶rneÄŸi\n",
    "bn = nn.BatchNorm2d(16, affine=False, track_running_stats=False)  # sadece batch stats gÃ¶rmek iÃ§in\n",
    "bn.train()\n",
    "\n",
    "def stats_for_batch(B):\n",
    "    x = torch.randn(B, 16, 32, 32)  # (B,C,H,W)\n",
    "    with torch.no_grad():\n",
    "        # BN forward iÃ§inde mean/var hesaplar; ama biz doÄŸrudan kendimiz de gÃ¶rebiliriz:\n",
    "        # Kanal bazlÄ±, batch ve spatial Ã¼zerinde\n",
    "        mu = x.mean(dim=(0,2,3))\n",
    "        var = x.var(dim=(0,2,3), unbiased=False)\n",
    "    return mu, var\n",
    "\n",
    "mu2, var2 = stats_for_batch(2)\n",
    "mu32, var32 = stats_for_batch(32)\n",
    "\n",
    "print(\"B=2  mean (ilk 5 kanal):\", mu2[:5])\n",
    "print(\"B=32 mean (ilk 5 kanal):\", mu32[:5])\n",
    "print(\"B=2  var  (ilk 5 kanal):\", var2[:5])\n",
    "print(\"B=32 var  (ilk 5 kanal):\", var32[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9240a5",
   "metadata": {},
   "source": [
    "\n",
    "**Yorum:**  \n",
    "B=2 gibi kÃ¼Ã§Ã¼k batchâ€™te kanal baÅŸÄ±na mean/var daha â€œoynakâ€ Ã§Ä±kar. Ã‡oklu GPUâ€™da her GPU B=2 gÃ¶rÃ¼yorsa, her GPUâ€™nun BNâ€™i farklÄ± normalize eder â‡’ eÄŸitim â€œdaÄŸÄ±lÄ±râ€.  \n",
    "SyncBN bu istatistikleri GPUâ€™lar arasÄ±nda toplayÄ±p daha stabil hale getirir.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Pratik Checklist \n",
    "- [ ] Ã‡oklu GPU mu? (DDP)  \n",
    "- [ ] Per-GPU batch kÃ¼Ã§Ã¼k mÃ¼?  \n",
    "- [ ] CNN aÄŸÄ±rlÄ±klÄ± model mi?  \n",
    "- [ ] `convert_sync_batchnorm` DDPâ€™den Ã¶nce mi yapÄ±ldÄ±?  \n",
    "- [ ] `DistributedSampler` kullanÄ±yor musun?  \n",
    "- [ ] `model.train()` / `model.eval()` doÄŸru mu?  \n",
    "- [ ] Overheadâ€™e deÄŸer mi? (mAP/IoU artÄ±ÅŸÄ± vs hÄ±z kaybÄ±)\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Son Notlar (Ä°leri Seviye Ä°puÃ§larÄ±)\n",
    "- BazÄ± modern frameworkâ€™ler â€œFrozen BNâ€ kullanÄ±r: BNâ€™i eÄŸitimde sabitler (Ã¶zellikle finetune). Bu, kÃ¼Ã§Ã¼k batchâ€™te alternatif olabilir.\n",
    "- Detection ekosisteminde GN/SyncBN seÃ§imi Ã§oÄŸu zaman â€œaltyapÄ± + hÄ±zâ€ ile belirlenir.\n",
    "- EÄŸer backbone pretrain + kÃ¼Ã§Ã¼k batch finetune yapÄ±yorsan:  \n",
    "  - BN katmanlarÄ±nÄ± freeze etmek veya GNâ€™ye Ã§evirmek bazen daha stabil Ã§Ä±kar.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
