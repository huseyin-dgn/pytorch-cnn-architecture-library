{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bc1c3d",
   "metadata": {},
   "source": [
    "\n",
    "# Çift GPU (Multi‑GPU) Mantığı ve SyncBatchNorm Nerede Devreye Girer?\n",
    "\n",
    "Bu notebook’ta şunları **temelden ileri seviyeye** netleştiriyoruz:\n",
    "\n",
    "- “Çift GPU” tam olarak ne demek?\n",
    "- Neden tek GPU’da SyncBatchNorm anlamsız?\n",
    "- Bir model “neden birden fazla GPU ister?” (VRAM ve compute motivasyonları)\n",
    "- **Data Parallel vs DDP (DistributedDataParallel)** farkı\n",
    "- **SyncBatchNorm** hangi yerlerde iş görür, nerede gereksizdir?\n",
    "- Pratik senaryolar: detection / segmentation / video / dev model\n",
    "- Çalıştırma şablonları (`torchrun`, `LOCAL_RANK`, `DistributedSampler`)\n",
    "\n",
    "> Not: Çoklu GPU kodları genelde notebook’tan ziyade `train.py` script’inde çalıştırılır.\n",
    "> Burada “kopyala‑yapıştır” şablonlar veriyorum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0768c9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) “Çift GPU” nedir?\n",
    "\n",
    "**Çift GPU**, bilgisayarda **iki adet fiziksel ekran kartı** (2× GPU) olmasıdır.\n",
    "\n",
    "Örnek:\n",
    "- GPU0: RTX 3060\n",
    "- GPU1: RTX 3060 (ya da başka bir GPU)\n",
    "\n",
    "Bu durumda her GPU:\n",
    "- Kendi **CUDA çekirdeklerine**\n",
    "- Kendi **VRAM**’ine (örn. 12 GB + 12 GB)\n",
    "sahiptir.\n",
    "\n",
    "### Kritik:\n",
    "- GPU’nun içindeki CUDA çekirdekleri “ikinci GPU” değildir.\n",
    "- CPU da “ikinci GPU” değildir.\n",
    "\n",
    "**Multi‑GPU** demek: *Birden fazla fiziksel GPU cihazı* demek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145c566",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Bir model neden “birden fazla GPU ister”?\n",
    "\n",
    "İki ana sebep var:\n",
    "\n",
    "### 2.1. VRAM (Bellek) yetmiyordur\n",
    "- Görüntü çözünürlüğü yüksek (ör. 1024×1024, 4K)\n",
    "- Model büyük (çok katman, çok kanal)\n",
    "- Batch büyütmek istiyorsun ama VRAM yetmiyor\n",
    "- 3D/video tensörleri çok büyük (B, C, T, H, W)\n",
    "\n",
    "Bu durumda birden fazla GPU ile:\n",
    "- **Veriyi paylaşarak** (data parallel) batch’i büyütürsün\n",
    "- Ya da **modeli bölerek** (model parallel) tek örneği sığdırırsın\n",
    "\n",
    "### 2.2. Compute (hesap) yetmiyordur / hız istiyorsundur\n",
    "- Tek GPU ile eğitim süresi çok uzun\n",
    "- Daha fazla GPU ile aynı işi daha hızlı bitirmek istersin\n",
    "\n",
    "> Multi‑GPU, her zaman “daha hızlı” demek değildir.\n",
    "> Haberleşme overhead’i (GPU‑GPU iletişimi) yüzünden bazı durumlarda verim düşebilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cfc10",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Multi‑GPU yaklaşımları: Data Parallel vs Model Parallel\n",
    "\n",
    "### 3.1. Data Parallel (Veri paralelliği)\n",
    "En yaygın yaklaşım budur.\n",
    "\n",
    "- Modelin **aynı kopyası** her GPU’da vardır\n",
    "- Dataset/batch, GPU’lar arasında bölünür\n",
    "- Her GPU kendi parçasında forward/backward yapar\n",
    "- Sonra gradyanlar toplanır ve tüm GPU’lar aynı ağırlıkları günceller\n",
    "\n",
    "**Kullanım amacı:** Batch’i büyütmek ve throughput artırmak\n",
    "\n",
    "### 3.2. Model Parallel (Model paralelliği)\n",
    "- Modelin katmanları GPU’lara bölünür\n",
    "- Örn. ilk yarı GPU0, ikinci yarı GPU1\n",
    "- Tek bir örnek bile tek GPU’ya sığmıyorsa kullanılır\n",
    "\n",
    "**Kullanım amacı:** Dev modeli/büyük aktivasyonları sığdırmak\n",
    "\n",
    "> SyncBatchNorm, esas olarak **data parallel (DDP)** dünyasında önemlidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd23479",
   "metadata": {},
   "source": [
    "\n",
    "## 4) DataParallel (DP) vs DistributedDataParallel (DDP)\n",
    "\n",
    "### 4.1. DataParallel (eski yol)\n",
    "- Tek process, çok GPU\n",
    "- Forward’da batch’i böler, sonuçları toplar\n",
    "- Çoğu durumda **DDP’den daha yavaş ve daha sorunlu**\n",
    "\n",
    "### 4.2. DDP (modern, doğru yol)\n",
    "- GPU başına **1 process**\n",
    "- Her process bir GPU’ya sabitlenir\n",
    "- Gradient senkronizasyonu **NCCL all‑reduce** ile verimli yapılır\n",
    "- Ölçeklenebilirlik daha iyi\n",
    "\n",
    "**Özet:** Multi‑GPU eğitimde PyTorch tarafında standart = **DDP**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab56a6",
   "metadata": {},
   "source": [
    "\n",
    "## 5) DDP nasıl çalışır? (Adım adım)\n",
    "\n",
    "Varsayalım 2 GPU var.\n",
    "\n",
    "### 5.1. Her GPU’da ayrı process\n",
    "- Process0 → GPU0\n",
    "- Process1 → GPU1\n",
    "\n",
    "### 5.2. Data bölünür\n",
    "- GPU0 batch’in ilk yarısını görür\n",
    "- GPU1 batch’in ikinci yarısını görür\n",
    "\n",
    "### 5.3. Her GPU kendi gradient’ini çıkarır\n",
    "- Forward + backward her GPU’da ayrı\n",
    "\n",
    "### 5.4. All‑Reduce ile gradyanlar toplanır\n",
    "- GPU0 ve GPU1 gradyanlarını birbirine gönderir/ortalama alır\n",
    "- Sonuç: iki GPU da **aynı** güncellenmiş ağırlıklara sahip olur\n",
    "\n",
    "### Ana fikir:\n",
    "DDP, GPU’lar arasında “ağırlık güncellemesini aynı tutmak” için sürekli senkronizasyon yapar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756643c",
   "metadata": {},
   "source": [
    "\n",
    "## 6) SyncBatchNorm bu resimde nereye oturuyor?\n",
    "\n",
    "### 6.1. Normal BatchNorm’un DDP’de sorunu\n",
    "BatchNorm train modunda mean/var’ı **mini‑batch’ten** hesaplar.\n",
    "\n",
    "DDP’de:\n",
    "- GPU0 kendi mini‑batch’inden mean/var hesaplar\n",
    "- GPU1 kendi mini‑batch’inden mean/var hesaplar\n",
    "\n",
    "Eğer per‑GPU batch küçükse (1–4–8 gibi):\n",
    "- İstatistikler gürültülü olur\n",
    "- GPU’lar farklı normalize eder\n",
    "- Eğitim kararsızlaşabilir (özellikle detection/segmentation)\n",
    "\n",
    "### 6.2. SyncBatchNorm çözümü\n",
    "SyncBatchNorm:\n",
    "- Her GPU’nun kısmi istatistiğini çıkarır\n",
    "- GPU’lar arasında **all‑reduce** ile birleştirir\n",
    "- Tek bir “global” mean/var elde eder\n",
    "- Her GPU aynı mean/var ile normalize eder ✅\n",
    "\n",
    "### 6.3. Neden tek GPU’da anlamsız?\n",
    "Tek GPU’da:\n",
    "- Senkronize edilecek başka GPU yok\n",
    "- Global = lokal\n",
    "Dolayısıyla SyncBN, normal BN ile aynı davranışa iner (fayda yok).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d312d",
   "metadata": {},
   "source": [
    "\n",
    "## 7) SyncBatchNorm nerelerde kullanılır?\n",
    "\n",
    "### 7.1. Çok yaygın kullanım alanları\n",
    "- **Object Detection** (YOLO türevleri, Faster R‑CNN, RetinaNet vb. CNN‑tabanlı backbone’lar)\n",
    "- **Segmentation** (U‑Net, DeepLab türevleri)\n",
    "- **Video / 3D Conv** (batch genelde küçük olur)\n",
    "- Büyük çözünürlükte training (VRAM yüzünden batch küçülür)\n",
    "\n",
    "### 7.2. Ne zaman gereksiz?\n",
    "- Per‑GPU batch zaten büyükse (göreli olarak)\n",
    "- Tek GPU eğitim\n",
    "- Transformer ağırlıklı mimariler (LayerNorm standardı)\n",
    "- İletişimin aşırı pahalı olduğu altyapı (yavaş interconnect)\n",
    "\n",
    "### 7.3. Alternatif: GroupNorm\n",
    "Tek GPU + küçük batch için en pratik alternatif çoğunlukla **GroupNorm**’dur.\n",
    "Batch’e bağımlı değildir, senkronizasyon gerektirmez.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859dafb",
   "metadata": {},
   "source": [
    "\n",
    "## 8) “İki GPU varsa batch nasıl büyür?” (Somut örnek)\n",
    "\n",
    "Varsayalım:\n",
    "- Per‑GPU batch = 4\n",
    "- GPU sayısı = 2\n",
    "\n",
    "DDP ile efektif global batch:\n",
    "- Global batch ≈ 4 × 2 = 8\n",
    "\n",
    "Ama dikkat:\n",
    "- Bu “global batch”,\n",
    "- BN istatistiği açısından **SyncBN kullanmazsan** yine GPU başına 4’ten hesaplanır.\n",
    "\n",
    "SyncBN ile:\n",
    "- BN istatistiği global 8 üzerinden hesaplanır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210cc9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basit hesap: global batch = per_gpu_batch * world_size\n",
    "per_gpu_batch = 4\n",
    "world_size = 2\n",
    "global_batch = per_gpu_batch * world_size\n",
    "global_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f4809a",
   "metadata": {},
   "source": [
    "\n",
    "## 9) DDP + SyncBatchNorm: doğru kurulum şablonu\n",
    "\n",
    "**Altın kural:** SyncBN dönüşümü DDP wrap’inden önce yapılır.\n",
    "\n",
    "Akış:\n",
    "1) `dist.init_process_group(backend=\"nccl\")`\n",
    "2) `torch.cuda.set_device(local_rank)`\n",
    "3) `model.cuda(local_rank)`\n",
    "4) `model = nn.SyncBatchNorm.convert_sync_batchnorm(model)`\n",
    "5) `model = DDP(model, device_ids=[local_rank])`\n",
    "6) `DistributedSampler` ile DataLoader\n",
    "\n",
    "Aşağıdaki script şablonunu alıp `train.py` yap:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa73445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# train.py\n",
      "import os\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.distributed as dist\n",
      "from torch.nn.parallel import DistributedDataParallel as DDP\n",
      "from torch.utils.data import DataLoader, DistributedSampler\n",
      "from torchvision import datasets, transforms\n",
      "\n",
      "def setup():\n",
      "    dist.init_process_group(backend=\"nccl\")\n",
      "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
      "    torch.cuda.set_device(local_rank)\n",
      "    return local_rank\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.net = nn.Sequential(\n",
      "            nn.Conv2d(3, 32, 3, padding=1),\n",
      "            nn.BatchNorm2d(32),\n",
      "            nn.ReLU(),\n",
      "            nn.Conv2d(32, 64, 3, padding=1),\n",
      "            nn.BatchNorm2d(64),\n",
      "            nn.ReLU(),\n",
      "            nn.AdaptiveAvgPool2d(1),\n",
      "            nn.Flatten(),\n",
      "            nn.Linear(64, 10),\n",
      "        )\n",
      "    def forward(self, x): return self.net(x)\n",
      "\n",
      "def main():\n",
      "    local_rank = setup()\n",
      "\n",
      "    tfm = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
      "    ds = datasets.FakeData(size=2000, image_size=(3,64,64), num_classes=10, transform=tfm)\n",
      "    sampler = DistributedSampler(ds, shuffle=True)\n",
      "    dl = DataLoader(ds, batch_size=8, sampler=sampler, num_w\n",
      "...\n",
      "       y = y.cuda(local_rank, non_blocking=True)\n",
      "            opt.zero_grad(set_to_none=True)\n",
      "            out = model(x)\n",
      "            loss = loss_fn(out, y)\n",
      "            loss.backward()\n",
      "            opt.step()\n",
      "\n",
      "        if local_rank == 0:\n",
      "            print(\"epoch\", epoch, \"done\")\n",
      "\n",
      "    dist.destroy_process_group()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ddp_syncbn_script = r'''\n",
    "# train.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def setup():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return local_rank\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def main():\n",
    "    local_rank = setup()\n",
    "\n",
    "    tfm = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "    ds = datasets.FakeData(size=2000, image_size=(3,64,64), num_classes=10, transform=tfm)\n",
    "    sampler = DistributedSampler(ds, shuffle=True)\n",
    "    dl = DataLoader(ds, batch_size=8, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = Net().cuda(local_rank)\n",
    "\n",
    "    # SyncBN dönüşümü DDP'den ÖNCE\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "\n",
    "    model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        sampler.set_epoch(epoch)\n",
    "        for x, y in dl:\n",
    "            x = x.cuda(local_rank, non_blocking=True)\n",
    "            y = y.cuda(local_rank, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if local_rank == 0:\n",
    "            print(\"epoch\", epoch, \"done\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "print(ddp_syncbn_script[:1200] + \"\\n...\\n\" + ddp_syncbn_script[-350:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5db5df",
   "metadata": {},
   "source": [
    "\n",
    "### Çalıştırma komutu\n",
    "2 GPU için:\n",
    "```bash\n",
    "torchrun --nproc_per_node=2 train.py\n",
    "```\n",
    "\n",
    "4 GPU için:\n",
    "```bash\n",
    "torchrun --nproc_per_node=4 train.py\n",
    "```\n",
    "\n",
    "> Tek GPU’da `--nproc_per_node=1` olur ama SyncBN’nin faydası yoktur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9cfc1",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Sık yapılan hatalar (kısa ama kritik)\n",
    "\n",
    "- **DataParallel kullanmak:** SyncBN/performans için önerilmez.\n",
    "- `convert_sync_batchnorm`’u DDP’den sonra yapmak: yanlış.\n",
    "- `DistributedSampler` kullanmamak: her GPU aynı veriyi görebilir, verim düşer.\n",
    "- `model.eval()` modunda SyncBN beklemek: eval’da BN running stats kullanır, sync yok.\n",
    "- Per‑GPU batch = 1 ise bile SyncBN stabilite sağlar ama iletişim maliyeti artabilir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
