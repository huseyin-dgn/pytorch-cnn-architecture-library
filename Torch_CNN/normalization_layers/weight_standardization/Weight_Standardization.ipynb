{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f7e5c8",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----\n",
    "----\n",
    "\n",
    "## Weight Standardization (WS) neden iÅŸe yarÄ±yor?\n",
    "\n",
    "Weight Standardizationâ€™da (WS) **optimizer gerÃ§ek aÄŸÄ±rlÄ±ÄŸÄ± (`w`) Ã¶ÄŸrenir**,  \n",
    "ama **forward sÄ±rasÄ±nda kullanÄ±lan aÄŸÄ±rlÄ±k her zaman standardize edilmiÅŸ halidir (`Åµ`)**.\n",
    "\n",
    "Bu ne demek?\n",
    "\n",
    "- Model **her zaman aynÄ± Ã¶lÃ§ekli (meanâ‰ˆ0, stdâ‰ˆ1) aÄŸÄ±rlÄ±kla** hesap yapar.\n",
    "- AÄŸÄ±rlÄ±klarÄ±n bÃ¼yÃ¼yerek veya kÃ¼Ã§Ã¼lerek â€œhile yapmasÄ±â€ engellenir.\n",
    "- Ã–ÄŸrenme **Ã¶lÃ§ek Ã¼zerinden deÄŸil, desen/pattern Ã¼zerinden** olur.\n",
    "\n",
    "Ã–nemli nokta:\n",
    "- Loss, **standardize edilmiÅŸ aÄŸÄ±rlÄ±kla yapÄ±lan forward** sonucuna gÃ¶re hesaplanÄ±r.\n",
    "- Gradient bu lossâ€™tan gelir ve **gerÃ§ek aÄŸÄ±rlÄ±ÄŸÄ±** gÃ¼nceller.\n",
    "- Yani model, **â€œstandardize edilmiÅŸ halim iyi sonuÃ§ versinâ€** diye Ã¶ÄŸrenmek zorunda kalÄ±r.\n",
    "\n",
    "SonuÃ§:\n",
    "- Katmanlar arasÄ± Ã¶lÃ§ek kaymasÄ± azalÄ±r\n",
    "- Gradient patlamasÄ± ihtimali dÃ¼ÅŸer\n",
    "- Ã–zellikle **GroupNorm + kÃ¼Ã§Ã¼k batch** senaryosunda eÄŸitim daha stabil olur\n",
    "\n",
    "Ã–zetle:\n",
    "> **WS, aÄŸÄ±rlÄ±ÄŸÄ± kalÄ±cÄ± olarak deÄŸiÅŸtirmez;  \n",
    "> ama modeli, aÄŸÄ±rlÄ±ÄŸÄ± her zaman kontrollÃ¼ bir formda kullanmaya zorlar.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91094a76",
   "metadata": {},
   "source": [
    "\n",
    "# Weight Standardization (WS) â€” BaÅŸtan Sona (En Temelden En Ä°leriye)\n",
    "## Tek GPU / Detection-Segmentation OdaklÄ±, PyTorch UygulamalÄ±\n",
    "\n",
    "Bu notebookâ€™ta **Weight Standardization** konusunu **sÄ±fÄ±rdan** baÅŸlayarak, saÄŸlam ve net ÅŸekilde kuruyoruz:\n",
    "\n",
    "- WS nedir? Ne deÄŸildir?\n",
    "- Neden â€œweightâ€ standardize edilir?\n",
    "- Matematik: Conv aÄŸÄ±rlÄ±klarÄ±nÄ± kanal bazÄ±nda standardize etmek\n",
    "- BN / GN / LN ile iliÅŸkisi (Ã¶zellikle **GN + WS** kombinasyonu)\n",
    "- Nerelerde kullanÄ±lÄ±r? Nerelerde gereksiz?\n",
    "- PyTorchâ€™ta 3 farklÄ± uygulama:\n",
    "  1) **WSConv2d** (drop-in Conv2d)\n",
    "  2) **Parametrization** ile WS (daha temiz)\n",
    "  3) Var olan modeli otomatik dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "- Mini demo: Ã¶lÃ§ek stabilitesi sezgisi\n",
    "- Pratik checklist (detection/segmentation)\n",
    "\n",
    "> Ã–nemli: WS, **aktivasyonu deÄŸil aÄŸÄ±rlÄ±ÄŸÄ±** normalize eder. Bu yÃ¼zden GN/LN gibi aktivasyon normlarÄ±yla birlikte en iyi sonucu verir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ec749",
   "metadata": {},
   "source": [
    "\n",
    "## 1) En temelden: â€œStandardizasyonâ€ ne demek?\n",
    "\n",
    "Bir vektÃ¶rÃ¼/diziyi standardize etmek:\n",
    "- ortalamasÄ±nÄ± 0â€™a yaklaÅŸtÄ±rmak\n",
    "- standart sapmasÄ±nÄ± 1â€™e yaklaÅŸtÄ±rmak\n",
    "\n",
    "FormÃ¼l (eleman bazÄ±nda):\n",
    "\\[\n",
    "z = \\frac{x - \\mu}{\\sigma + \\epsilon}\n",
    "\\]\n",
    "\n",
    "Buradaki amaÃ§:\n",
    "- sayÄ±sal Ã¶lÃ§ekleri kontrol etmek\n",
    "- optimizasyonu daha stabil yapmak\n",
    "- â€œbir katmanÄ±n aÄŸÄ±rlÄ±klarÄ± Ã§ok bÃ¼yÃ¼dÃ¼, diÄŸeri Ã§ok kÃ¼Ã§Ã¼k kaldÄ±â€ gibi dengesizlikleri azaltmak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915497a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Weight Standardization (WS) nedir?\n",
    "\n",
    "**WS = Convolution (veya Linear) aÄŸÄ±rlÄ±klarÄ±nÄ±** forward sÄ±rasÄ±nda standardize etmek demektir.\n",
    "\n",
    "Yani:\n",
    "- Her forwardâ€™da weight tensorÃ¼nÃ¼ al\n",
    "- Her output kanal iÃ§in (out_channel) weight meanâ€™ini Ã§Ä±kar\n",
    "- stdâ€™ye bÃ¶l\n",
    "- Sonra standart Convâ€™u bu â€œstandardize weightâ€ ile yap\n",
    "\n",
    "### WS ne deÄŸildir?\n",
    "- BatchNorm deÄŸildir (aktivasyon istatistiÄŸi yok)\n",
    "- SyncBN deÄŸildir (GPUâ€™lar arasÄ± istatistik yok)\n",
    "- â€œBatch kÃ¼Ã§Ã¼kse BNâ€™i dÃ¼zeltirâ€ iddiasÄ± **tek baÅŸÄ±na** doÄŸru deÄŸil\n",
    "\n",
    "WSâ€™nin ana rolÃ¼: **weight Ã¶lÃ§eÄŸini stabilize etmek**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2972b3",
   "metadata": {},
   "source": [
    "\n",
    "## 3) WS neden iÅŸe yarar? (Sezgi)\n",
    "\n",
    "Convolutionâ€™da Ã§Ä±ktÄ± Ã¶lÃ§eÄŸini en Ã§ok belirleyen ÅŸeylerden biri **weight Ã¶lÃ§eÄŸi**dir.\n",
    "EÄŸer bazÄ± katmanlarda weight daÄŸÄ±lÄ±mÄ± â€œkayarsaâ€ (mean bÃ¼yÃ¼r, std artar):\n",
    "- aktivasyon Ã¶lÃ§eÄŸi zÄ±plar\n",
    "- gradient Ã¶lÃ§eÄŸi zÄ±plar\n",
    "- eÄŸitim kararsÄ±zlaÅŸÄ±r\n",
    "\n",
    "WS ÅŸu etkiyi verir:\n",
    "- Her output kanalÄ±n weightâ€™ini â€œbenzer Ã¶lÃ§ekâ€e Ã§eker\n",
    "- BÃ¶ylece katmanlar arasÄ± Ã¶lÃ§ek driftâ€™i azalÄ±r\n",
    "- Ã–zellikle **GN/LN** gibi batch baÄŸÄ±msÄ±z normlarla birleÅŸince gÃ¼zel stabilite saÄŸlar\n",
    "\n",
    "> Detection/segmentationâ€™da kÃ¼Ã§Ã¼k batch olduÄŸu iÃ§in GN yaygÄ±n; GN ile WS kombinasyonu bu yÃ¼zden popÃ¼ler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdebde",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Matematik: Conv2dâ€™de WS nasÄ±l uygulanÄ±r?\n",
    "\n",
    "Conv2d weight ÅŸekli:\n",
    "- \\(W \\in \\mathbb{R}^{O \\times I \\times kH \\times kW}\\)\n",
    "  - O: out_channels\n",
    "  - I: in_channels\n",
    "\n",
    "WS (klasik) per-output-channel yapÄ±lÄ±r:\n",
    "\n",
    "1) Her output kanal iÃ§in mean:\n",
    "\\[\n",
    "\\mu_o = \\frac{1}{I kH kW} \\sum W_o\n",
    "\\]\n",
    "\n",
    "2) Her output kanal iÃ§in std:\n",
    "\\[\n",
    "\\sigma_o = \\sqrt{\\frac{1}{I kH kW} \\sum (W_o - \\mu_o)^2 + \\epsilon}\n",
    "\\]\n",
    "\n",
    "3) Standardize:\n",
    "\\[\n",
    "\\hat{W}_o = \\frac{W_o - \\mu_o}{\\sigma_o}\n",
    "\\]\n",
    "\n",
    "4) Convâ€™u \\(\\hat{W}\\) ile yap:\n",
    "\\[\n",
    "y = x * \\hat{W}\n",
    "\\]\n",
    "\n",
    "Bu kadar. Ä°statistik **batchâ€™ten deÄŸil**, weightâ€™in kendisinden geliyor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553194d",
   "metadata": {},
   "source": [
    "\n",
    "## 5) WS hangi normlarla iyi gider?\n",
    "\n",
    "### 5.1. WS + GroupNorm (en yaygÄ±n pratik)\n",
    "- GN aktivasyonu normalize eder (sample iÃ§i)\n",
    "- WS aÄŸÄ±rlÄ±ÄŸÄ± normalize eder (channel baÅŸÄ±)\n",
    "- KÃ¼Ã§Ã¼k batchâ€™te Ã§ok stabil\n",
    "\n",
    "### 5.2. WS + LayerNorm\n",
    "- Transformer tabanlÄ± conv-lar / hibritlerde kullanÄ±labilir\n",
    "- Ama klasik CNN detection tarafÄ±nda GN daha yaygÄ±n\n",
    "\n",
    "### 5.3. WS + BatchNorm\n",
    "- BN zaten aktivasyon Ã¶lÃ§eÄŸini batch Ã¼zerinden ayarlar\n",
    "- WS bazen yardÄ±mcÄ± olur ama Ã§oÄŸu zaman â€œolmazsa olmazâ€ deÄŸil\n",
    "- KÃ¼Ã§Ã¼k batchâ€™te BN zaten sorunlu; GN tercih edilir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e344a7",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Nerelerde kullanÄ±lÄ±r? Nerelerde gereksiz?\n",
    "\n",
    "### Kullan âœ… (Ã§ok olasÄ±)\n",
    "- Detection / segmentation + kÃ¼Ã§Ã¼k batch\n",
    "- GN kullanan CNN backbone/head\n",
    "- Deep conv aÄŸlarÄ±: Ã¶lÃ§ek driftâ€™ine hassas\n",
    "- Training stabilitesi (loss zÄ±plamasÄ±, grad patlamasÄ±) yaÅŸanÄ±yorsa\n",
    "\n",
    "### Kullanma / dikkat âš ï¸\n",
    "- Zaten BN + bÃ¼yÃ¼k batch ile Ã§ok stabil gidiyorsan: etkisi sÄ±nÄ±rlÄ± olabilir\n",
    "- Ã‡ok hÄ±zlÄ± inference istiyorsan: WS forwardâ€™da ekstra hesap getirir (genelde kÃ¼Ã§Ã¼k ama var)\n",
    "- Quantization / fused conv-bn optimizasyonlarÄ±nda ek iÅŸ Ã§Ä±karabilir (pipelineâ€™a baÄŸlÄ±)\n",
    "\n",
    "> WS genelde â€œstabilite iÃ§in kÃ¼Ã§Ã¼k bir masrafâ€tÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8cfcd7",
   "metadata": {},
   "source": [
    "\n",
    "## 7) PyTorch Uygulama 1: WSConv2d (en pratik)\n",
    "\n",
    "AÅŸaÄŸÄ±daki sÄ±nÄ±f, `nn.Conv2d` gibi davranÄ±r ama forwardâ€™da weightâ€™i standardize eder.\n",
    "Drop-in replacement: Conv2d yerine WSConv2d koyarsÄ±n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485eaa6",
   "metadata": {},
   "source": [
    "**w.flatten(1).std(dim=1) her out_channel filtre iÃ§in std hesaplar. Sonra w / w_std yaparak her filtrenin Ã¶lÃ§eÄŸini 1â€™e Ã§ekersin; bu da katmanlar arasÄ± aktivasyon/gradient Ã¶lÃ§eÄŸinin zÄ±plamasÄ±nÄ± azaltÄ±r. Convâ€™un diÄŸer parametreleri aynÄ± kalÄ±r; sadece forwardâ€™da kullanÄ±lan weight standardize edilir.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fe927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WSConv2d(nn.Conv2d):\n",
    "    \"\"\"Weight Standardization uygulanmÄ±ÅŸ Conv2d\"\"\"\n",
    "    def forward(self, x):\n",
    "        # w.shape = (O, I, kH, kW)\n",
    "            # O: out_channels (filtre sayÄ±sÄ±)\n",
    "            # I: in_channels\n",
    "            # kH,kW: kernel boyutu\n",
    "\n",
    "        w = self.weight\n",
    "        # Her out_channel iÃ§in mean/std\n",
    "        w_mean = w.mean(dim=(1,2,3), keepdim=True)\n",
    "        w = w - w_mean\n",
    "\n",
    "        # flatten(1) demek: 0. boyutu (O) koru, geri kalan her ÅŸeyi dÃ¼zleÅŸtir.\n",
    "        w_std = w.flatten(1).std(dim=1, keepdim=True).view(-1,1,1,1) + 1e-5 # O=64 ise â†’ 64 tane std Ã§Ä±kar (her filtreye 1 std).\n",
    "        # Åu anda w_std ÅŸekli (O,1). Ama biz bunu w ile bÃ¶lmek istiyoruz :: w ÅŸekli (O, I, kH, kW) :: Broadcast dÃ¼zgÃ¼n olsun diye w_stdâ€™yi (O,1,1,1) yapÄ±yoruz.\n",
    "\n",
    "        w = w / w_std\n",
    "\n",
    "        # Forward sÄ±rasÄ±nda kullanÄ±lan weight (w) standardize edilmiÅŸ oluyor.\n",
    "        # Bias, stride, padding, dilation, groups aynÄ± kalÄ±yor.\n",
    "        # Modelin parametresi kalÄ±cÄ± olarak deÄŸiÅŸmiyor\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        \n",
    "# quick test\n",
    "conv = WSConv2d(3, 16, 3, padding=1, bias=False)\n",
    "x = torch.randn(2,3,64,64)\n",
    "y = conv(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a6d19",
   "metadata": {},
   "source": [
    "\n",
    "## 8) WS + GN blok (Detection/Segmentation iÃ§in gerÃ§ekÃ§i)\n",
    "\n",
    "Bu blok: `WSConv2d -> GroupNorm -> SiLU`\n",
    "Bu tarz bloklar U-Net, FPN head, backbone gibi yerlerde rahatÃ§a kullanÄ±lÄ±r.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e99d73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 128, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WSGNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, groups=32, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv = WSConv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False)\n",
    "        g = min(groups, out_ch)\n",
    "        while out_ch % g != 0 and g > 1:\n",
    "            g -= 1\n",
    "        self.gn = nn.GroupNorm(num_groups=g, num_channels=out_ch)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.gn(self.conv(x)))\n",
    "\n",
    "blk = WSGNBlock(3, 64, groups=32)\n",
    "blk(torch.randn(2,3,128,128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b26a28",
   "metadata": {},
   "source": [
    "\n",
    "## 9) PyTorch Uygulama 2: Parametrization ile WS (daha temiz yaklaÅŸÄ±m)\n",
    "\n",
    "PyTorch `torch.nn.utils.parametrize` ile bir parametreye â€œdÃ¶nÃ¼ÅŸÃ¼mâ€ ekleyebilirsin.\n",
    "Avantaj:\n",
    "- ModÃ¼l kodunu Ã§ok bozmadan weight Ã¼zerinde WS uygularsÄ±n\n",
    "- Daha modÃ¼ler olur\n",
    "\n",
    "AÅŸaÄŸÄ±daki Ã¶rnek, Conv2d weightâ€™ine WS parametrizasyonu uygular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390279d9",
   "metadata": {},
   "source": [
    "#### iki katman var:\n",
    "\n",
    "* Orijinal (Ã¶ÄŸrenilen) parametre â†’ optimizer bunun Ã¼stÃ¼nden gÃ¼nceller\n",
    "\n",
    "* GÃ¶rÃ¼nen parametre (forwardâ€™da kullanÄ±lan) â†’ orijinalden her seferinde hesaplanÄ±r\n",
    "\n",
    "##### Åunu yaptÄ±ÄŸÄ±mÄ±zda:\n",
    "```python \n",
    "parametrize.register_parametrization(conv, \"weight\", WeightStandardization())\n",
    "```\n",
    "##### PyTorch ÅŸunu yapar:\n",
    "\n",
    "* conv.weight artÄ±k bir Tensor parametresi deÄŸil, hesaplanan bir property gibi davranÄ±r.\n",
    "\n",
    "* AsÄ±l Ã¶ÄŸrenilen parametre ÅŸuraya taÅŸÄ±nÄ±r: **conv.parametrizations.weight.original**\n",
    "\n",
    "Biz conv.weight dediÄŸimizde, PyTorch otomatik olarak: **WeightStandardization()(conv.parametrizations.weight.original)** dÃ¶ndÃ¼rÃ¼r.\n",
    "\n",
    "##### **Yani forwardâ€™da kullanÄ±lan weight = standardize edilmiÅŸ aÄŸÄ±rlÄ±k.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee924d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "class WeightStandardization(nn.Module):\n",
    "    def __init__(self, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, w):\n",
    "        w_mean = w.mean(dim=(1,2,3), keepdim=True)\n",
    "        w = w - w_mean\n",
    "        w_std = w.flatten(1).std(dim=1, keepdim=True).view(-1,1,1,1) + self.eps\n",
    "        return w / w_std\n",
    "\n",
    "conv2 = nn.Conv2d(3, 16, 3, padding=1, bias=False)\n",
    "parametrize.register_parametrization(conv2, \"weight\", WeightStandardization(eps=1e-5))\n",
    "\n",
    "x = torch.randn(2,3,64,64)\n",
    "y2 = conv2(x)\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd90a91",
   "metadata": {},
   "source": [
    "## Bunu metafora vuralÄ±m\n",
    "\n",
    "#### **Parametrize = Filtre takmak ğŸ§ƒ**\n",
    "\n",
    "Åimdi dÃ¼ÅŸÃ¼nelim:\n",
    "\n",
    "* Elimizde orijinal meyve suyu var â†’ bu gerÃ§ek weight\n",
    "\n",
    "Biz iÃ§meden Ã¶nce:\n",
    "\n",
    "* sÃ¼zgeÃ§ten geÃ§iriyorsuz\n",
    "\n",
    "* tadÄ±nÄ± dengeliyorsuz\n",
    "\n",
    "Ama:\n",
    "\n",
    "* Meyve suyunu dÃ¶kÃ¼p yenisini koymuyoruz\n",
    "\n",
    "* ÅiÅŸeyi deÄŸiÅŸtirmiyoruz\n",
    "\n",
    "* Sadece iÃ§erken sÃ¼zÃ¼yoruz\n",
    "\n",
    "âŒ Parametreyi baÅŸka parametreyle deÄŸiÅŸtirmiyor\n",
    "\n",
    "âŒ KalÄ±cÄ± kopya koymuyor\n",
    "\n",
    "âœ… AynÄ± parametreyi, kullanmadan Ã¶nce makyajlÄ±yor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36553c6e",
   "metadata": {},
   "source": [
    "\n",
    "### Parametrization notu\n",
    "Bu yÃ¶ntemde:\n",
    "- `conv2.weight` artÄ±k â€œparametrizedâ€ bir gÃ¶rÃ¼nÃ¼m olur\n",
    "- Orijinal parametre `conv2.parametrizations.weight.original` iÃ§inde durur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c349dbf",
   "metadata": {},
   "source": [
    "\n",
    "## 10) PyTorch Uygulama 3: Model iÃ§indeki Conv2dâ€™leri otomatik WSâ€™e dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "\n",
    "Elinde bÃ¼yÃ¼k bir model var diyelim (backbone + head). Hepsini tek tek elle deÄŸiÅŸtirmek eziyet.\n",
    "AÅŸaÄŸÄ±daki fonksiyon, model iÃ§inde gezip **Conv2d â†’ WSConv2d** yapar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf03fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã–nce: TinyNet(\n",
      "  (stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (mid): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "Sonra: TinyNet(\n",
      "  (stem): WSConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (mid): Sequential(\n",
      "    (0): WSConv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): WSConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def convert_conv_to_wsconv(module: nn.Module):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Conv2d) and not isinstance(child, WSConv2d):\n",
    "            ws = WSConv2d(\n",
    "                in_channels=child.in_channels,\n",
    "                out_channels=child.out_channels,\n",
    "                kernel_size=child.kernel_size,\n",
    "                stride=child.stride,\n",
    "                padding=child.padding,\n",
    "                dilation=child.dilation,\n",
    "                groups=child.groups,\n",
    "                bias=(child.bias is not None),\n",
    "                padding_mode=child.padding_mode,\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                ws.weight.copy_(child.weight)\n",
    "                if child.bias is not None:\n",
    "                    ws.bias.copy_(child.bias)\n",
    "            setattr(module, name, ws)\n",
    "        else:\n",
    "            convert_conv_to_wsconv(child)\n",
    "    return module\n",
    "\n",
    "# demo\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Conv2d(3, 16, 3, padding=1, bias=False)\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1, bias=False),\n",
    "        )\n",
    "    def forward(self, x): return self.mid(self.stem(x))\n",
    "\n",
    "m = TinyNet()\n",
    "print(\"Ã–nce:\", m)\n",
    "m = convert_conv_to_wsconv(m)\n",
    "print(\"Sonra:\", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf1db4",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Mini demo: WSâ€™in â€œÃ¶lÃ§ek stabilitesiâ€ sezgisi\n",
    "\n",
    "Bu demo eÄŸitim performansÄ±nÄ± ispatlamaz; ama WSâ€™in weight daÄŸÄ±lÄ±mÄ±nÄ± kontrol ettiÄŸini gÃ¶rÃ¼rsÃ¼n.\n",
    "\n",
    "- Normal Conv: weightâ€™in channel mean/stdâ€™si serbest\n",
    "- WSConv: forwardâ€™da her out_channel iÃ§in mean ~0, std ~1â€™e zorlanÄ±r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46beb910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain weight mean (ilk 5): tensor([-0.0016, -0.0173,  0.0059, -0.0146, -0.0172])\n",
      "Plain weight std  (ilk 5): tensor([0.1036, 0.1192, 0.1182, 0.1214, 0.0989])\n",
      "WS    weight mean (ilk 5): tensor([ 2.6491e-08,  4.4152e-08, -1.3245e-08,  1.3245e-08,  8.8303e-09])\n",
      "WS    weight std  (ilk 5): tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "conv_plain = nn.Conv2d(3, 8, 3, padding=1, bias=False)\n",
    "conv_ws = WSConv2d(3, 8, 3, padding=1, bias=False)\n",
    "with torch.no_grad():\n",
    "    conv_ws.weight.copy_(conv_plain.weight)\n",
    "\n",
    "w = conv_plain.weight.detach()\n",
    "w_mean = w.mean(dim=(1,2,3))\n",
    "w_std  = w.flatten(1).std(dim=1)\n",
    "\n",
    "# WS sonrasÄ± efektif weight'i hesaplayalÄ±m (forward iÃ§indeki gibi)\n",
    "w2 = conv_ws.weight.detach()\n",
    "w2 = w2 - w2.mean(dim=(1,2,3), keepdim=True)\n",
    "w2 = w2 / (w2.flatten(1).std(dim=1, keepdim=True).view(-1,1,1,1) + 1e-5)\n",
    "w2_mean = w2.mean(dim=(1,2,3))\n",
    "w2_std  = w2.flatten(1).std(dim=1)\n",
    "\n",
    "print(\"Plain weight mean (ilk 5):\", w_mean[:5])\n",
    "print(\"Plain weight std  (ilk 5):\", w_std[:5])\n",
    "print(\"WS    weight mean (ilk 5):\", w2_mean[:5])\n",
    "print(\"WS    weight std  (ilk 5):\", w2_std[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04efd2a2",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Detection/Segmentationâ€™da WSâ€™yi nereye koyarÄ±m?\n",
    "\n",
    "Pratik yerleÅŸim:\n",
    "- Backbone conv bloklarÄ±\n",
    "- FPN / decoder conv bloklarÄ±\n",
    "- Head conv bloklarÄ± (box/class/mask)\n",
    "\n",
    "En yaygÄ±n pattern:\n",
    "- `WSConv2d -> GroupNorm -> Activation`\n",
    "\n",
    "Ã–zellikle:\n",
    "- batch=1â€“4\n",
    "- GN kullanÄ±yorsun\n",
    "- loss dalgalanmasÄ± / grad patlamasÄ± gÃ¶rÃ¼yorsun\n",
    "â†’ WS eklemek mantÄ±klÄ±.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd403405",
   "metadata": {},
   "source": [
    "\n",
    "## 13) SÄ±k hatalar ve tuzaklar\n",
    "\n",
    "- **WSâ€™yi BN ile â€œkÃ¼Ã§Ã¼k batch Ã§Ã¶zÃ¼mÃ¼â€ sanmak:** Tek baÅŸÄ±na BN sorununu Ã§Ã¶zmez.\n",
    "- **GN gruplarÄ±nÄ± yanlÄ±ÅŸ seÃ§mek:** `out_channels % num_groups == 0` olmalÄ±.\n",
    "- **Bias kullanÄ±mÄ±:** WSConv2dâ€™de bias kullanÄ±labilir ama birÃ§ok backboneâ€™da bias=False tercih edilir (GN zaten shift yapÄ±yor).\n",
    "- **Inference optimizasyonu:** BazÄ± deployment/fusion adÄ±mlarÄ±nda ekstra iÅŸlem olabilir; trainingâ€™de genelde problem deÄŸil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc77384",
   "metadata": {},
   "source": [
    "\n",
    "## 14) HÄ±zlÄ± checklist\n",
    "\n",
    "- [ ] Batch kÃ¼Ã§Ã¼k mÃ¼? (1â€“4â€“8)  \n",
    "- [ ] Normalization GN mi? (evet olmalÄ±)  \n",
    "- [ ] Training dalgalÄ± mÄ±?  \n",
    "- [ ] WS + GN block denedin mi?  \n",
    "- [ ] LR/warmup/AMP ayarlarÄ± yerinde mi?  \n",
    "\n",
    "Ã–nerilen baÅŸlangÄ±Ã§:\n",
    "- GN (groups=32, kanal kÃ¼Ã§Ã¼kse 16/8)\n",
    "- WSConv2d + GN + SiLU\n",
    "- AMP aÃ§Ä±k\n",
    "- LR biraz daha dÃ¼ÅŸÃ¼k + warmup\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
