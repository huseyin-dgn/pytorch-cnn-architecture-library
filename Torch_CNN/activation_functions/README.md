# Activation Functions â€“ KarÅŸÄ±laÅŸtÄ±rmalÄ± Deneyler ve SeÃ§im Rehberi

Bu dizin, CNN mimarilerinde kullanÄ±lan **aktivasyon fonksiyonlarÄ±nÄ±n** teorik arka planÄ±nÄ±, pratik kullanÄ±m alanlarÄ±nÄ± ve **aynÄ± koÅŸullarda yapÄ±lan deneysel karÅŸÄ±laÅŸtÄ±rmalarÄ±nÄ±** iÃ§erir.

AmaÃ§:

- Aktivasyon fonksiyonlarÄ±nÄ±n **gerÃ§ek model performansÄ±na etkisini** gÃ¶stermek
- â€œHangisi daha iyi?â€ sorusuna **sayÄ±sal kanÄ±tla** cevap vermek
- Backbone, attention ve deploy senaryolarÄ± iÃ§in **net seÃ§im kurallarÄ±** sunmak

---

## Kapsanan Aktivasyonlar

- ReLU
- LeakyReLU
- PReLU
- SiLU (Swish)
- HardSwish
- Sigmoid / HardSigmoid (Ã¶zellikle attention & gating iÃ§in)

---

## Deney OrtamÄ± (StandartlaÅŸtÄ±rÄ±lmÄ±ÅŸ)

TÃ¼m karÅŸÄ±laÅŸtÄ±rmalar:

- **AynÄ± mimari**
- **AynÄ± optimizer, LR, scheduler**
- **AynÄ± dataset ve augmentasyon**
- **AynÄ± seed**
- **AynÄ± epoch sayÄ±sÄ±**

kullanÄ±larak yapÄ±lmÄ±ÅŸtÄ±r.

Bu sayede farklar **yalnÄ±zca aktivasyon fonksiyonundan** kaynaklanmaktadÄ±r.

---

## ğŸ“Š Aktivasyon KarÅŸÄ±laÅŸtÄ±rmasÄ± â€“ Kesin SonuÃ§

### Final Epoch (15/15)

| Activation | Final Train Loss | Final Test Loss | Final Test Accuracy |
| ---------- | ---------------- | --------------- | ------------------- |
| ReLU       | 0.4770           | 0.6740          | 0.7797              |
| SiLU       | 0.4712           | 0.5892          | 0.8029              |
| LeakyReLU  | 0.5041           | 0.5645          | 0.8088              |
| **PReLU**  | **0.4614**       | **0.5573**      | **0.8149**          |

### Peak Accuracy (TÃ¼m EÄŸitim Boyunca)

| Activation | Peak Accuracy | Epoch  |
| ---------- | ------------- | ------ |
| ReLU       | 0.7915        | 14     |
| SiLU       | 0.8102        | 14     |
| LeakyReLU  | 0.8088        | 15     |
| **PReLU**  | **0.8149**    | **15** |

---

## ğŸ† Genel DeÄŸerlendirme

### 1ï¸âƒ£ PReLU â€“ En GÃ¼Ã§lÃ¼ Performans

- En yÃ¼ksek **final accuracy**
- En dÃ¼ÅŸÃ¼k **test loss**
- EÄŸitim ilerledikÃ§e performans dÃ¼ÅŸmÃ¼yor
- Kanal baÅŸÄ±na Ã¶ÄŸrenilebilir negatif eÄŸim sayesinde **daha esnek temsil**

â¡ï¸ **Accuracy Ã¶ncelikli projeler iÃ§in birincil tercih**

---

### 2ï¸âƒ£ LeakyReLU â€“ Dengeli ve Hafif

- Parametresiz
- Stabil eÄŸitim
- PReLUâ€™ya Ã§ok yakÄ±n performans
- Deploy / sade mimariler iÃ§in avantajlÄ±

â¡ï¸ **Lightweight ve production senaryolarÄ± iÃ§in ideal**

---

### 3ï¸âƒ£ SiLU (Swish) â€“ Modern ama bu deneyde Ã¼Ã§Ã¼ncÃ¼

- DÃ¼zgÃ¼n gradyan akÄ±ÅŸÄ±
- ReLUâ€™dan aÃ§Ä±kÃ§a iyi
- Ancak bu deneyde **PReLU ve LeakyReLUâ€™nun gerisinde**

â¡ï¸ **Alternatif olarak kullanÄ±labilir ama varsayÄ±lan tercih deÄŸil**

---

### 4ï¸âƒ£ ReLU â€“ En ZayÄ±f SeÃ§enek

- Dying ReLU problemi
- Test loss dalgalÄ±
- Final accuracy en dÃ¼ÅŸÃ¼k

â¡ï¸ **Bu mimari ve ayarlarda Ã¶nerilmez**

---

## HardSigmoid & HardSwish Notu (Ã–zel KullanÄ±m)

Bu aktivasyonlar backbone iÃ§in deÄŸil, **gating ve efficiency** odaklÄ±dÄ±r:

- **HardSigmoid**
  - SE / CBAM / Coordinate Attention gibi **attention gate**â€™lerde uygundur
  - 0â€“1 aralÄ±ÄŸÄ±nda Ã¶lÃ§ekleme yapar
- **HardSwish**
  - Mobil / edge modellerde
  - Swishâ€™e yakÄ±n performans + daha dÃ¼ÅŸÃ¼k hesaplama maliyeti

---

## ğŸ”§ Pratik SeÃ§im Rehberi

| Senaryo                     | Ã–nerilen Aktivasyon   |
| --------------------------- | --------------------- |
| Accuracy Ã¶ncelikli CNN      | **PReLU**             |
| Dengeli / sade mimari       | LeakyReLU             |
| Modern backbone (opsiyonel) | SiLU                  |
| Mobil / edge backbone       | HardSwish             |
| Attention / gating          | Sigmoid / HardSigmoid |
| Klasik ReLU                 | âŒ Ã¶nerilmez          |

---

## SonuÃ§ (Tek CÃ¼mle)

> Bu deneylerde **PReLU**, CNN backboneâ€™larÄ± iÃ§in en yÃ¼ksek doÄŸruluk ve en iyi genelleme performansÄ±nÄ± saÄŸlamÄ±ÅŸ; **LeakyReLU** ise deploy aÃ§Ä±sÄ±ndan en dengeli alternatif olmuÅŸtur.

---

ğŸ“Œ Bu dizindeki `.ipynb` ve `models/` klasÃ¶rleri, yukarÄ±daki sonuÃ§larÄ±n **doÄŸrudan yeniden Ã¼retilebilir** kodlarÄ±nÄ± iÃ§erir.
