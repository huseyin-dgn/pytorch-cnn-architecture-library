{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb64aa2b",
   "metadata": {},
   "source": [
    "# SiLU / Swish — CNN & Object Detection için Uçtan Uca Notlar\n",
    "\n",
    "Bu defter, **SiLU (Sigmoid Linear Unit)** aktivasyonunu (diğer adıyla **Swish**) temelden ileriye anlatır.\n",
    "\n",
    "Özellikler:\n",
    "- CNN ve detection bağlamı (YOLO tarzı ağlarda neden yaygın)\n",
    "- Kod örnekleri **minimal**\n",
    "- Matematik **hafif**: sadece gerekli kadar (türev/ileri-geri akışı için sezgi)\n",
    "\n",
    "> Not: PyTorch’ta `nn.SiLU` kullanılır. “Swish” genelde aynı fonksiyona referans verir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2f87d",
   "metadata": {},
   "source": [
    "## 1) SiLU / Swish nedir?\n",
    "\n",
    "SiLU fonksiyonu:\n",
    "\n",
    "\\[\n",
    "\\mathrm{SiLU}(x) = x \\cdot \\sigma(x)\n",
    "\\]\n",
    "\n",
    "Burada \\(\\sigma(x)\\) sigmoid fonksiyonudur:\n",
    "\n",
    "\\[\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\]\n",
    "\n",
    "### Neden önemli?\n",
    "- ReLU gibi “keskin kapı” değil, **yumuşak (smooth)**\n",
    "- Negatif tarafta tamamen sıfırlamaz (ReLU gibi “dead” yapma ihtimali daha az)\n",
    "- Gradient akışı daha sürekli olabilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214ec3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x    : [-4.0, -2.0, -1.0, 0.0, 1.0, 2.0, 4.0]\n",
      "SiLU : [-0.07194484025239944, -0.23840583860874176, -0.2689414322376251, 0.0, 0.7310585975646973, 1.7615940570831299, 3.9280550479888916]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "act = nn.SiLU()  # Swish ile aynı kullanım (pratikte)\n",
    "x = torch.tensor([-4.0, -2.0, -1.0, 0.0, 1.0, 2.0, 4.0])\n",
    "print(\"x    :\", x.tolist())\n",
    "print(\"SiLU :\", act(x).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4dbbd",
   "metadata": {},
   "source": [
    "## 2) ReLU / LeakyReLU / SiLU farkı (sezgisel)\n",
    "\n",
    "- **ReLU:** negatifleri 0 yapar → hızlı ve basit ama bazı nöronlar “ölü” kalabilir.\n",
    "- **LeakyReLU:** negatifte küçük eğim bırakır → gradient tamamen kesilmez.\n",
    "- **SiLU:** negatifte “yumuşak” şekilde bastırır, pozitifte lineere yaklaşır → daha smooth optimizasyon.\n",
    "\n",
    "SiLU’nun en önemli karakteri:\n",
    "- **x negatifken** çıktı negatif ama küçük (tam sıfır değil)\n",
    "- **x pozitifken** çıktı yaklaşık lineer (≈ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a8345",
   "metadata": {},
   "source": [
    "## 3) SiLU neden object detection’da çok kullanılıyor?\n",
    "\n",
    "YOLO ailesi ve benzeri CNN tabanlı detector’larda SiLU’nun pratik avantajları:\n",
    "\n",
    "- Aktivasyon **smooth** olduğu için optimizasyon bazen daha stabil olur\n",
    "- Düşük seviyeli özelliklerde (kenar/texture) negatif değerlerin tamamen ölmemesi fayda verebilir\n",
    "- BN/GN ile birlikte iyi çalışır\n",
    "- Birçok modern backbone/neck tasarımında varsayılan aktivasyon haline geldi\n",
    "\n",
    "**Ama:** Her zaman kazanmaz.\n",
    "- Bazı görevlerde ReLU daha iyi olabilir\n",
    "- SiLU hesap olarak ReLU’dan pahalıdır (sigmoid içerir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4ef2e",
   "metadata": {},
   "source": [
    "## 4) Forward/backward sezgisi (çok hafif matematik)\n",
    "\n",
    "\\[\n",
    "\\mathrm{SiLU}(x) = x\\sigma(x)\n",
    "\\]\n",
    "\n",
    "Türev (bilmek faydalı):\n",
    "\\[\n",
    "\\frac{d}{dx}\\mathrm{SiLU}(x) = \\sigma(x) + x\\sigma(x)(1-\\sigma(x))\n",
    "\\]\n",
    "\n",
    "### Buradan çıkan pratik sonuç\n",
    "- Türev **her yerde** tanımlı ve süreklidir (ReLU gibi köşeli değil)\n",
    "- Negatif bölgede türev 0’a tam çakılmaz → gradient akışı daha “nazik” kesilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8840b11",
   "metadata": {},
   "source": [
    "## 5) Minimal blok: Conv → BN → SiLU\n",
    "\n",
    "CNN tarafında en klasik kullanım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e55496f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 16, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBNSiLU(nn.Module):\n",
    "    def __init__(self, cin, cout, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(cin, cout, k, stride=s, padding=p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(cout)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "x = torch.randn(2, 16, 32, 32)\n",
    "block = ConvBNSiLU(16, 32, s=2)\n",
    "y = block(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52694c16",
   "metadata": {},
   "source": [
    "## 6) SiLU vs ReLU: ne zaman hangisi?\n",
    "\n",
    "### SiLU tercih edilebilir:\n",
    "- YOLO tarzı detector backbone/neck’lerinde baseline olarak\n",
    "- Eğitim “hırçınsa” (loss dalgalanıyorsa)\n",
    "- Küçük objelerde/ince detaylarda özellik taşımayı iyileştirmek istiyorsan\n",
    "\n",
    "### ReLU tercih edilebilir:\n",
    "- Çok hızlı inference gerekiyorsa\n",
    "- Basit baseline veya mobil/edge optimizasyonu önemliyse\n",
    "- Quantization / deployment hattın çok kısıtlıysa\n",
    "\n",
    "### LeakyReLU arada bir çözümdür:\n",
    "- ReLU kadar basit\n",
    "- Negatifte gradient kesilmesini azaltır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ab08f",
   "metadata": {},
   "source": [
    "## 7) Pratik ablation planı\n",
    "\n",
    "SiLU eklerken:\n",
    "1. Aynı modelin ReLU / LeakyReLU / SiLU versiyonlarını hazırla\n",
    "2. Aynı seed, aynı dataloader, aynı epoch ile koştur\n",
    "3. Kıyas metrikleri:\n",
    "   - Test acc / mAP\n",
    "   - Test loss\n",
    "   - Eğitim stabilitesi (loss curve dalgalanması)\n",
    "   - Hız (iter/s veya inference latency)\n",
    "\n",
    "**Amaç:** “SiLU daha iyi mi?” değil, “benim senaryomda ne yapıyor?”u ölçmek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ec885",
   "metadata": {},
   "source": [
    "## 8) Kısa özet\n",
    "\n",
    "- SiLU/Swish = \\(x \\cdot \\sigma(x)\\)\n",
    "- Smooth bir aktivasyondur; gradient akışı daha süreklidir\n",
    "- Detection’da pratikte sık kullanılır\n",
    "- ReLU’dan daha pahalıdır ama bazen performans/stabilite getirir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
