{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c916d8a",
   "metadata": {},
   "source": [
    "# SiLU / Swish — Matematik, Türev ve Backprop \n",
    "\n",
    "Bu defter, **SiLU (Swish)** aktivasyonunun matematiğini **okunur** ve **neden–sonuç** ilişkisi kuracak şekilde anlatır.\n",
    "\n",
    "Kapsam:\n",
    "- Fonksiyon tanımı\n",
    "- Limit davranışı\n",
    "- Süreklilik ve türev\n",
    "- Backprop’ta gradyan akışı\n",
    "- ReLU / LeakyReLU ile matematiksel kıyas\n",
    "- Pratik çıkarımlar\n",
    "\n",
    "> Not: Bu defter “saf matematik” değil; **deep learning açısından gerekli matematik**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41af2cb",
   "metadata": {},
   "source": [
    "## 1) Tanım\n",
    "\n",
    "SiLU (Swish) fonksiyonu:\n",
    "\n",
    "\\[\n",
    "\\mathrm{SiLU}(x) = x \\cdot \\sigma(x)\n",
    "\\]\n",
    "\n",
    "Burada sigmoid:\n",
    "\n",
    "\\[\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\]\n",
    "\n",
    "SiLU iki parçalı değildir; **her yerde tek bir ifadeyle** tanımlanır. Bu yüzden:\n",
    "- süreklilik, türev, gradyan akışı konuları daha “temiz” çıkar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00b69a",
   "metadata": {},
   "source": [
    "## 2) Temel limit davranışı (sezgi)\n",
    "\n",
    "Sigmoid’in limitleri:\n",
    "\n",
    "\\[\n",
    "\\lim_{x \\to +\\infty} \\sigma(x) = 1,\n",
    "\\quad\n",
    "\\lim_{x \\to -\\infty} \\sigma(x) = 0\n",
    "\\]\n",
    "\n",
    "Bunları SiLU’ya uygularsak:\n",
    "\n",
    "- \\(x \\to +\\infty\\) iken:\n",
    "\\[\n",
    "\\mathrm{SiLU}(x) = x \\sigma(x) \\approx x \\cdot 1 = x\n",
    "\\]\n",
    "→ Pozitif büyük değerlerde **lineer** davranır (ReLU benzeri).\n",
    "\n",
    "- \\(x \\to -\\infty\\) iken:\n",
    "\\[\n",
    "\\mathrm{SiLU}(x) = x \\sigma(x) \\approx x \\cdot 0 = 0\n",
    "\\]\n",
    "→ Negatif büyük değerlerde **0’a yaklaşır** (ama ReLU gibi keskin değil)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24814c5",
   "metadata": {},
   "source": [
    "## 3) Süreklilik ve türevlenebilirlik\n",
    "\n",
    "- \\(\\sigma(x)\\) her yerde süreklidir ve türevlenebilir.\n",
    "- \\(x\\) her yerde süreklidir ve türevlenebilir.\n",
    "- Çarpımları olan \\(\\mathrm{SiLU}(x) = x\\sigma(x)\\) da **her yerde süreklidir ve türevlenebilir**.\n",
    "\n",
    "Bu, ReLU’ya göre büyük farktır:\n",
    "- ReLU’da \\(x=0\\) noktasında “köşe” vardır (klasik türev yok).\n",
    "- SiLU’da böyle bir köşe yoktur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e6ada",
   "metadata": {},
   "source": [
    "## 4) Türev: adım adım\n",
    "\n",
    "\\[\n",
    "\\mathrm{SiLU}(x) = x \\sigma(x)\n",
    "\\]\n",
    "\n",
    "Çarpım kuralı:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dx}\\mathrm{SiLU}(x)\n",
    "=\n",
    "\\frac{d}{dx}\\big(x\\sigma(x)\\big)\n",
    "=\n",
    "\\sigma(x) + x \\sigma'(x)\n",
    "\\]\n",
    "\n",
    "Sigmoid’in türevi:\n",
    "\n",
    "\\[\n",
    "\\sigma'(x) = \\sigma(x)\\big(1 - \\sigma(x)\\big)\n",
    "\\]\n",
    "\n",
    "Yerine koyarsak:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dx}\\mathrm{SiLU}(x)\n",
    "=\n",
    "\\sigma(x) + x\\sigma(x)\\big(1-\\sigma(x)\\big)\n",
    "\\]\n",
    "\n",
    "Bu en yaygın yazılıştır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfd13f",
   "metadata": {},
   "source": [
    "## 5) Türevin işareti ve “neden bazen negatif bölgede de öğrenir?”\n",
    "\n",
    "Türevi:\n",
    "\n",
    "\\[\n",
    "\\mathrm{SiLU}'(x) = \\sigma(x) + x\\sigma(x)(1-\\sigma(x))\n",
    "\\]\n",
    "\n",
    "Buradan şu çıkar:\n",
    "- \\(\\sigma(x) > 0\\) her yerde\n",
    "- İkinci terim \\(x\\)’e bağlı olduğu için negatif bölgede türev:\n",
    "  - **küçük ama sıfır olmayan** değerler alır\n",
    "  - hatta bazı aralıklarda türev negatif bile olabilir (Swish'in “non-monotonic” özelliği)\n",
    "\n",
    "Pratik yorum:\n",
    "- ReLU negatifte **tam 0 türev** ile gradyanı kesebilir.\n",
    "- SiLU negatifte gradyanı “yumuşak” şekilde azaltır, genelde tamamen öldürmez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdba77",
   "metadata": {},
   "source": [
    "## 6) Backprop: “Buradan dolayı bu oldu”\n",
    "\n",
    "Bir katmanda:\n",
    "\n",
    "\\[\n",
    "z = Wx + b\n",
    "\\quad,\\quad\n",
    "a = \\mathrm{SiLU}(z)\n",
    "\\quad,\\quad\n",
    "\\mathcal{L} = \\mathcal{L}(a)\n",
    "\\]\n",
    "\n",
    "Zincir kuralı:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "\\cdot\n",
    "\\frac{da}{dz}\n",
    "=\n",
    "\\delta \\cdot \\mathrm{SiLU}'(z)\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "\n",
    "\\[\n",
    "\\delta = \\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "\\]\n",
    "\n",
    "### Sonuç\n",
    "- Gradyanın büyüklüğü doğrudan \\(\\mathrm{SiLU}'(z)\\) ile ölçeklenir.\n",
    "- \\(\\mathrm{SiLU}'(z)\\) süreklidir → eğitimde “ani sıçramalar” daha az olabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdea2e0",
   "metadata": {},
   "source": [
    "## 7) ReLU / LeakyReLU / SiLU (matematiksel kıyas)\n",
    "\n",
    "### ReLU\n",
    "\\[\n",
    "\\mathrm{ReLU}(x)=\\max(0,x)\n",
    "\\]\n",
    "- Parçalı\n",
    "- \\(x=0\\) noktasında klasik türev yok\n",
    "- Negatifte türev 0 → gradyan kesilebilir\n",
    "\n",
    "### LeakyReLU\n",
    "\\[\n",
    "\\mathrm{LReLU}(x)=\\begin{cases}\n",
    "x, & x\\ge 0\\\\\n",
    "\\alpha x, & x<0\n",
    "\\end{cases}\n",
    "\\]\n",
    "- Parçalı\n",
    "- Negatifte türev \\(\\alpha\\) → gradyan tamamen kesilmez\n",
    "- \\(\\alpha\\) sabittir\n",
    "\n",
    "### SiLU\n",
    "\\[\n",
    "\\mathrm{SiLU}(x)=x\\sigma(x)\n",
    "\\]\n",
    "- Tek ifade\n",
    "- Her yerde türevlenebilir (smooth)\n",
    "- Negatifte gradyan “yumuşak” azalır\n",
    "- Sabit bir eğim değil, **x’e bağlı** bir ölçekleme vardır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685a3a0",
   "metadata": {},
   "source": [
    "## 8) Pratik çıkarımlar (matematikten doğan)\n",
    "\n",
    "Matematik şunu söylüyor:\n",
    "\n",
    "- SiLU’nun türevi **sürekli** → optimizasyonda “kırılma” daha az\n",
    "- Negatif bölgede gradyan genelde tamamen sıfırlanmaz → dead ReLU riskini azaltabilir\n",
    "- Pozitif büyük değerlerde lineere yaklaşır → güçlü aktivasyon taşır\n",
    "- Ancak sigmoid içerdiği için hesaplama maliyeti ReLU’dan yüksektir\n",
    "\n",
    "Bu yüzden SiLU:\n",
    "- modern CNN/detection mimarilerinde sık tercih edilir\n",
    "- ama mobil/edge gibi hız kritik yerlerde ReLU/LeakyReLU seçilebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b395b3",
   "metadata": {},
   "source": [
    "## 9) Mini doğrulama (türevi otomatik türev ile kontrol)\n",
    "\n",
    "Aşağıdaki küçük demo, türev formülünün autograd ile uyumlu olduğunu gösterir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243f04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x             : [-3.0, -1.0, 0.0, 1.0, 3.0]\n",
      "autograd grad : [-0.08810410648584366, 0.07232949137687683, 0.5, 0.9276705384254456, 1.0881041288375854]\n",
      "analytic grad : [-0.08810412138700485, 0.07232949137687683, 0.5, 0.9276705384254456, 1.0881041288375854]\n",
      "max abs diff  : 1.4901161193847656e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def silu(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "x = torch.tensor([-3.0, -1.0, 0.0, 1.0, 3.0], requires_grad=True)\n",
    "y = silu(x).sum()\n",
    "y.backward()\n",
    "autograd_grad = x.grad.detach().clone()\n",
    "\n",
    "# Analitik türev: σ(x) + xσ(x)(1-σ(x))\n",
    "x2 = x.detach()\n",
    "sigma = torch.sigmoid(x2)\n",
    "analytic_grad = sigma + x2 * sigma * (1 - sigma)\n",
    "\n",
    "print(\"x             :\", x2.tolist())\n",
    "print(\"autograd grad :\", autograd_grad.tolist())\n",
    "print(\"analytic grad :\", analytic_grad.tolist())\n",
    "print(\"max abs diff  :\", (autograd_grad - analytic_grad).abs().max().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
