{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56dd9da0",
   "metadata": {},
   "source": [
    "# PReLU — Matematiksel Temel, Türev ve Backprop Yorumu\n",
    "\n",
    "Bu defter, **PReLU (Parametric ReLU)** aktivasyonunun matematiğini **okunur ve neden–sonuç odaklı** biçimde ele alır.\n",
    "\n",
    "Kapsam:\n",
    "- Parçalı tanım\n",
    "- Süreklilik ve türev\n",
    "- Backprop’ta gradyan akışı\n",
    "- Öğrenilebilir parametrenin (a) etkisi\n",
    "- ReLU / LeakyReLU ile matematiksel fark\n",
    "- Pratik çıkarımlar\n",
    "\n",
    "Amaç: Formül ezberi değil, **“buradan dolayı bu oluyor”** ilişkisini kurmak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dbd04f",
   "metadata": {},
   "source": [
    "## 1) Tanım\n",
    "\n",
    "PReLU, ReLU ailesine ait **parçalı doğrusal** bir aktivasyon fonksiyonudur.\n",
    "\n",
    "\\[\n",
    "\\mathrm{PReLU}(x) =\n",
    "\\begin{cases}\n",
    "x, & x \\ge 0 \\\\\n",
    "a \\, x, & x < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "- \\(x\\): ön-aktivasyon (pre-activation)\n",
    "- \\(a\\): **öğrenilebilir** negatif eğim parametresi\n",
    "\n",
    "Önemli fark:\n",
    "- ReLU: \\(a = 0\\)\n",
    "- LeakyReLU: \\(a = \\text{sabit}\\)\n",
    "- **PReLU:** \\(a\\) eğitim sırasında **öğrenilir**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5046ed",
   "metadata": {},
   "source": [
    "## 2) Süreklilik\n",
    "\n",
    "Fonksiyon her noktada süreklidir.\n",
    "\n",
    "Özellikle \\(x = 0\\) noktasında:\n",
    "\n",
    "- Sol limit: \\(\\lim_{x \\to 0^-} a x = 0\\)\n",
    "- Sağ limit: \\(\\lim_{x \\to 0^+} x = 0\\)\n",
    "\n",
    "İki parça aynı değeri verdiği için PReLU,\n",
    "\\[\n",
    "x = 0 \\text{ noktasında süreklidir.}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69435e52",
   "metadata": {},
   "source": [
    "## 3) Türev (x ≠ 0)\n",
    "\n",
    "PReLU’nun türevi, parça parça şu şekildedir:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dx}\\mathrm{PReLU}(x) =\n",
    "\\begin{cases}\n",
    "1, & x > 0 \\\\\n",
    "a, & x < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "### x = 0 noktasında\n",
    "\n",
    "- Sol türev: \\(a\\)\n",
    "- Sağ türev: \\(1\\)\n",
    "\n",
    "Bu nedenle klasik anlamda tek bir türev yoktur.  \n",
    "Derin öğrenmede bu nokta **ölçüsü sıfır** olan bir küme olduğu için **subgradient** yaklaşımı kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15db83b",
   "metadata": {},
   "source": [
    "## 4) Backprop: gradyan neden kaybolmaz?\n",
    "\n",
    "Bir katmanda:\n",
    "\\[\n",
    "z = Wx + b, \\quad y = \\mathrm{PReLU}(z)\n",
    "\\]\n",
    "\n",
    "Kayıp fonksiyonu:\n",
    "\\[\n",
    "\\mathcal{L} = \\mathcal{L}(y)\n",
    "\\]\n",
    "\n",
    "Zincir kuralı:\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y}\n",
    "\\cdot\n",
    "\\frac{dy}{dz}\n",
    "=\n",
    "\\delta \\cdot \\mathrm{PReLU}'(z)\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "\\[\n",
    "\\delta = \\frac{\\partial \\mathcal{L}}{\\partial y}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916513ba",
   "metadata": {},
   "source": [
    "### Negatif ve pozitif bölgede gradyan\n",
    "\n",
    "- Eğer \\(z > 0\\):\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z} = \\delta\n",
    "\\]\n",
    "\n",
    "- Eğer \\(z < 0\\):\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z} = a \\, \\delta\n",
    "\\]\n",
    "\n",
    "### Buradan dolayı:\n",
    "\n",
    "- ReLU’da (\\(a=0\\)) negatif bölgede gradyan **tamamen sıfırlanır**\n",
    "- LeakyReLU’da gradyan **sabit bir katsayıyla** akar\n",
    "- **PReLU’da gradyan hem akar hem de büyüklüğü öğrenilebilir**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23234fc",
   "metadata": {},
   "source": [
    "## 5) Öğrenilebilir parametre a için gradyan\n",
    "\n",
    "PReLU’nun kritik farkı:  \n",
    "Kayıp fonksiyonu **a parametresine de bağlıdır**.\n",
    "\n",
    "Negatif bölgede (\\(z<0\\)):\n",
    "\n",
    "\\[\n",
    "y = a z\n",
    "\\]\n",
    "\n",
    "Bu durumda:\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y}\n",
    "\\cdot\n",
    "\\frac{\\partial y}{\\partial a}\n",
    "=\n",
    "\\delta \\cdot z\n",
    "\\]\n",
    "\n",
    "### Yorum\n",
    "\n",
    "- Eğer \\(z < 0\\) ve \\(\\delta\\) büyükse → \\(a\\) güçlü şekilde güncellenir\n",
    "- Model, negatif tarafta **ne kadar bilgi geçirmek gerektiğini** veriden öğrenir\n",
    "\n",
    "Bu, LeakyReLU’da **olmayan** bir serbestliktir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4ed69",
   "metadata": {},
   "source": [
    "## 6) Kanal başına PReLU (Channel-wise) matematiği\n",
    "\n",
    "Eğer her kanal için ayrı \\(a_c\\) parametresi kullanılırsa:\n",
    "\n",
    "\\[\n",
    "y_c =\n",
    "\\begin{cases}\n",
    "z_c, & z_c \\ge 0 \\\\\n",
    "a_c z_c, & z_c < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Her kanalın gradyanı **kendi dağılımına göre** şekillenir.\n",
    "\n",
    "### Sonuç:\n",
    "- Daha yüksek temsil gücü\n",
    "- Ama daha fazla parametre ve overfitting riski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60140c",
   "metadata": {},
   "source": [
    "## 7) Ölçek ve optimizasyon yorumu\n",
    "\n",
    "Negatif tarafta eğimin öğrenilebilir olması şu anlama gelir:\n",
    "\n",
    "- Model, bazı katmanlarda ReLU’ya yakın davranabilir (\\(a \\to 0\\))\n",
    "- Bazı katmanlarda daha lineer davranabilir (\\(a\\) büyür)\n",
    "\n",
    "Bu durum:\n",
    "- Optimizasyon sürecini daha **esnek**\n",
    "- Ama aynı zamanda daha **hassas**\n",
    "hale getirir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df70910",
   "metadata": {},
   "source": [
    "## 8) ReLU – LeakyReLU – PReLU (matematiksel özet)\n",
    "\n",
    "| Aktivasyon | Negatif eğim | Öğrenilebilir mi | Gradyan |\n",
    "|-----------|--------------|------------------|---------|\n",
    "| ReLU | 0 | ❌ | Negatifte 0 |\n",
    "| LeakyReLU | Sabit \\(a\\) | ❌ | Negatifte \\(a\\delta\\) |\n",
    "| PReLU | \\(a\\) | ✅ | Negatifte öğrenilen \\(a\\delta\\) |\n",
    "\n",
    "PReLU, matematiksel olarak **en esnek**, ama en riskli seçenektir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfa9c9",
   "metadata": {},
   "source": [
    "## 9) Sonuç (matematikten çıkan pratik)\n",
    "\n",
    "Matematiksel analiz şunu gösterir:\n",
    "\n",
    "- PReLU, ReLU’nun gradyan kesilmesi problemini çözer\n",
    "- LeakyReLU’dan farklı olarak bu çözümü **veriye adapte eder**\n",
    "- Bu esneklik:\n",
    "  - derin ve karmaşık modellerde avantaj\n",
    "  - küçük veri / kısa eğitimde dezavantaj\n",
    "olabilir\n",
    "\n",
    "Bu yüzden PReLU:\n",
    "- backbone güçlendirme\n",
    "- research / ablation\n",
    "senaryolarında matematiksel olarak anlamlıdır."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
