{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa9aba5",
   "metadata": {},
   "source": [
    "# PReLU (CNN & Object Detection) — Uçtan Uca Notlar\n",
    "\n",
    "Bu defter, **PReLU’nun ne olduğunu** en temelden başlayıp pratikte **CNN ve object detection** tarafında doğru yerde nasıl kullanılacağını anlatır.  \n",
    "Kod örnekleri **minimal** tutulmuştur. Matematiğe **bilerek girmiyoruz** (ayrı defterde).\n",
    "\n",
    "> Hedef: “Ne zaman PReLU?”, “LeakyReLU’dan farkı ne?”, “Nerede iş yapar / nerede gereksiz?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b011a",
   "metadata": {},
   "source": [
    "## 1) ReLU ve LeakyReLU’yu 1 cümlede hatırlayalım\n",
    "\n",
    "- **ReLU:** negatif tarafta tamamen 0’a keser  \n",
    "- **LeakyReLU:** negatif tarafta küçük bir sızıntı (sabit eğim) bırakır: `negative_slope = α`\n",
    "\n",
    "Bu iki yaklaşımın ortak problemi:  \n",
    "**Negatif taraftaki davranış sabittir** (ya 0 ya da seçtiğin α)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d5832",
   "metadata": {},
   "source": [
    "## 2) PReLU fikri: “Negatif eğimi model öğrensin”\n",
    "\n",
    "PReLU = *Parametric ReLU*.\n",
    "\n",
    "LeakyReLU’da negatif eğim **senin verdiğin sabit** bir sayıdır.  \n",
    "PReLU’da negatif eğim **öğrenilebilir bir parametredir**.\n",
    "\n",
    "Bu şu anlama gelir:\n",
    "- Model, veriye göre “negatif tarafta ne kadar sızdıracağını” kendi ayarlar.\n",
    "- Bazı görevlerde LeakyReLU’dan daha esnek ve daha güçlü olabilir.\n",
    "\n",
    "Ama bedeli var:\n",
    "- Ek parametre (küçük ama var)\n",
    "- Overfitting riski (özellikle küçük dataset / çok parametre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353c2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x    : [-3.0, -1.0, 0.0, 1.0, 3.0]\n",
      "PReLU: [-0.75, -0.25, 0.0, 1.0, 3.0]\n",
      "learnable a: [0.25]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "prelu = nn.PReLU(num_parameters=1, init=0.25)  # tek parametre (shared)\n",
    "x = torch.tensor([-3.0, -1.0, 0.0, 1.0, 3.0])\n",
    "print(\"x    :\", x.tolist())\n",
    "print(\"PReLU:\", prelu(x).tolist())\n",
    "print(\"learnable a:\", prelu.weight.detach().cpu().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66557b28",
   "metadata": {},
   "source": [
    "## 3) PReLU parametresi: “a” nasıl çalışır?\n",
    "\n",
    "PyTorch’ta PReLU’nun öğrenilebilir parametresi `weight` olarak tutulur.  \n",
    "Bu parametre genelde “a” olarak anılır.\n",
    "\n",
    "- **a küçükse** → ReLU’ya yaklaşır (negatifleri daha çok bastırır)\n",
    "- **a büyükse** → negatiften daha çok bilgi geçirir (daha lineer)\n",
    "\n",
    "Önemli: PReLU’da `a` eğitim boyunca optimizer ile güncellenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bec1f",
   "metadata": {},
   "source": [
    "## 4) “Shared” mi “Channel-wise” mı?\n",
    "\n",
    "PReLU’nun iki yaygın kullanımı var:\n",
    "\n",
    "### A) Shared PReLU (tek parametre)\n",
    "- Tüm kanallar aynı `a` değerini paylaşır\n",
    "- Parametre sayısı çok az\n",
    "- Daha az overfit\n",
    "\n",
    "PyTorch:\n",
    "```python\n",
    "nn.PReLU(num_parameters=1)\n",
    "```\n",
    "\n",
    "### B) Channel-wise PReLU (kanal başına parametre)\n",
    "- Her kanalın ayrı `a` parametresi olur\n",
    "- Daha esnek (daha yüksek kapasite)\n",
    "- Parametre sayısı artar\n",
    "\n",
    "PyTorch:\n",
    "```python\n",
    "nn.PReLU(num_parameters=C)\n",
    "```\n",
    "\n",
    "CNN/backbone içinde çoğu zaman kanal-başı PReLU daha “mantıklı” görünür, ama veri küçükse shared tercih edilebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddd397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared weight shape : torch.Size([1])\n",
      "channel weight shape: torch.Size([16])\n",
      "output shapes: torch.Size([2, 16, 8, 8]) torch.Size([2, 16, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "C = 16\n",
    "x = torch.randn(2, C, 8, 8)\n",
    "\n",
    "prelu_shared = nn.PReLU(num_parameters=1, init=0.25)\n",
    "prelu_channel = nn.PReLU(num_parameters=C, init=0.25)\n",
    "\n",
    "y1 = prelu_shared(x)\n",
    "y2 = prelu_channel(x)\n",
    "\n",
    "print(\"shared weight shape :\", prelu_shared.weight.shape)\n",
    "print(\"channel weight shape:\", prelu_channel.weight.shape)\n",
    "print(\"output shapes:\", y1.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e5e33",
   "metadata": {},
   "source": [
    "## 5) CNN ve Object Detection’da nerede kullanılır?\n",
    "\n",
    "### Backbone (özellik çıkarıcı)\n",
    "PReLU backbone’da güçlü bir opsiyondur:\n",
    "- Derin ağlarda gradient akışını iyileştirebilir\n",
    "- Bazı mimarilerde doğruluğu artırabilir\n",
    "\n",
    "### Neck (FPN / PAN vb.)\n",
    "Kullanılır ama genelde “default” değildir.  \n",
    "Modern YOLO tarafında SiLU daha standarttır.  \n",
    "PReLU, daha çok “backbone kapasitesini artırma” denemelerinde öne çıkar.\n",
    "\n",
    "### Head\n",
    "Head’de kullanmak mümkün ama çoğu sistem:\n",
    "- hız/kararlılık için SiLU/LeakyReLU\n",
    "- veya baseline ReLU\n",
    "seçer.\n",
    "\n",
    "**Pratik kural:**\n",
    "- “Kapasite + esneklik” istiyorsan → PReLU (özellikle backbone)\n",
    "- “Hız + basitlik” istiyorsan → ReLU / LeakyReLU / SiLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe6ad1",
   "metadata": {},
   "source": [
    "## 6) PReLU ne zaman mantıklı?\n",
    "\n",
    "Aşağıdaki durumlarda PReLU denemek mantıklıdır:\n",
    "\n",
    "- Model derin ve karmaşıksa\n",
    "- Veride dağılım kaymaları varsa (domain shift)\n",
    "- ReLU/LeakyReLU ile yakınsama sorunları görüyorsan\n",
    "- Backbone tarafında “az ama anlamlı” iyileştirme arıyorsan\n",
    "\n",
    "Ve özellikle:\n",
    "- Ablation yapmayı planlıyorsan (aktivasyon değişimi tek başına)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471aa14",
   "metadata": {},
   "source": [
    "## 7) PReLU’nun riskleri (net)\n",
    "\n",
    "PReLU “bedava” değil:\n",
    "\n",
    "- Kanal başına PReLU, parametre sayısını artırır (küçük görünür ama vardır)\n",
    "- Küçük dataset’te overfit riskini artırabilir\n",
    "- Bazı deployment/quantization akışlarında ekstra dikkat ister\n",
    "- Bazen kazanım yoktur (ya da çok küçük olur)\n",
    "\n",
    "Bu yüzden PReLU genelde:\n",
    "- **Research/ablation**\n",
    "- **backbone güçlendirme**\n",
    "senaryolarında daha iyi bir seçimdir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b158f1",
   "metadata": {},
   "source": [
    "## 8) PyTorch’ta pratik kullanım (Conv + Norm + PReLU)\n",
    "\n",
    "Aşağıdaki blok CNN’de klasik formdur:\n",
    "**Conv → Norm → PReLU**\n",
    "\n",
    "Not: PReLU parametresi normal activation gibi “stateless” değildir; öğrenilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b6afcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 32, 32]) PReLU params: 32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBNPReLU(nn.Module):\n",
    "    def __init__(self, cin, cout, k=3, s=1, p=1, per_channel=True, init=0.25):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(cin, cout, k, stride=s, padding=p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(cout)\n",
    "        num_p = cout if per_channel else 1\n",
    "        self.act = nn.PReLU(num_parameters=num_p, init=init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "x = torch.randn(2, 16, 32, 32)\n",
    "block = ConvBNPReLU(16, 32, per_channel=True)\n",
    "y = block(x)\n",
    "print(y.shape, \"PReLU params:\", block.act.weight.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced5211",
   "metadata": {},
   "source": [
    "## 9) “PReLU gerçekten öğreniyor mu?” mini kontrol\n",
    "\n",
    "Aşağıdaki mini örnek, birkaç adımda `a` parametresinin değişebildiğini gösterir.\n",
    "Bu bir demo; gerçek eğitim gibi düşünme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a87f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a before: 0.25\n",
      "a after : 0.20293855667114258\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "prelu = nn.PReLU(num_parameters=1, init=0.25)\n",
    "lin = nn.Linear(4, 4)\n",
    "\n",
    "model = nn.Sequential(lin, prelu)\n",
    "opt = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "x = torch.randn(32, 4)\n",
    "target = torch.zeros_like(x)  # sadece demo\n",
    "\n",
    "print(\"a before:\", prelu.weight.item())\n",
    "\n",
    "for step in range(5):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    out = model(x)\n",
    "    loss = (out - target).pow(2).mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "print(\"a after :\", prelu.weight.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c9fa9",
   "metadata": {},
   "source": [
    "## 10) LeakyReLU ile karşılaştırma (tek paragraf)\n",
    "\n",
    "- **LeakyReLU:** negatif eğim sabit → basit, stabil, hızlı  \n",
    "- **PReLU:** negatif eğim öğrenilir → daha esnek, bazen daha iyi performans, ama overfit/karmaşıklık riski\n",
    "\n",
    "Bu yüzden repo tasarımında mantıklı ayrım:\n",
    "- LeakyReLU → **Core**\n",
    "- PReLU → **Core ama “learnable activation” notu** (veya “Core/Advanced” gibi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753a332",
   "metadata": {},
   "source": [
    "## 11) Mini kontrol listesi ✅\n",
    "\n",
    "PReLU eklerken:\n",
    "- [ ] shared mi channel-wise mı karar ver\n",
    "- [ ] init değerini belirle (genelde 0.25 iyi başlangıç)\n",
    "- [ ] aynı mimaride sadece aktivasyonu değiştirip ablation yap\n",
    "- [ ] küçük dataset’te overfit izlerini kontrol et\n",
    "- [ ] deployment/quantization planın varsa erkenden test et\n",
    "\n",
    "Bu kadar. Matematik ve türev/backprop kısmını ayrı defterde ele alacağız."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
