## âœ… Aktivasyon FonksiyonlarÄ± â€“ Kesin SonuÃ§ (15 Epoch)

### ğŸ“Š Final Epoch (15/15) SonuÃ§larÄ±

| Activation | Final Train Loss | Final Test Loss | Final Test Accuracy |
| ---------- | ---------------- | --------------- | ------------------- |
| ReLU       | 0.4770           | 0.6740          | 0.7797              |
| SiLU       | 0.4712           | 0.5892          | 0.8029              |
| LeakyReLU  | 0.5041           | 0.5645          | 0.8088              |
| **PReLU**  | **0.4614**       | **0.5573**      | **0.8149**          |

---

### ğŸ† En Ä°yi DeÄŸerler (TÃ¼m EÄŸitim Boyunca)

| Activation | Peak Accuracy | Peak Epoch   |
| ---------- | ------------- | ------------ |
| ReLU       | 0.7915        | Epoch 14     |
| SiLU       | 0.8102        | Epoch 14     |
| LeakyReLU  | 0.8088        | Epoch 15     |
| **PReLU**  | **0.8149**    | **Epoch 15** |

---

## ğŸ§  Kesin Yorum ve Karar

- **PReLU aÃ§Ä±k ara en iyi aktivasyon**:
  - En yÃ¼ksek **final accuracy**
  - En dÃ¼ÅŸÃ¼k **test loss**
  - EÄŸitim sonunda performans **dÃ¼ÅŸmÃ¼yor**, aksine gÃ¼Ã§leniyor
- **LeakyReLU**, parametresiz olmasÄ±na raÄŸmen oldukÃ§a gÃ¼Ã§lÃ¼ ve stabil:
  - Deploy / sade mimari iÃ§in iyi alternatif
- **SiLU**, modern ve gÃ¼Ã§lÃ¼ olsa da:
  - Bu deneyde **PReLU ve LeakyReLUâ€™nun gerisinde**
- **ReLU**, en zayÄ±f genel performansÄ± gÃ¶steriyor:
  - Finalde hem accuracy dÃ¼ÅŸÃ¼yor hem test loss yÃ¼kseliyor

---

## ğŸ”’ Kesin SonuÃ§ (Tek CÃ¼mle)

> **Bu deney koÅŸullarÄ±nda PReLU, hem doÄŸruluk hem genelleme aÃ§Ä±sÄ±ndan en iyi aktivasyon fonksiyonudur; LeakyReLU ikinci en iyi ve deploy aÃ§Ä±sÄ±ndan en dengeli alternatiftir.**

---

## ğŸ“Œ Proje / Repo iÃ§in Net KullanÄ±m KararÄ±

- **Default activation:** `PReLU`
- **Lightweight / deploy alternatifi:** `LeakyReLU`
- **ReLU:** âš ï¸ opsiyonel
- **SiLU:** âš ï¸ opsiyonel ama birincil tercih deÄŸil
