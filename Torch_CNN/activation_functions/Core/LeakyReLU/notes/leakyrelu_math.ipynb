{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58d51ec",
   "metadata": {},
   "source": [
    "# LeakyReLU — Matematiksel Temel, Türev, Backprop ve Pratik Sonuçlar\n",
    "\n",
    "Bu defter, **LeakyReLU** aktivasyonunun matematiğini uçtan uca ele alır:\n",
    "\n",
    "- Tanım (piecewise)\n",
    "- Türev / subgradient ve 0 noktasındaki durum\n",
    "- Backprop’ta gradyan akışı: “buradan dolayı şu oldu”\n",
    "- ReLU ile farkın teorik sonucu: *dying ReLU* ve gradyan sönmesi\n",
    "- Varyans/ölçek etkisi (init ve norm ile ilişkisi)\n",
    "- α (negative_slope) seçiminin etkileri\n",
    "- PReLU bağlantısı\n",
    "- Kısa doğrulama kodları (minimal)\n",
    "\n",
    "> Not: Matematik var, ama hedef “formül ezberi” değil; sonuçları **neden-sonuç** ilişkisiyle kurmak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210e454",
   "metadata": {},
   "source": [
    "## 1) Tanım\n",
    "\n",
    "LeakyReLU, negatif bölgede küçük bir eğim (sızıntı) bırakan piecewise lineer fonksiyondur.\n",
    "\n",
    "Bir parametre seçilir:  \n",
    "- \\( \\alpha \\in (0, 1) \\) genelde küçük (örn. 0.01 veya 0.1)\n",
    "\n",
    "Fonksiyon:\n",
    "\n",
    "\\[\n",
    "\\mathrm{LeakyReLU}_\\alpha(x) =\n",
    "\\begin{cases}\n",
    "x, & x \\ge 0 \\\\\n",
    "\\alpha x, & x < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "**ReLU** bunun özel halidir: \\( \\alpha = 0 \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8c931",
   "metadata": {},
   "source": [
    "## 2) Süreklilik ve türev (genel)\n",
    "\n",
    "LeakyReLU fonksiyonu **her noktada süreklidir**.  \n",
    "Özellikle \\( x = 0 \\) noktasında:\n",
    "\n",
    "- Sol limit: \\( \\lim_{x \\to 0^-} \\alpha x = 0 \\)\n",
    "- Sağ limit: \\( \\lim_{x \\to 0^+} x = 0 \\)\n",
    "\n",
    "İki parça da aynı değeri verdiği için fonksiyon **0 noktasında süreklidir**.\n",
    "\n",
    "\n",
    "Türev, \\( x = 0 \\) **hariç** olmak üzere açıkça tanımlıdır:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dx}\\mathrm{LeakyReLU}_\\alpha(x) =\n",
    "\\begin{cases}\n",
    "1, & x > 0 \\\\\n",
    "\\alpha, & x < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "### 0 noktasında ne olur?\n",
    "\n",
    "\\( x = 0 \\) noktasında sol türev \\( \\alpha \\), sağ türev ise \\( 1 \\)’dir.  \n",
    "Bu nedenle klasik anlamda **tek bir türev değeri yoktur**.\n",
    "\n",
    "Derin öğrenme pratiğinde bu durum problem oluşturmaz; çünkü:\n",
    "- \\( x = 0 \\) noktası **ölçüsü sıfır** olan bir kümedir\n",
    "- Sürekli dağılımlardan örneklenen verilerde tam olarak bu noktaya düşme olasılığı ihmal edilebilirdir\n",
    "\n",
    "Bu yüzden otomatik türev alan framework’ler (PyTorch, TensorFlow vb.)  \n",
    "\\( x = 0 \\) için **subgradient** yaklaşımı kullanır ve genellikle:\n",
    "- \\( 1 \\),\n",
    "- \\( \\alpha \\),\n",
    "- veya framework’e özgü deterministik bir değeri\n",
    "\n",
    "atanır. Bu seçim, eğitim sürecini pratikte etkilemez.\n",
    "``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e8613",
   "metadata": {},
   "source": [
    "## 3) Backprop: “Buradan dolayı bu oldu”\n",
    "\n",
    "Bir katmanda ileri yayılım şu şekildedir:\n",
    "\n",
    "- Ön-aktivasyon (pre-activation):  \n",
    "  \\[\n",
    "  z = W x + b\n",
    "  \\]\n",
    "\n",
    "- Aktivasyon:  \n",
    "  \\[\n",
    "  a = f(z), \\quad \\text{burada } f = \\mathrm{LeakyReLU}_\\alpha\n",
    "  \\]\n",
    "\n",
    "Kayıp fonksiyonu aktivasyon üzerinden tanımlıdır:\n",
    "\\[\n",
    "\\mathcal{L} = \\mathcal{L}(a)\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### Zincir kuralı\n",
    "\n",
    "Zincir kuralına göre, kaybın \\( z \\)’ye göre türevi:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "\\cdot\n",
    "\\frac{da}{dz}\n",
    "=\n",
    "\\delta \\cdot f'(z)\n",
    "\\]\n",
    "\n",
    "burada\n",
    "\\[\n",
    "\\delta = \\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "\\]\n",
    "olarak tanımlanır.\n",
    "\n",
    "\n",
    "### LeakyReLU için türev davranışı\n",
    "\n",
    "LeakyReLU’nun türevi parça parça tanımlıdır:\n",
    "\n",
    "- Eğer \\( z > 0 \\) ise:\n",
    "  \\[\n",
    "  f'(z) = 1\n",
    "  \\quad \\Rightarrow \\quad\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial z} = \\delta\n",
    "  \\]\n",
    "\n",
    "- Eğer \\( z < 0 \\) ise:\n",
    "  \\[\n",
    "  f'(z) = \\alpha\n",
    "  \\quad \\Rightarrow \\quad\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial z} = \\alpha \\, \\delta\n",
    "  \\]\n",
    "\n",
    "\n",
    "### Sonuç (neden–sonuç ilişkisi)\n",
    "\n",
    "- **ReLU**’da negatif bölgede \\( f'(z) = 0 \\) olduğu için:\n",
    "  \\[\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial z} = 0\n",
    "  \\]\n",
    "  Bu durumda negatif tarafta **gradyan akışı tamamen kesilir**.\n",
    "\n",
    "- **LeakyReLU**’da ise negatif bölgede \\( f'(z) = \\alpha > 0 \\) olduğundan:\n",
    "  \\[\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial z} = \\alpha \\, \\delta \\neq 0\n",
    "  \\]\n",
    "\n",
    "  Bu nedenle gradyan akışı **tamamen kaybolmaz**.  \n",
    "  Nöron negatif bölgede kalsa bile öğrenmeye devam eder; yalnızca gradyan **\\( \\alpha \\) katsayısı kadar zayıflar**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565182f",
   "metadata": {},
   "source": [
    "## 4) “Dying ReLU” matematiksel bakış\n",
    "\n",
    "ReLU’da bir nöronun pre-activation’ı uzun süre negatifte kalırsa:\n",
    "\n",
    "- Forward: çıktısı 0’a kilitlenir  \n",
    "- Backward: türev 0 olduğu için ağırlık gradyanı 0 olur\n",
    "\n",
    "Örneğin tek nöronlu durumda:\n",
    "\\[\n",
    "z = w^\\top x + b,\\quad a=\\max(0,z)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a}\n",
    "\\cdot\n",
    "\\frac{da}{dz}\n",
    "\\cdot\n",
    "\\frac{\\partial z}{\\partial w}\n",
    "=\n",
    "\\delta \\cdot \\mathbf{1}[z>0] \\cdot x\n",
    "\\]\n",
    "\n",
    "Eğer \\(z<0\\) ise \\(\\mathbf{1}[z>0]=0\\) ve gradyan **tam 0**.  \n",
    "Bu, nöronun ağırlıklarının “negatif taraftan geri dönmesini” zorlaştırır.\n",
    "\n",
    "LeakyReLU’da aynı ifade:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "=\n",
    "\\delta \\cdot\n",
    "\\begin{cases}\n",
    "1, & z>0 \\\\\n",
    "\\alpha, & z<0\n",
    "\\end{cases}\n",
    "\\cdot x\n",
    "\\]\n",
    "\n",
    "Buradan dolayı:\n",
    "- \\(z<0\\) iken bile \\(\\alpha \\delta x\\) gradyanı gelir  \n",
    "- nöron parametreleri güncellenir  \n",
    "- zamanla \\(z\\) dağılımı pozitife kayabilir (veya daha dengeli hale gelebilir)\n",
    "\n",
    "**Özet:** ReLU’da “negatifte kalırsan öğrenemezsin”; LeakyReLU’da “negatifte kalırsan daha yavaş öğrenirsin”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78776c07",
   "metadata": {},
   "source": [
    "## 5) Ölçek/variance etkisi (init ve dağılım)\n",
    "\n",
    "LeakyReLU lineer parçalardan oluştuğu için giriş dağılımının ölçeğini etkiler.\n",
    "\n",
    "Varsayım (yaklaşık analiz):\n",
    "- \\(z\\) simetrik bir dağılımdan geliyor (örn. ortalaması 0)\n",
    "\n",
    "LeakyReLU çıktısı:\n",
    "- pozitif yarıda \\(z\\)\n",
    "- negatif yarıda \\(\\alpha z\\)\n",
    "\n",
    "Bu durumda ikinci moment (kabaca):\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[f(z)^2]\n",
    "=\n",
    "\\mathbb{E}[z^2\\mathbf{1}[z>0]] + \\mathbb{E}[\\alpha^2 z^2\\mathbf{1}[z<0]]\n",
    "\\]\n",
    "\n",
    "Simetriden dolayı:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[z^2\\mathbf{1}[z>0]] \\approx \\frac{1}{2}\\mathbb{E}[z^2],\\quad\n",
    "\\mathbb{E}[z^2\\mathbf{1}[z<0]] \\approx \\frac{1}{2}\\mathbb{E}[z^2]\n",
    "\\]\n",
    "\n",
    "Dolayısıyla:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[f(z)^2]\n",
    "\\approx\n",
    "\\frac{1+\\alpha^2}{2}\\mathbb{E}[z^2]\n",
    "\\]\n",
    "\n",
    "### Buradan çıkan pratik sonuç\n",
    "\n",
    "- ReLU için \\(\\alpha=0\\): \\(\\frac{1+\\alpha^2}{2}=\\frac{1}{2}\\)  \n",
    "  ➜ aktivasyon ikinci momenti düşürür (ölçek küçülür)\n",
    "- LeakyReLU için \\(\\alpha>0\\): \\(\\frac{1+\\alpha^2}{2}\\) daha büyük  \n",
    "  ➜ ölçek “ReLU kadar” düşmeyebilir\n",
    "\n",
    "Bu, özellikle **He/Kaiming init** gibi yöntemlerin neden aktivasyon tipine göre ayarlandığını açıklar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3917aa",
   "metadata": {},
   "source": [
    "## 6) Kaiming/He init ile ilişki (neden α önemli)\n",
    "\n",
    "Kaiming init’in amacı: katmanlar arası sinyalin (ve gradyanın) **patlamadan/sönmeden** taşınması.\n",
    "\n",
    "LeakyReLU için Kaiming init genelde “a=α” parametresiyle kullanılır.  \n",
    "Çünkü aktivasyonun ikinci momenti ve efektif kazancı α’ya bağlıdır.\n",
    "\n",
    "PyTorch tarafında bu ilişki şuna karşılık gelir:\n",
    "- `nn.init.kaiming_normal_(..., a=negative_slope, nonlinearity=\"leaky_relu\")`\n",
    "\n",
    "**Buradan dolayı:** α değiştirirsen, init de “ideal” noktadan sapabilir.  \n",
    "(Pratikte BN/GN varsa bu etki azalır ama tamamen yok olmaz.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00038d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0010277541587129235 0.12382026761770248\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "w = torch.empty(128, 128)  # örnek ağırlık matrisi\n",
    "alpha = 0.1\n",
    "\n",
    "init.kaiming_normal_(w, a=alpha, nonlinearity=\"leaky_relu\")\n",
    "print(w.mean().item(), w.std().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cbfc8d",
   "metadata": {},
   "source": [
    "## 7) Lipschitz / gradyan üst sınırı (stabilite açısından)\n",
    "\n",
    "LeakyReLU piecewise lineer ve türevi \\(\\in \\{\\alpha, 1\\}\\).\n",
    "\n",
    "Bu, fonksiyonun Lipschitz sabitinin (en büyük türev büyüklüğü) yaklaşık **1** olmasını sağlar:\n",
    "\n",
    "\\[\n",
    "|f(x_1)-f(x_2)| \\le 1\\cdot|x_1-x_2|\n",
    "\\]\n",
    "\n",
    "**Buradan dolayı:**\n",
    "- Aktivasyon tek başına gradyanı büyütmez (maksimum çarpan 1)\n",
    "- Negatif tarafta gradyanı küçültür (çarpan α)\n",
    "- Bu da bazı eğitim senaryolarında daha “kontrollü” bir gradyan akışı sağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9732f5f",
   "metadata": {},
   "source": [
    "## 8) α (negative_slope) seçimi — teorik/pratik trade-off\n",
    "\n",
    "Negatif tarafta gradyan katsayısı α:\n",
    "\n",
    "- α küçükse (0.01):\n",
    "  - ReLU’ya çok benzer\n",
    "  - Dying ReLU riskini azaltır ama negatifte öğrenme yavaş\n",
    "\n",
    "- α büyükse (0.1, 0.2):\n",
    "  - Negatifte daha çok bilgi/gradient taşınır\n",
    "  - “Daha lineer” davranış artar (çok büyürse ReLU benzeri keskin ayrım azalır)\n",
    "\n",
    "**Buradan dolayı:** α büyütmek her zaman iyi değildir; kapasite/regularization dengesi değişir.\n",
    "\n",
    "Pratikte object detection’da 0.1 sık görülür; CNN genelinde 0.01 yaygındır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8cd02",
   "metadata": {},
   "source": [
    "## 9) LeakyReLU ↔ PReLU bağı\n",
    "\n",
    "PReLU şu fikri getirir:\n",
    "- α sabit olmasın, öğrenilsin.\n",
    "\n",
    "PReLU tanımı:\n",
    "\n",
    "\\[\n",
    "\\mathrm{PReLU}(x) =\n",
    "\\begin{cases}\n",
    "x, & x \\ge 0 \\\\\n",
    "a x, & x < 0\n",
    "\\end{cases}\n",
    "\\quad\\text{burada } a \\text{ öğrenilir}\n",
    "\\]\n",
    "\n",
    "**Buradan dolayı:**\n",
    "- Model negatif taraftaki “ne kadar sızdıracağını” veriye göre ayarlar\n",
    "- Ama:\n",
    "  - ekstra parametre\n",
    "  - overfit riski\n",
    "  - bazen küçük fayda / ek karmaşıklık\n",
    "\n",
    "LeakyReLU daha “sabit ve ucuz” bir çözümdür."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7559c",
   "metadata": {},
   "source": [
    "## 10) BatchNorm / GroupNorm ile birlikte neden etkisi bazen azalır?\n",
    "\n",
    "Norm katmanları (BN/GN), aktivasyon öncesi dağılımı düzenler:\n",
    "- ortalamayı/ölçeği stabilize eder\n",
    "- z değerlerinin sürekli aşırı negatifte kalmasını azaltabilir\n",
    "\n",
    "**Buradan dolayı:**\n",
    "- ReLU’nun dying riski bazı ağlarda zaten düşük kalabilir\n",
    "- LeakyReLU farkı küçülebilir\n",
    "\n",
    "Ama şu durumlarda LeakyReLU farkı tekrar görünür:\n",
    "- small batch (BN’nin zayıflaması)\n",
    "- domain shift\n",
    "- aşırı LR\n",
    "- agresif regularization / pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e108da",
   "metadata": {},
   "source": [
    "## 11) Backprop’ta bir “gradyan kapısı” olarak LeakyReLU\n",
    "\n",
    "Aktivasyonları “gate” gibi düşünürsen:\n",
    "\n",
    "- ReLU: negatifte kapı **tam kapanır**\n",
    "- LeakyReLU: negatifte kapı **kısmen açık kalır**\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z} = \\delta \\cdot g(z),\n",
    "\\quad\n",
    "g(z)\\in\\{\\alpha,1\\}\n",
    "\\]\n",
    "\n",
    "**Buradan dolayı:**\n",
    "- gradient flow daha sürekli hale gelir\n",
    "- bazı katmanlarda “öğrenme durması” daha az olur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ccf9e",
   "metadata": {},
   "source": [
    "## 12) Minimal doğrulama: türev davranışı (numerik kontrol)\n",
    "\n",
    "Aşağıdaki hücre:\n",
    "- autograd türevi ile\n",
    "- numerik fark türevini\n",
    "yaklaştırmalı karşılaştırır.\n",
    "\n",
    "(0 noktasına çok yaklaşınca parça geçişi nedeniyle dalgalanma normaldir.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affbcfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [-1.0, -0.20000000298023224, -9.999999974752427e-07, 0.0, 9.999999974752427e-07, 0.20000000298023224, 1.0]\n",
      "autograd: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 1.0, 1.0, 1.0]\n",
      "numgrad : 3.8504600524902344\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def num_grad(f, x, eps=1e-4):\n",
    "    return (f(x + eps) - f(x - eps)) / (2 * eps)\n",
    "\n",
    "alpha = 0.1\n",
    "x = torch.tensor([-1.0, -0.2, -1e-6, 0.0, 1e-6, 0.2, 1.0], requires_grad=True)\n",
    "\n",
    "y = F.leaky_relu(x, negative_slope=alpha).sum()\n",
    "y.backward()\n",
    "auto = x.grad.detach().clone()\n",
    "\n",
    "x2 = x.detach()\n",
    "ng = num_grad(lambda t: F.leaky_relu(t, negative_slope=alpha).sum(), x2)\n",
    "\n",
    "print(\"x:\", x2.tolist())\n",
    "print(\"autograd:\", auto.tolist())\n",
    "print(\"numgrad :\", ng.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca1bba",
   "metadata": {},
   "source": [
    "## 13) “Dying” etkisini oyuncak bir deneyle sezmek (mini)\n",
    "\n",
    "Aşağıdaki mini deney:\n",
    "- Rastgele lineer katman\n",
    "- ReLU vs LeakyReLU\n",
    "- Negatif bölgede kalan oranı ve gradyanların sıfırlanma oranını kaba şekilde gözler\n",
    "\n",
    "Bu, gerçek ağların yerine geçmez; sadece mekanizmayı görselleştirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a275832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatif oran (z<0): 0.499\n",
      "ReLU çıktı 0 oranı: 0.499\n",
      "ReLU'da gradyan tamamen kapanan oran: 0.499\n",
      "LeakyReLU'da gradyan tamamen kapanan oran: 0.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N, D = 4096, 64\n",
    "x = torch.randn(N, D)\n",
    "\n",
    "lin = nn.Linear(D, D, bias=True)\n",
    "\n",
    "z = lin(x)  # pre-activation\n",
    "\n",
    "relu = nn.ReLU()\n",
    "lrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "a_relu = relu(z)\n",
    "a_lrelu = lrelu(z)\n",
    "\n",
    "neg_ratio = (z < 0).float().mean().item()\n",
    "zero_ratio_relu = (a_relu == 0).float().mean().item()\n",
    "\n",
    "# \"gradyan kapısı\" oranını sezmek için: türev maskesi\n",
    "mask_relu = (z > 0).float()\n",
    "mask_lrelu = torch.where(z > 0, torch.ones_like(z), torch.full_like(z, 0.1))\n",
    "\n",
    "dead_grad_ratio_relu = (mask_relu == 0).float().mean().item()\n",
    "dead_grad_ratio_lrelu = (mask_lrelu == 0).float().mean().item()\n",
    "\n",
    "print(f\"Negatif oran (z<0): {neg_ratio:.3f}\")\n",
    "print(f\"ReLU çıktı 0 oranı: {zero_ratio_relu:.3f}\")\n",
    "print(f\"ReLU'da gradyan tamamen kapanan oran: {dead_grad_ratio_relu:.3f}\")\n",
    "print(f\"LeakyReLU'da gradyan tamamen kapanan oran: {dead_grad_ratio_lrelu:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd3574",
   "metadata": {},
   "source": [
    "## 14) Sonuçların özeti (maddeler halinde)\n",
    "\n",
    "- LeakyReLU, ReLU’nun “negatifte gradyanı sıfırlama” davranışını yumuşatır.\n",
    "- Türev negatifte α olduğu için gradyan akışı **kesilmez**, sadece ölçeklenir.\n",
    "- Bu, “dying ReLU” riskini teorik olarak düşürür:\n",
    "  - ReLU: negatifte tam kapı (0)\n",
    "  - LeakyReLU: negatifte kısmi kapı (α)\n",
    "- α büyüdükçe negatif bilgi taşınır; ama fonksiyon daha lineerleşir (trade-off).\n",
    "- Init (Kaiming/He) ve aktivasyon kazancı α’ya bağlıdır; α değişirse ideal init ayarı da değişir.\n",
    "- BN/GN gibi normlar varsa fark bazen küçülür, ama small-batch / domain shift gibi durumlarda tekrar değer kazanabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19e3b2",
   "metadata": {},
   "source": [
    "## 15) “Matematik defteri” sonrası pratik öneri\n",
    "\n",
    "Bu matematik kısmından çıkan pratik aksiyon:\n",
    "1. LeakyReLU kullanıyorsan α’yı sabitle (0.01 veya 0.1)  \n",
    "2. Kaiming init’de `a=α` kullan (eğer init’i manuel yapıyorsan)  \n",
    "3. Ablation ile (tek değişken aktivasyon) mAP + stability + latency ölç  \n",
    "4. Eğer model zaten SiLU ile güçlü baseline veriyorsa, LeakyReLU’yu “stabilite/hız” için tercih et"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
