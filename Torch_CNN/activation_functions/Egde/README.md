# Sigmoid / HardSigmoid & Swish / HardSwish

## KarÅŸÄ±laÅŸtÄ±rmalÄ± Deneyler â€“ Uygulama ve SonuÃ§ Ã–zeti

Bu Ã§alÄ±ÅŸma kapsamÄ±nda, CNN mimarilerinde sÄ±k kullanÄ±lan iki aktivasyon ailesi **aynÄ± koÅŸullar altÄ±nda deneysel olarak test edilmiÅŸtir**:

- **Sigmoid vs HardSigmoid** (gating / attention kullanÄ±mÄ±)
- **Swish (SiLU) vs HardSwish** (backbone activation kullanÄ±mÄ±)

AmaÃ§:

- Aktivasyon fonksiyonlarÄ±nÄ± teorik deÄŸil, **pratik performans Ã¼zerinden** deÄŸerlendirmek
- AynÄ± mimari ve eÄŸitim koÅŸullarÄ±nda **gerÃ§ek farklarÄ± Ã¶lÃ§mek**
- Backbone ve attention rollerini **deneysel veriye dayanarak ayÄ±rmak**

---

## Deney Kurulumu (Standart)

TÃ¼m karÅŸÄ±laÅŸtÄ±rmalar ÅŸu ÅŸekilde yapÄ±lmÄ±ÅŸtÄ±r:

- AynÄ± model mimarisi
- Sadece aktivasyon fonksiyonu deÄŸiÅŸtirilmiÅŸtir
- AynÄ± optimizer, learning rate ve scheduler
- AynÄ± dataset ve augmentasyon
- AynÄ± seed
- AynÄ± epoch sayÄ±sÄ±

Bu sayede gÃ¶zlemlenen farklar **doÄŸrudan aktivasyon fonksiyonuna** aittir.

---

## 1ï¸âƒ£ Swish (SiLU) vs HardSwish â€“ Backbone KarÅŸÄ±laÅŸtÄ±rmasÄ±

Bu deneyde:

- Swish ve HardSwish **ana aktivasyon** olarak kullanÄ±lmÄ±ÅŸ
- Modeller yan yana eÄŸitilmiÅŸ
- Epoch bazlÄ± loss ve accuracy loglanmÄ±ÅŸtÄ±r

### GÃ¶zlem

- Swish, tÃ¼m eÄŸitim boyunca **daha yÃ¼ksek test accuracy** Ã¼retmiÅŸtir
- HardSwish daha hÄ±zlÄ± yakÄ±nsasa da **final performansta geride kalmÄ±ÅŸtÄ±r**
- Fark kÃ¼Ã§Ã¼k ama **istikrarlÄ±**dÄ±r

### SonuÃ§

- **Accuracy Ã¶ncelikli senaryolarda Swish**
- **Mobil / edge / latency Ã¶ncelikli senaryolarda HardSwish**

Backbone iÃ§in iki aktivasyon da kullanÄ±labilir; seÃ§im **hedef platforma** baÄŸlÄ±dÄ±r.

---

## 2ï¸âƒ£ Sigmoid vs HardSigmoid â€“ Attention / Gating KarÅŸÄ±laÅŸtÄ±rmasÄ±

Bu deneylerde:

- Sigmoid ve HardSigmoid **backbone activation olarak deÄŸil**
- SE / attention benzeri **gating mekanizmalarÄ± iÃ§inde** kullanÄ±lmÄ±ÅŸtÄ±r
- AmaÃ§, feature Ã¼retmek deÄŸil **feature Ã¶lÃ§eklemek**tir

### GÃ¶zlem

- Her iki aktivasyon da gate Ã§Ä±ktÄ±sÄ±nÄ± `[0,1]` aralÄ±ÄŸÄ±nda Ã¼retmiÅŸtir
- HardSigmoid, sigmoidâ€™e Ã§ok yakÄ±n davranÄ±ÅŸ gÃ¶stermiÅŸtir
- Hesaplama maliyeti HardSigmoid tarafÄ±nda daha dÃ¼ÅŸÃ¼ktÃ¼r

### SonuÃ§

- Attention / gating iÃ§in **ikisinin de kullanÄ±mÄ± doÄŸrudur**
- Accuracy Ã¶ncelikli senaryolarda **Sigmoid**
- Mobil / verimlilik Ã¶ncelikli senaryolarda **HardSigmoid**

Bu aile **ana aktivasyon olarak kullanÄ±lmamÄ±ÅŸtÄ±r** ve kullanÄ±lmamaktadÄ±r.

---

## Net AyrÄ±m (Bu Repoda Benimsenen YaklaÅŸÄ±m)

| Rol                        | KullanÄ±lan Aktivasyon    |
| -------------------------- | ------------------------ |
| Backbone ana aktivasyon    | Swish (SiLU) / HardSwish |
| Attention / SE / Gate      | Sigmoid / HardSigmoid    |
| Output probability         | Sigmoid                  |
| Backboneâ€™da Sigmoid ailesi | âŒ kullanÄ±lmaz           |

---

## Genel SonuÃ§

Bu Ã§alÄ±ÅŸmalar gÃ¶stermiÅŸtir ki:

- Aktivasyon fonksiyonlarÄ± **tek baÅŸÄ±na iyi/kÃ¶tÃ¼ deÄŸildir**
- DoÄŸru sonuÃ§, **doÄŸru yerde kullanÄ±lan aktivasyondan** gelir
- Swish/HardSwish â†’ **temsil gÃ¼cÃ¼**
- Sigmoid/HardSigmoid â†’ **kontrol ve Ã¶lÃ§ekleme**

Bu repo, bu ayrÄ±mÄ± teorik anlatÄ±mla deÄŸil, **aynÄ± koÅŸullarda yapÄ±lan deneylerle** ortaya koymaktadÄ±r.

---

ğŸ“Œ TÃ¼m sonuÃ§lar:

- `.ipynb` dosyalarÄ±nda yeniden Ã¼retilebilir
- Loglar epoch bazÄ±nda kayÄ±t altÄ±ndadÄ±r
- Modeller aynÄ± mimariyi paylaÅŸmaktadÄ±r
