{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1b54c6",
   "metadata": {},
   "source": [
    "# HardSigmoid: Attention/Gating Ä°Ã§inde mi, Ana Aktivasyon Olarak mÄ±? (Net Karar Rehberi) ğŸ¯\n",
    "\n",
    "Bu notebook'un amacÄ± ÅŸu soruyu **net** cevaplamak:\n",
    "\n",
    "> **HardSigmoid CNNâ€™de ana aktivasyon (ReLU/SiLU gibi) mu olmalÄ±, yoksa SE/Attention gibi gating mekanizmalarÄ±nda mÄ± kullanÄ±lmalÄ±?**\n",
    "\n",
    "Burada:\n",
    "- HardSigmoidâ€™in matematiksel davranÄ±ÅŸÄ±nÄ± â€œkullanÄ±m baÄŸlamÄ±ylaâ€ iliÅŸkilendiriyoruz\n",
    "- Attention/gating iÃ§inde neden iyi bir seÃ§enek olabildiÄŸini gÃ¶steriyoruz\n",
    "- Ana aktivasyon olarak kullanÄ±ldÄ±ÄŸÄ±nda ne gibi riskler doÄŸurduÄŸunu aÃ§Ä±klÄ±yoruz\n",
    "- PyTorch ile kÃ¼Ã§Ã¼k ve anlaÅŸÄ±lÄ±r demoâ€™lar veriyoruz (SE/CBAM benzeri)\n",
    "\n",
    "**Ã–zet mesaj (ÅŸimdiden):**  \n",
    "âœ… HardSigmoid Ã§oÄŸunlukla **gating/attention** iÃ§in mantÄ±klÄ±dÄ±r.  \n",
    "âš ï¸ Ana aktivasyon olarak kullanÄ±mÄ± daha sÄ±nÄ±rlÄ± senaryolarda mantÄ±klÄ±dÄ±r (Ã¶zellikle edge/latency hedefi varsa).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ceade6",
   "metadata": {},
   "source": [
    "## 1) HardSigmoidâ€™in â€œdavranÄ±ÅŸ profiliâ€ (kullanÄ±m kararÄ± buradan Ã§Ä±kar)\n",
    "\n",
    "HardSigmoid tipik form:\n",
    "\n",
    "\\[\n",
    "\\mathrm{hardsigmoid}(x)=\\max(0,\\min(1,\\frac{x+3}{6}))\n",
    "\\]\n",
    "\n",
    "Bu ÅŸu demek:\n",
    "- **Ã‡Ä±kÄ±ÅŸ aralÄ±ÄŸÄ±:** \\([0,1]\\)  â†’ yani â€œkapÄ±â€ (gate) gibi davranÄ±r\n",
    "- **Orta bÃ¶lge gradyanÄ±:** sabit \\(1/6\\)\n",
    "- **UÃ§larda gradyan:** 0 (tam doygunluk)\n",
    "\n",
    "ğŸ‘‰ Bu Ã¼Ã§ madde bize ÅŸunu sÃ¶yler:\n",
    "- HardSigmoid, doÄŸal olarak **gating** iÃ§in uygundur (0â€“1 arasÄ±nda Ã§arpan Ã¼retir).\n",
    "- Ama ana aktivasyon olarak kullanÄ±rsan, negatif tarafta ve pozitif uÃ§larda **Ã§ok Ã§abuk doyar** â†’ temsil gÃ¼cÃ¼nÃ¼ kÄ±sÄ±tlayabilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f1dd1",
   "metadata": {},
   "source": [
    "## 2) â€œAna aktivasyonâ€ ile â€œgatingâ€ arasÄ±ndaki fark (bunu ayÄ±rmazsan yanlÄ±ÅŸ karar verirsin)\n",
    "\n",
    "### A) Ana aktivasyon (ReLU/SiLU/Swish/GELU gibi)\n",
    "Ana aktivasyonun gÃ¶revi:\n",
    "- Feature mapâ€™i **nonlinearity** ile zenginleÅŸtirmek\n",
    "- Temsil gÃ¼cÃ¼nÃ¼ artÄ±rmak\n",
    "- Gradyan akÄ±ÅŸÄ±nÄ± mÃ¼mkÃ¼n olduÄŸunca â€œÃ¶ldÃ¼rmemekâ€\n",
    "\n",
    "### B) Gating (SE/Attention iÃ§i)\n",
    "Gatingâ€™in gÃ¶revi:\n",
    "- Feature mapâ€™i **Ã¶lÃ§eklemek** (kanal veya uzamsal)\n",
    "- â€œNeye dikkat edeceÄŸiniâ€ belirlemek\n",
    "- Genelde **0â€“1** veya **0â€“1+** aralÄ±ÄŸÄ±nda Ã§arpan Ã¼retmek\n",
    "\n",
    "ğŸ‘‰ Bu yÃ¼zden gating tarafÄ±nda Sigmoid ailesi (sigmoid, hardsigmoid) zaten â€œdoÄŸalâ€ bir seÃ§imdir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16cdaf",
   "metadata": {},
   "source": [
    "## 3) HardSigmoid neden attention/gating iÃ§inde Ã§ok kullanÄ±lÄ±r?\n",
    "\n",
    "### 3.1 Ã‡arpan (multiplier) Ã¼retir\n",
    "SE gibi bloklarda tipik form:\n",
    "\\[\n",
    "y = x \\cdot g(x), \\quad g(x)\\in[0,1]\n",
    "\\]\n",
    "\n",
    "HardSigmoid, tam olarak bunu verir: \\(g(x)\\in[0,1]\\).\n",
    "\n",
    "### 3.2 Mobil/edge iÃ§in daha ucuz\n",
    "Sigmoid `exp` iÃ§erir. HardSigmoid parÃ§alÄ± doÄŸrusal olduÄŸu iÃ§in Ã§oÄŸu durumda daha ucuzdur.\n",
    "\n",
    "### 3.3 Gatingâ€™in hassasiyet ihtiyacÄ± sÄ±nÄ±rlÄ±dÄ±r\n",
    "Gatingâ€™de genellikle â€œÃ§ok inceâ€ karar deÄŸil, **yeterince iyi** bir Ã¶lÃ§ekleme istenir.\n",
    "ParÃ§alÄ± doÄŸrusal yaklaÅŸÄ±m pratikte yeterli olabilir.\n",
    "\n",
    "**SonuÃ§:**  \n",
    "HardSigmoid = **attention/gating iÃ§in klasik sigmoidâ€™e pratik alternatif**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668e979",
   "metadata": {},
   "source": [
    "## 4) HardSigmoid ana aktivasyon olarak ne zaman mantÄ±klÄ±?\n",
    "\n",
    "Åunu aÃ§Ä±k sÃ¶yleyeyim: **HardSigmoid ana aktivasyon iÃ§in default tercih deÄŸildir.**\n",
    "\n",
    "Ama bazÄ± Ã¶zel durumlarda mantÄ±klÄ± olabilir:\n",
    "\n",
    "### âœ… MantÄ±klÄ± olabileceÄŸi durumlar\n",
    "- Model tamamen **edge/mobil** hedefli ve activation maliyeti Ã§ok kritik\n",
    "- â€œFeatureâ€™larÄ± kapat-aÃ§â€ gibi kapÄ± davranÄ±ÅŸÄ± bilinÃ§li isteniyor\n",
    "- BaÅŸka yerlerde (Ã¶r. residual + norm) gradyan akÄ±ÅŸÄ± zaten gÃ¼Ã§lÃ¼ tutuluyor\n",
    "\n",
    "### âš ï¸ Riskler (ana aktivasyon olarak)\n",
    "- Ã‡Ä±kÄ±ÅŸÄ± \\([0,1]\\) aralÄ±ÄŸÄ±nda olduÄŸu iÃ§in aktivasyon sonrasÄ± dinamik aralÄ±k daralÄ±r\n",
    "- UÃ§larda gradyan 0 â†’ doygunluk hÄ±zlÄ±\n",
    "- ReLU/SiLU gibi â€œtemsil gÃ¼cÃ¼â€ saÄŸlayan aktivasyonlar yerine kullanmak genelde accuracy dÃ¼ÅŸÃ¼rebilir\n",
    "\n",
    "**Net karar:**  \n",
    "HardSigmoidâ€™i ana aktivasyon olarak deÄŸil, Ã§oÄŸunlukla **gating mekanizmalarÄ±nda** konumlandÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e760dfa",
   "metadata": {},
   "source": [
    "## 5) â€œKuralâ€ gibi dÃ¼ÅŸÃ¼n: Nerede ne kullanÄ±lÄ±r? (Net tablo)\n",
    "\n",
    "| BileÅŸen | AmaÃ§ | Uygun Aktivasyon | HardSigmoidâ€™in yeri |\n",
    "|---|---|---|---|\n",
    "| Backbone ana conv bloklarÄ± | temsil gÃ¼cÃ¼ / nonlinearity | ReLU, SiLU, GELU, HardSwish | âŒ genelde deÄŸil |\n",
    "| Lightweight backbone (mobil) | latency/efficiency | HardSwish, ReLU6, SiLU | âš ï¸ nadiren |\n",
    "| SE / Channel Attention | kanal Ã¶lÃ§ekleme (0â€“1) | Sigmoid, HardSigmoid | âœ… Ã§ok uygun |\n",
    "| Spatial Attention gate | uzamsal Ã¶lÃ§ekleme | Sigmoid, HardSigmoid | âœ… uygun |\n",
    "| Output head gate (multi-task) | skor/olasÄ±lÄ±k kapÄ±sÄ± | Sigmoid/HardSigmoid | âœ… uygun |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecdc856",
   "metadata": {},
   "source": [
    "## 6) Uygulama Demo 1: SE Gateâ€™de Sigmoid vs HardSigmoid\n",
    "\n",
    "Burada aynÄ± SE tarzÄ± gateâ€™i iki farklÄ± kapÄ± ile kuruyoruz:\n",
    "- Gate A: sigmoid\n",
    "- Gate B: hardsigmoid\n",
    "\n",
    "Sonra:\n",
    "- Ãœretilen gate daÄŸÄ±lÄ±mÄ±na bakÄ±yoruz\n",
    "- Ã‡Ä±ktÄ± farkÄ±nÄ± Ã¶lÃ§Ã¼yoruz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class SEGate(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, gate: str = \"sigmoid\"):\n",
    "        super().__init__()\n",
    "        hidden = max(4, channels // reduction)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "        if gate == \"sigmoid\":\n",
    "            self.gate = torch.sigmoid\n",
    "        elif gate == \"hardsigmoid\":\n",
    "            self.gate = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate must be 'sigmoid' or 'hardsigmoid'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.pool(x)\n",
    "        w = self.act(self.fc1(w))\n",
    "        w = self.gate(self.fc2(w))\n",
    "        return x * w, w\n",
    "\n",
    "x = torch.randn(4, 64, 32, 32)\n",
    "\n",
    "g_sig = SEGate(64, gate=\"sigmoid\")\n",
    "g_hsig = SEGate(64, gate=\"hardsigmoid\")\n",
    "\n",
    "y_sig, w_sig = g_sig(x)\n",
    "y_hsig, w_hsig = g_hsig(x)\n",
    "\n",
    "print(\"w_sig range:\", float(w_sig.min()), float(w_sig.max()))\n",
    "print(\"w_hsig range:\", float(w_hsig.min()), float(w_hsig.max()))\n",
    "print(\"mean abs diff output:\", float((y_sig - y_hsig).abs().mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8020b",
   "metadata": {},
   "source": [
    "### Gate daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶rselleÅŸtirelim\n",
    "Gate Ã§Ä±ktÄ±larÄ± \\( [0,1] \\) aralÄ±ÄŸÄ±nda ama daÄŸÄ±lÄ±m ÅŸekli farklÄ± olabilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ws = w_sig.detach().flatten().cpu().numpy()\n",
    "wh = w_hsig.detach().flatten().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(ws, bins=40, alpha=0.6, label=\"sigmoid gate\")\n",
    "plt.hist(wh, bins=40, alpha=0.6, label=\"hardsigmoid gate\")\n",
    "plt.title(\"SE Gate Weight Distribution\")\n",
    "plt.xlabel(\"gate value\"); plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207010d",
   "metadata": {},
   "source": [
    "## 7) Uygulama Demo 2: AynÄ± Attention bloÄŸu iÃ§inde hÄ±z (kaba)\n",
    "\n",
    "Bu â€œbenchmarkâ€ deÄŸildir ama fikir verir.\n",
    "HardSigmoid genelde sigmoidâ€™den daha ucuzdur (exp yok).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def bench(fn, x, iters=2000, warmup=200):\n",
    "    for _ in range(warmup):\n",
    "        fn(x)\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        fn(x)\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters\n",
    "\n",
    "z = torch.randn(1024, 1024)\n",
    "t_sig = bench(torch.sigmoid, z)\n",
    "t_hsig = bench(F.hardsigmoid, z)\n",
    "\n",
    "print(f\"avg time/iter sigmoid:     {t_sig*1e3:.3f} ms\")\n",
    "print(f\"avg time/iter hardsigmoid: {t_hsig*1e3:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98093768",
   "metadata": {},
   "source": [
    "## 8) â€œAna aktivasyon mu?â€ sorusuna pratik cevap (Ã§ok net)\n",
    "\n",
    "### Benim Ã¶nerim (standart CNN/ResNet/YOLO vb.)\n",
    "- Backbone aktivasyonu: **SiLU / ReLU / HardSwish**\n",
    "- Attention gate: **Sigmoid veya HardSigmoid**\n",
    "- Output probability gate (multi-label): **Sigmoid**\n",
    "- Edge hedefin varsa attention gateâ€™te **HardSigmoid** tercih edebilirsin\n",
    "\n",
    "### HardSigmoidâ€™i ana aktivasyon yapacaksan:\n",
    "Bunu bilinÃ§li yap:\n",
    "- â€œBen mobil hedefliyorum, temsil gÃ¼cÃ¼nden biraz feragat ediyorumâ€ diyeceksin.\n",
    "- Ve mutlaka kÄ±yas yapacaksÄ±n (sen zaten bunu yapÄ±yorsun).\n",
    "\n",
    "**KÄ±sa kural:**  \n",
    "HardSigmoid = **gate**  \n",
    "SiLU/HardSwish/ReLU = **main activation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39822f0f",
   "metadata": {},
   "source": [
    "## 9) CNN iÃ§inde Ã¶rnek yerleÅŸim ÅŸemasÄ± (net)\n",
    "\n",
    "```\n",
    "Conv -> BN -> SiLU\n",
    "Conv -> BN -> SiLU\n",
    "Residual Add\n",
    "â†“\n",
    "SE / Channel Attention:\n",
    "    GAP -> 1x1 -> act -> 1x1 -> HardSigmoid  (gate)\n",
    "    x * gate\n",
    "â†“\n",
    "Next block...\n",
    "```\n",
    "\n",
    "Burada HardSigmoid'in rolÃ¼:\n",
    "- Feature mapâ€™i â€œyeniden ÅŸekillendirmekâ€ deÄŸil\n",
    "- Feature mapâ€™in bazÄ± kanallarÄ±nÄ± **kÄ±sma/aÃ§ma** (Ã¶lÃ§ekleme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a8c0e",
   "metadata": {},
   "source": [
    "## 10) SonuÃ§ (tek paragraf)\n",
    "\n",
    "HardSigmoid, Ã§Ä±ktÄ±sÄ± **0â€“1 aralÄ±ÄŸÄ±nda** olan ve orta bÃ¶lgede **sabit gradyan** veren parÃ§alÄ± doÄŸrusal bir fonksiyondur. Bu profil onu, SE/CBAM/Coordinate Attention gibi mekanizmalarda **gating** iÃ§in doÄŸal bir aday yapar: Ã§Ã¼nkÃ¼ attention bloÄŸunun temel iÅŸi featureâ€™larÄ± **Ã¶lÃ§eklemek** ve â€œkapÄ±â€ Ã¼retmektir. Buna karÅŸÄ±lÄ±k, backboneâ€™un ana aktivasyonu temsil gÃ¼cÃ¼ ve gradyan akÄ±ÅŸÄ± aÃ§Ä±sÄ±ndan daha talepkardÄ±r; HardSigmoidâ€™in hÄ±zlÄ± doygunluÄŸu ve dar Ã§Ä±ktÄ± aralÄ±ÄŸÄ± nedeniyle ana aktivasyon olarak kullanÄ±mÄ± genelde daha risklidir. Bu yÃ¼zden pratikte en doÄŸru yerleÅŸim: **backboneâ€™da SiLU/HardSwish/ReLU**, attention gateâ€™te ise **Sigmoid veya HardSigmoid**â€™dir.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
