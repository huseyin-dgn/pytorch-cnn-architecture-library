{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c64cc17",
   "metadata": {},
   "source": [
    "# HardSwish (Hardswish) â€” BaÅŸtan Sona, UygulamalÄ± Rehber âš¡\n",
    "\n",
    "Bu notebookâ€™ta **HardSwish** aktivasyon fonksiyonunu uÃ§tan uca Ã¶ÄŸreneceksin:\n",
    "\n",
    "- HardSwish nedir, Swish ile farkÄ± ne?\n",
    "- Matematiksel tanÄ±m (parÃ§alÄ± yapÄ±)\n",
    "- TÃ¼rev / gradyan davranÄ±ÅŸÄ±\n",
    "- HardSigmoid ile baÄŸlantÄ± (HardSwish = x * HardSigmoid(x))\n",
    "- PyTorch kullanÄ±mÄ± (`nn.Hardswish`, `F.hardswish`)\n",
    "- Sigmoid/Swish/HardSwish eÄŸri ve tÃ¼rev karÅŸÄ±laÅŸtÄ±rmalarÄ±\n",
    "- Basit hÄ±z (kaba) kÄ±yasÄ±\n",
    "- KÃ¼Ã§Ã¼k bir toy eÄŸitim deneyi\n",
    "\n",
    "> Hedef: â€œEzberâ€ deÄŸil, **nerede mantÄ±klÄ±** olduÄŸunu anlamak.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c708f71",
   "metadata": {},
   "source": [
    "## 1) Swish nedir? HardSwish neden Ã§Ä±ktÄ±?\n",
    "\n",
    "**Swish** genelde ÅŸu ÅŸekilde tanÄ±mlanÄ±r:\n",
    "\n",
    "\\[\n",
    "\\mathrm{swish}(x) = x \\cdot \\sigma(x)\n",
    "\\]\n",
    "\n",
    "Burada \\(\\sigma(x)\\) sigmoidâ€™dir.\n",
    "\n",
    "Swishâ€™in avantajÄ±:\n",
    "- ReLUâ€™dan daha â€œyumuÅŸakâ€ bir geÃ§iÅŸ saÄŸlar\n",
    "- Negatif bÃ¶lgede tamamen Ã¶lmez\n",
    "- BazÄ± mimarilerde daha iyi doÄŸruluk verebilir\n",
    "\n",
    "Swishâ€™in dezavantajÄ±:\n",
    "- \\(\\sigma(x)\\) iÃ§in `exp` gerekir â†’ **hesap maliyeti**\n",
    "- Mobil/edge cihazlarda latency aÃ§Ä±sÄ±ndan pahalÄ± olabilir\n",
    "\n",
    "**HardSwish**, Swishâ€™in mobil hedeflerle uyumlu **daha ucuz** bir yaklaÅŸÄ±mÄ±dÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8931ad9",
   "metadata": {},
   "source": [
    "## 2) HardSwishâ€™in matematiksel tanÄ±mÄ±\n",
    "\n",
    "HardSwish genellikle ÅŸu ÅŸekilde tanÄ±mlanÄ±r:\n",
    "\n",
    "\\[\n",
    "\\mathrm{hardswish}(x) = x \\cdot \\mathrm{hardsigmoid}(x)\n",
    "\\]\n",
    "\n",
    "HardSigmoidâ€™in sÄ±k kullanÄ±lan formu:\n",
    "\n",
    "\\[\n",
    "\\mathrm{hardsigmoid}(x)=\\max(0, \\min(1, \\frac{x+3}{6}))\n",
    "\\]\n",
    "\n",
    "DolayÄ±sÄ±yla:\n",
    "\n",
    "\\[\n",
    "\\mathrm{hardswish}(x)=x\\cdot \\max(0, \\min(1, \\frac{x+3}{6}))\n",
    "\\]\n",
    "\n",
    "ParÃ§alÄ± (piecewise) olarak yazarsak:\n",
    "\n",
    "\\[\n",
    "\\mathrm{hardswish}(x)=\n",
    "\\begin{cases}\n",
    "0, & x \\le -3 \\\\\n",
    "x\\cdot\\frac{x+3}{6}=\\frac{x^2+3x}{6}, & -3 < x < 3 \\\\\n",
    "x, & x \\ge 3\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "> Bu form Ã§ok Ã¶nemli: Orta bÃ¶lgede **kuadratik**, uÃ§larda **doÄŸrusal/sÄ±fÄ±r**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bd650",
   "metadata": {},
   "source": [
    "## 3) TÃ¼rev (Gradient) analizi\n",
    "\n",
    "ParÃ§alÄ± tÃ¼rev:\n",
    "\n",
    "- \\(x \\le -3\\): \\(f(x)=0\\) â‡’ \\(f'(x)=0\\)\n",
    "- \\(-3 < x < 3\\): \\(f(x)=\\frac{x^2+3x}{6}\\) â‡’\n",
    "\n",
    "\\[\n",
    "f'(x)=\\frac{2x+3}{6}\n",
    "\\]\n",
    "\n",
    "- \\(x \\ge 3\\): \\(f(x)=x\\) â‡’ \\(f'(x)=1\\)\n",
    "\n",
    "Yani:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dx}\\mathrm{hardswish}(x)=\n",
    "\\begin{cases}\n",
    "0, & x < -3 \\\\\n",
    "\\frac{2x+3}{6}, & -3 < x < 3 \\\\\n",
    "1, & x > 3\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Not:\n",
    "- \\(x=-3\\) ve \\(x=3\\) noktalarÄ±nda tÃ¼rev â€œkeskin kÄ±rÄ±lÄ±râ€ (tam tÃ¼revlenebilir deÄŸil).\n",
    "- Backpropâ€™ta pratikte sorun olmaz (bu noktalar Ã¶lÃ§Ã¼ sÄ±fÄ±r).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea7d8a",
   "metadata": {},
   "source": [
    "## 4) ReLU / Swish / HardSwish â€” sezgisel kÄ±yas\n",
    "\n",
    "- **ReLU(x)=max(0,x)**: Negatifte sÄ±fÄ±r â†’ bazÄ± nÃ¶ronlar â€œÃ¶lÃ¼râ€.\n",
    "- **Swish**: Negatifte tamamen Ã¶lmez, yumuÅŸak, ama daha pahalÄ±.\n",
    "- **HardSwish**: Swishâ€™e benzer davranÄ±ÅŸÄ± **daha ucuz** ÅŸekilde taklit eder.\n",
    "\n",
    "HardSwishâ€™in tipik kullanÄ±m nedeni:\n",
    "- Mobil mimariler (MobileNetV3 gibi) iÃ§in **accuracy-per-latency** dengesi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1808af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AralÄ±k\n",
    "x = torch.linspace(-10, 10, steps=2000)\n",
    "\n",
    "relu = torch.relu(x)\n",
    "sig = torch.sigmoid(x)\n",
    "swish = x * sig\n",
    "hsig = F.hardsigmoid(x)\n",
    "hswish = F.hardswish(x)\n",
    "\n",
    "# TÃ¼revleri autograd ile\n",
    "xg = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "relu_g = torch.relu(xg)\n",
    "swish_g = xg * torch.sigmoid(xg)\n",
    "hswish_g = F.hardswish(xg)\n",
    "\n",
    "relu_g.sum().backward(retain_graph=True)\n",
    "drelu = xg.grad.clone().detach()\n",
    "xg.grad.zero_()\n",
    "\n",
    "swish_g.sum().backward(retain_graph=True)\n",
    "dswish = xg.grad.clone().detach()\n",
    "xg.grad.zero_()\n",
    "\n",
    "hswish_g.sum().backward()\n",
    "dhswish = xg.grad.clone().detach()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(x.numpy(), relu.numpy(), label=\"ReLU\")\n",
    "plt.plot(x.numpy(), swish.numpy(), label=\"Swish = x*sigmoid(x)\")\n",
    "plt.plot(x.numpy(), hswish.numpy(), label=\"HardSwish = x*hardsigmoid(x)\")\n",
    "plt.title(\"Activation Curves\")\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(x.numpy(), drelu.numpy(), label=\"d ReLU\")\n",
    "plt.plot(x.numpy(), dswish.numpy(), label=\"d Swish\")\n",
    "plt.plot(x.numpy(), dhswish.numpy(), label=\"d HardSwish\")\n",
    "plt.title(\"Derivatives (Gradients)\")\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"dy/dx\")\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf0332",
   "metadata": {},
   "source": [
    "## 5) PyTorchâ€™ta kullanÄ±m\n",
    "\n",
    "Ä°ki yol:\n",
    "\n",
    "### a) Module\n",
    "- `nn.Hardswish()`\n",
    "\n",
    "### b) Functional\n",
    "- `torch.nn.functional.hardswish(x)`\n",
    "\n",
    "HardSwish Ã§oÄŸunlukla:\n",
    "- CNN bloklarÄ±nda activation olarak\n",
    "- Mobil/edge hedefli modellerde\n",
    "kullanÄ±lÄ±r.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1db64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "m = nn.Hardswish()\n",
    "t = torch.tensor([-5.0, -3.0, -1.0, 0.0, 1.0, 3.0, 5.0])\n",
    "print(\"x:\", t.tolist())\n",
    "print(\"hardswish:\", m(t).tolist())\n",
    "print(\"hardsigmoid:\", F.hardsigmoid(t).tolist())\n",
    "print(\"x*hardsigmoid:\", (t * F.hardsigmoid(t)).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c24625",
   "metadata": {},
   "source": [
    "## 6) HardSwishâ€™i neden â€œmobil iÃ§in iyiâ€ derler?\n",
    "\n",
    "Sebep: HardSwish, Swishâ€™e gÃ¶re genellikle:\n",
    "- `exp` iÃ§ermez\n",
    "- daha basit aritmetik + clamp benzeri iÅŸlemlerle Ã§alÄ±ÅŸÄ±r\n",
    "\n",
    "Bu Ã§oÄŸu cihazda inferenceâ€™Ä± hÄ±zlandÄ±rabilir. AÅŸaÄŸÄ±daki test **kaba** bir fikir verir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeea54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def bench(fn, x, iters=2000, warmup=200):\n",
    "    for _ in range(warmup):\n",
    "        fn(x)\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        fn(x)\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters\n",
    "\n",
    "x = torch.randn(1024, 1024)\n",
    "\n",
    "# Swish'i manuel hesaplayalÄ±m\n",
    "def swish_fn(z): \n",
    "    return z * torch.sigmoid(z)\n",
    "\n",
    "t_relu = bench(torch.relu, x)\n",
    "t_swish = bench(swish_fn, x)\n",
    "t_hswish = bench(F.hardswish, x)\n",
    "\n",
    "print(f\"avg time/iter ReLU:      {t_relu*1e3:.3f} ms\")\n",
    "print(f\"avg time/iter Swish:     {t_swish*1e3:.3f} ms\")\n",
    "print(f\"avg time/iter HardSwish: {t_hswish*1e3:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90247c",
   "metadata": {},
   "source": [
    "## 7) KÃ¼Ã§Ã¼k bir â€œToyâ€ eÄŸitim deneyi (sezgi iÃ§in)\n",
    "\n",
    "AÅŸaÄŸÄ±da basit bir MLP ile sentetik ikili sÄ±nÄ±flandÄ±rma yapÄ±yoruz.\n",
    "AmaÃ§:\n",
    "- ReLU vs Swish vs HardSwish eÄŸitim eÄŸrilerini gÃ¶rmek\n",
    "- Bu bir â€œbenchmarkâ€ deÄŸil; davranÄ±ÅŸ sezgisini gÃ¼Ã§lendirir\n",
    "\n",
    "> CNNâ€™deki etkisi farklÄ± olabilir; ama activation hissiyatÄ± iÃ§in faydalÄ±.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def make_data(n=4096):\n",
    "    c0 = torch.randn(n//2, 2) * 0.8 + torch.tensor([-2.0, -1.0])\n",
    "    c1 = torch.randn(n//2, 2) * 0.8 + torch.tensor([ 2.0,  1.0])\n",
    "    X = torch.cat([c0, c1], dim=0)\n",
    "    y = torch.cat([torch.zeros(n//2), torch.ones(n//2)], dim=0).long()\n",
    "    idx = torch.randperm(n)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "X, y = make_data()\n",
    "Xtr, ytr = X[:3072], y[:3072]\n",
    "Xte, yte = X[3072:], y[3072:]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 2)\n",
    "        self.act_name = act\n",
    "\n",
    "    def act(self, z):\n",
    "        if self.act_name == \"relu\":\n",
    "            return F.relu(z)\n",
    "        if self.act_name == \"swish\":\n",
    "            return z * torch.sigmoid(z)\n",
    "        if self.act_name == \"hardswish\":\n",
    "            return F.hardswish(z)\n",
    "        raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.fc1(x))\n",
    "        h = self.act(self.fc2(h))\n",
    "        return self.out(h)\n",
    "\n",
    "def train(act=\"relu\", epochs=25, lr=1e-3):\n",
    "    model = MLP(act=act)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def acc(logits, y):\n",
    "        return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "    hist = []\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        logits = model(Xtr)\n",
    "        loss = loss_fn(logits, ytr)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            te_logits = model(Xte)\n",
    "            te_loss = loss_fn(te_logits, yte).item()\n",
    "            te_acc = acc(te_logits, yte)\n",
    "\n",
    "        hist.append((loss.item(), te_loss, te_acc))\n",
    "    return hist\n",
    "\n",
    "hist_relu = train(\"relu\")\n",
    "hist_swish = train(\"swish\")\n",
    "hist_hsw = train(\"hardswish\")\n",
    "\n",
    "relu_acc = [h[2] for h in hist_relu]\n",
    "swish_acc = [h[2] for h in hist_swish]\n",
    "hsw_acc = [h[2] for h in hist_hsw]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(relu_acc, label=\"ReLU\")\n",
    "plt.plot(swish_acc, label=\"Swish\")\n",
    "plt.plot(hsw_acc, label=\"HardSwish\")\n",
    "plt.title(\"Toy: Test Accuracy Curves\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"acc\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "print(\"Final acc -> ReLU: %.4f | Swish: %.4f | HardSwish: %.4f\" % (relu_acc[-1], swish_acc[-1], hsw_acc[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57106f",
   "metadata": {},
   "source": [
    "## 8) Ne zaman HardSwish kullanÄ±lÄ±r? (net rehber)\n",
    "\n",
    "### âœ… MantÄ±klÄ± senaryolar\n",
    "- Mobil/edge inference hedefi (MobileNetV3 gibi)\n",
    "- Accuracyâ€“latency dengesi gerekiyorsa\n",
    "- ReLUâ€™ya gÃ¶re daha â€œsmoothâ€ bir nonlinearity isteyip, Swishâ€™in maliyetinden kaÃ§Ä±nÄ±yorsan\n",
    "\n",
    "### âš ï¸ Dikkat\n",
    "- Ã‡ok negatif bÃ¶lgede Ã§Ä±ktÄ± 0â€™a yapÄ±ÅŸÄ±r (ReLU benzeri)\n",
    "- Orta bÃ¶lgede kuadratik davranÄ±ÅŸ: optimizasyon etkisi mimariye gÃ¶re deÄŸiÅŸebilir\n",
    "\n",
    "### Pratik karar\n",
    "- â€œEn yÃ¼ksek accuracyâ€ peÅŸindeysen: Swish/GELU ile kÄ±yasla\n",
    "- â€œEdge/deployâ€ Ã¶nemliyse: HardSwish Ã§ok mantÄ±klÄ± bir default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07c46f",
   "metadata": {},
   "source": [
    "## 9) KÄ±sa Ã¶zet ğŸ“Œ\n",
    "\n",
    "- HardSwish, Swishâ€™in daha ucuz (parÃ§alÄ±) yaklaÅŸÄ±mÄ±dÄ±r.\n",
    "- Matematiksel olarak: \\(\\mathrm{hardswish}(x)=x\\cdot \\mathrm{hardsigmoid}(x)\\)\n",
    "- Orta bÃ¶lgede kuadratik, uÃ§larda 0 ve x davranÄ±ÅŸÄ± gÃ¶sterir.\n",
    "- Mobil mimarilerde Ã§ok sÄ±k kullanÄ±lÄ±r; latency avantajÄ± olabilir.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
