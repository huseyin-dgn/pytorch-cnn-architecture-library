{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f87336",
   "metadata": {},
   "source": [
    "\n",
    "# Bu soruya cevap arıyoruz: Spatial Dropout, Stochastic Depth (DropPath) ve DropBlock nasıl olur da “öğrenmeyi iyileştirir”?\n",
    "\n",
    "Soru özeti:\n",
    "\n",
    "- CNN’e uygulanan **Spatial Dropout**, **Stochastic Depth / DropPath**, **DropBlock** gibi regularization işlemleri\n",
    "  nasıl olur da modelin **genelleme** (generalization) performansını artırır?\n",
    "- “Modelin öğrendiği kısımlardan eksiltmek” veya forward akışındaki bilgiyi azaltmak\n",
    "  nasıl **daha iyi öğrenmeye** yol açar?\n",
    "- Eğer tek bir teknik seçilecekse **hangisi** daha mantıklı?\n",
    "\n",
    "> Önemli ayrım: Bu teknikler genelde modelin **genellemesini** artırır.  \n",
    "> “Saf öğrenme kapasitesini (training accuracy)” çoğu zaman artırmaz; hatta training’i zorlaştırabilir.  \n",
    "> Ama overfit’i azaltarak *test/val performansını* yükseltir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a949a46",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1) “Bilgi azaltmak nasıl iyileştirir?”: co-adaptation kırma\n",
    "\n",
    "Derin ağlar çok güçlüdür. Bu güç şu riski getirir:\n",
    "\n",
    "- Ağ, train set’te işe yarayan ama genellemesi zayıf **kolay kısa yollar** (shortcuts) öğrenebilir.\n",
    "- Bazı nöron/kanal/bloklar birbirine aşırı bağımlı hale gelir: **co-adaptation**.\n",
    "\n",
    "Regularization’ın hedefi:\n",
    "- ağı “daha az ezber yapan”\n",
    "- daha **robust**, daha **dağıtık** temsil öğrenen\n",
    "bir modele dönüştürmektir.\n",
    "\n",
    "Eğitim sırasında ağın bazı parçalarını rastgele kapatmak:\n",
    "- tek bir yol/özelliğe aşırı güvenmeyi engeller\n",
    "- modelin birden fazla “çözüm yolunu” paylaşmasına zorlar (**implicit ensemble**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc360d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2) Matematiksel çatı: “gürültülü ağ = implicit ensemble”\n",
    "\n",
    "Dropout ailesinin ortak fikri:\n",
    "\n",
    "- Forward sırasında aktivasyonlara/brancha **rastgele maske** uygulanır.\n",
    "- Eğitim, bu rastgeleliğin altında optimize edilir.\n",
    "\n",
    "Basit dropout için:\n",
    "\n",
    "\\[\n",
    "y = \\frac{m}{1-p} \\odot x,\\quad m \\sim Bernoulli(1-p)\n",
    "\\]\n",
    "\n",
    "- \\(p\\): drop olasılığı\n",
    "- \\(\\frac{1}{1-p}\\): **beklenen değeri korumak** için ölçekleme (inverted dropout)\n",
    "\n",
    "Sonuç:\n",
    "- Her iterasyonda farklı bir “alt model” çalışır.\n",
    "- Testte dropout kapalıyken bu alt modellerin “ortalamasına yakın” davranış elde edilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d742a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[x] ~ -0.010698173195123672\n",
      "E[y] ~ -0.004418932367116213 (beklenen değer korunur)\n",
      "Var[x] ~ 1.0037707090377808\n",
      "Var[y] ~ 1.4233806133270264 (gürültü artar)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.randn(10000)\n",
    "p = 0.3\n",
    "m = (torch.rand_like(x) > p).float()\n",
    "y = (m / (1 - p)) * x\n",
    "\n",
    "print(\"E[x] ~\", x.mean().item())\n",
    "print(\"E[y] ~\", y.mean().item(), \"(beklenen değer korunur)\")\n",
    "print(\"Var[x] ~\", x.var(unbiased=False).item())\n",
    "print(\"Var[y] ~\", y.var(unbiased=False).item(), \"(gürültü artar)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc4feb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3) CNN’de üç teknik arasındaki temel fark: “hangi düzeyde dropout yapıyorsun?”\n",
    "\n",
    "CNN’de element-wise dropout çoğu zaman zayıf kalır (uzamsal korelasyon yüzünden).\n",
    "Bu yüzden daha yapılandırılmış versiyonlar kullanılır:\n",
    "\n",
    "### 3.1) Spatial Dropout (Channel Dropout)\n",
    "- Bir feature map’in **kanallarını** düşürür.\n",
    "- Bir kanal kapanınca tüm H×W haritası gider.\n",
    "\n",
    "**Ne çözer?** Kanal co-adaptation’ını kırar.  \n",
    "**Ne zaman?** CNN’lerde düşük/orta katmanlarda; hafif ve ucuz regularization için.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2) DropBlock\n",
    "- Uzamsal bölgeleri **blok halinde** (k×k) düşürür.\n",
    "\n",
    "**Ne çözer?** Lokal pattern’e aşırı güveni azaltır; uzamsal ezberi kırar.  \n",
    "**Ne zaman?** Detection/segmentation gibi uzamsal görevlerde, orta/derin katmanlarda.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3) Stochastic Depth / DropPath\n",
    "- Bireysel nöron/kanal değil, **tüm residual branch** (blok yolu) kapatılır.\n",
    "\n",
    "Residual blok:\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "DropPath ile:\n",
    "\\[\n",
    "y = x + \\frac{m}{1-p}F(x)\n",
    "\\]\n",
    "\n",
    "**Ne çözer?** Çok derin ağlarda bloklara aşırı güveni azaltır; derinlik boyunca implicit ensemble sağlar.  \n",
    "**Ne zaman?** Çok derin residual mimarilerde (ResNet/ConvNeXt/modern YOLO backbone’ları).\n",
    "\n",
    "> Özet: SpatialDropout = kanal düzeyi, DropBlock = uzamsal düzey, DropPath = blok düzeyi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6c79f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4) “Önceden öğrendiğini silmek” aslında ne demek?\n",
    "\n",
    "- Eğitimde maske uygulamak, “kalıcı olarak bilgiyi silmek” değildir.\n",
    "- Her iterasyonda farklı bir alt ağ çalıştırmak demektir.\n",
    "\n",
    "Genelleme artışının iki ana nedeni:\n",
    "\n",
    "### 4.1) Dağıtık temsil zorunluluğu\n",
    "Ağ, tek bir kanal/blok üzerinden çözüm kuramaz.\n",
    "Aynı bilgiyi birden fazla yoldan temsil etmeye zorlanır.\n",
    "\n",
    "### 4.2) Gürültü enjeksiyonu → daha düz minimum\n",
    "Random mask kayıp yüzeyine gürültü ekler.\n",
    "Bu, SGD’yi genelde “keskin minimum” yerine “daha düz minimum”a iter.\n",
    "Düz minimumlar pratikte daha iyi geneller (gözlemsel ama güçlü).\n",
    "\n",
    "> Yani “bilgi azaltmak” = “ezberi kolaylaştıran kırılgan yolu bozmak”tır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fccacb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 5) Mini implementasyonlar (PyTorch): SpatialDropout, DropPath, DropBlock\n",
    "\n",
    "Aşağıdaki kodlar kavramı netleştirmek içindir (prod-ready değil).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b7e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatialDropout: torch.Size([2, 8, 32, 32])\n",
      "DropPath: torch.Size([2, 8, 32, 32])\n",
      "DropBlock: torch.Size([2, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpatialDropout2d(nn.Module):\n",
    "    # Channel-wise dropout for NCHW (drop entire channels).\n",
    "    def __init__(self, p=0.2):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (not self.training) or self.p == 0.0:\n",
    "            return x\n",
    "        keep = 1.0 - self.p\n",
    "        mask = (torch.rand(x.size(0), x.size(1), 1, 1, device=x.device) < keep).float()\n",
    "        return x * mask / keep\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    # Stochastic Depth: drop entire residual path per-sample.\n",
    "    def __init__(self, p=0.2):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (not self.training) or self.p == 0.0:\n",
    "            return x\n",
    "        keep = 1.0 - self.p\n",
    "        mask = (torch.rand(x.size(0), 1, 1, 1, device=x.device) < keep).float()\n",
    "        return x * mask / keep\n",
    "\n",
    "def dropblock_mask(x, drop_prob=0.1, block_size=5):\n",
    "    # x: N,C,H,W -> returns mask N,1,H,W (same spatial mask for all channels)\n",
    "    if drop_prob == 0.0:\n",
    "        return torch.ones(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    gamma = drop_prob * (H * W) / (block_size ** 2) / ((H - block_size + 1) * (W - block_size + 1) + 1e-6)\n",
    "    center = (torch.rand(N, 1, H, W, device=x.device) < gamma).float()\n",
    "    block = F.max_pool2d(center, kernel_size=block_size, stride=1, padding=block_size // 2)\n",
    "    mask = 1 - block\n",
    "\n",
    "    keep_ratio = mask.mean().clamp(min=1e-6)\n",
    "    mask = mask / keep_ratio  # expectation correction\n",
    "    return mask\n",
    "\n",
    "class DropBlock2d(nn.Module):\n",
    "    def __init__(self, drop_prob=0.1, block_size=5):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (not self.training) or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        m = dropblock_mask(x, self.drop_prob, self.block_size)\n",
    "        return x * m\n",
    "\n",
    "# Shape sanity\n",
    "x = torch.randn(2, 8, 32, 32)\n",
    "sd = SpatialDropout2d(0.3).train()\n",
    "dp = DropPath(0.3).train()\n",
    "db = DropBlock2d(0.2, 7).train()\n",
    "\n",
    "print(\"SpatialDropout:\", sd(x).shape)\n",
    "print(\"DropPath:\", dp(x).shape)\n",
    "print(\"DropBlock:\", db(x).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb889b2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6) Hangi durumda hangisi?\n",
    "\n",
    "### Spatial Dropout\n",
    "- ✅ Kanal korelasyonu yüksek → kanal bazlı robustluk\n",
    "- ✅ Çok ucuz\n",
    "- ❌ Agresif p kapasiteyi düşürür\n",
    "\n",
    "### DropBlock;\n",
    "- ✅ Uzamsal ezberi kırar (lokal pattern bağımlılığı)\n",
    "- ✅ Detection/segmentation’da sık işe yarar\n",
    "- ❌ hyperparametre (block_size, drop_prob) daha hassas\n",
    "\n",
    "### DropPath\n",
    "- ✅ Derin residual ağlarda “genel amaç” güçlü regularizer\n",
    "- ✅ Modern backbone’larda yaygın; tuning çoğu zaman daha stabil\n",
    "- ❌ Residual yapı yoksa etkisi sınırlı\n",
    "\n",
    "> Hepsinde ortak: Val/test artışı hedeflenir; training loss genelde yükselir (normal).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d3543",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 7) “Bir teknik seçecek olsan hangisi?” (net ama koşullu)\n",
    "\n",
    "Genel CNN/YOLO backbone bağlamında tek bir teknik seçilecekse:\n",
    "\n",
    "✅ **DropPath (Stochastic Depth)**\n",
    "\n",
    "**Neden?**\n",
    "- Residual tabanlı modern mimarilerde en “taşınabilir” regularizer.\n",
    "- Derinlik boyunca implicit ensemble etkisi güçlü.\n",
    "- DropBlock’a göre çoğu zaman daha az nazlı ayar ister.\n",
    "\n",
    "**İstisnalar**\n",
    "- Uzamsal görev + lokal overfit çok belirginse → **DropBlock** daha direkt vurabilir.\n",
    "- Hafif/ucuz reg gerekiyorsa → **Spatial Dropout** en pratik seçenek olabilir.\n",
    "\n",
    "Pratik kural:\n",
    "> Derin residual ağ = önce DropPath  \n",
    "> Uzamsal ezber/detection = DropBlock ikinci aday  \n",
    "> Hafif reg = Spatial Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd181c1f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 8) Kısa hyperparametre notları\n",
    "\n",
    "### DropPath\n",
    "- p genelde derinliğe göre artan schedule ile kullanılır:\n",
    "  - erken bloklarda düşük\n",
    "  - son bloklarda yüksek\n",
    "\n",
    "### DropBlock\n",
    "- block_size feature map çözünürlüğüne göre seçilir\n",
    "- drop_prob küçükten başlatılır (0.05 → 0.2)\n",
    "- çoğunlukla orta/derin katmanlarda\n",
    "\n",
    "### Spatial Dropout\n",
    "- p genelde 0.1–0.3 aralığında\n",
    "- erken katmanlarda agresif p risklidir\n",
    "\n",
    "> Her zaman: tek değişken - tek deney (controlled ablation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa171e3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 9) Son söz\n",
    "\n",
    "Bu regularization’lar “modelin bilgisini azaltmak” için değil,\n",
    "modelin **kırılgan ezber yollarını** bozmaya ve\n",
    "daha **dayanıklı/genellenebilir** temsiller öğrenmesine zorlamaya yarar.\n",
    "\n",
    "Model “öğrenmeyi” değil, **ezberi** kaybeder → val/test artar.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
