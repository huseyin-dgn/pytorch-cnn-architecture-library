{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7160a67",
   "metadata": {},
   "source": [
    "# Dynamic Convolution (Dynamic Conv / CondConv) — Tam Anlatım + Uygulamalı PyTorch\n",
    "\n",
    "Bu notebook’ta **Dynamic Convolution** kavramını sıfırdan, adım adım öğreneceğiz ve **çalışan bir PyTorch implementasyonu** yazacağız.\n",
    "\n",
    "## Bu notebook sonunda net öğrenmiş olacaksın\n",
    "1) Dynamic Conv **neden var**? (static conv’un sınırı)\n",
    "2) “Input’a göre filtre seçme / filtre karıştırma” fikri\n",
    "3) **K adet uzman kernel** + **routing (gating)** ağı\n",
    "4) Softmax ile ağırlıklandırılmış kernel birleşimi\n",
    "5) PyTorch’ta iki implementasyon:\n",
    "   - (A) Anlaşılır: per-sample ağırlık hesapla + kernel oluştur (eğitim amaçlı)\n",
    "   - (B) Daha verimli: grouped-conv numarası (opsiyonel)\n",
    "6) DynamicConv kullanan mini model (DynamicNet)\n",
    "\n",
    "Not: Dynamic Conv güçlüdür ama **maliyet ve karmaşıklık** getirir. Bu notebook’ta bunun trade-off’larını da açıkça konuşacağız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fffcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "print('torch:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10cae9b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Motivasyon: Neden Dynamic Conv?\n",
    "\n",
    "Normal (static) `Conv2d` şudur:\n",
    "- Ağırlıklar **sabit** (`W`) ve her input için aynı filtre uygulanır.\n",
    "\n",
    "Bu iyi bir inductive bias’tır ama bir sınırlama getirir:\n",
    "- Farklı görüntüler / farklı içerikler için aynı filtre setini zorunlu kullanırsın.\n",
    "\n",
    "Dynamic Conv fikri:\n",
    "> Filtreleri input’a göre **dinamik** hale getir.\n",
    "\n",
    "Yani model şunu yapar:\n",
    "- Input’tan bir özet çıkarır (global context)\n",
    "- Bu özetle **hangi kernel(ler)** daha uygun karar verir\n",
    "- K adet kernel’in ağırlıklı karışımıyla “o anki” conv filtresini üretir\n",
    "\n",
    "Bu yaklaşımın iki tipik ismi:\n",
    "- **CondConv (Conditional Convolution)**: koşullu kernel kombinasyonu\n",
    "- **Dynamic Conv**: genel isim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3aa663",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Temel Formül (K uzman kernel)\n",
    "\n",
    "Elimizde K adet uzman kernel olsun:\n",
    "- `W_1, W_2, ..., W_K`\n",
    "\n",
    "Routing (gating) ağı, input x için K boyutlu bir ağırlık üretsin:\n",
    "- `a(x) = [a_1, ..., a_K]`\n",
    "- Genellikle **softmax** ile normalize edilir: `sum a_k = 1`\n",
    "\n",
    "Dinamik kernel:\n",
    "\n",
    "$W(x) = \\sum_{k=1}^{K} a_k(x)\\, W_k$\n",
    "\n",
    "Sonra conv:\n",
    "\n",
    "$y = \\mathrm{Conv}(x, W(x))$\n",
    "\n",
    "Özet:\n",
    "- Kernel sayısı K kadar artar\n",
    "- Ama her örnek için **tek bir efektif kernel** çalıştırırsın\n",
    "- Ek maliyet: routing ağı + kernel kombinasyonu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb2969",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Routing Ağı (Gating) Ne Kullanır?\n",
    "\n",
    "Pratikte gating çoğu zaman şu şekilde yapılır:\n",
    "1) Global Average Pooling (GAP): `x -> v` (B, C)\n",
    "2) Küçük bir MLP (Linear → ReLU → Linear)\n",
    "3) Softmax ile K ağırlık\n",
    "\n",
    "Bu, SE (Squeeze-Excitation) mantığına benzer ama çıkış **kanal ağırlığı** değil, **kernel karışımı ağırlığıdır**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c671570",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) DynamicConv2d — Eğitim Odaklı, Anlaşılır Implementasyon\n",
    "\n",
    "Bu versiyonun amacı **okunabilirlik**:\n",
    "- Her örnek için `a_k` hesaplanır\n",
    "- Kernel ağırlıklı toplanır\n",
    "- Sonra conv uygulanır\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb48509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConv2d(nn.Module):\n",
    "    \"\"\"Dynamic/Conditional Conv2d (CondConv tarzı)\n",
    "\n",
    "    K adet uzman kernel saklar.\n",
    "    Her input için gating ağı K ağırlık üretir.\n",
    "    Kernel'ler ağırlıklı toplanıp efektif kernel ile conv yapılır.\n",
    "\n",
    "    Not: Bu notebook'ta anlaşılır olması için per-sample conv döngüsü kullanıyoruz.\n",
    "    \"\"\"\n",
    "    def __init__(self, cin, cout, k=3, stride=1, padding=1, dilation=1,\n",
    "                 K=4, reduction=4, bias=False, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.cin = cin\n",
    "        self.cout = cout\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.K = K\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Uzman kernel bankası: (K, cout, cin, k, k)\n",
    "        self.weight = nn.Parameter(torch.randn(K, cout, cin, k, k) * 0.02)\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(K, cout))\n",
    "\n",
    "        # Routing / gating: GAP -> MLP -> K logits\n",
    "        hidden = max(1, cin // reduction)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(cin, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, K)\n",
    "\n",
    "    def routing(self, x):\n",
    "        # x: (B, C, H, W) -> a: (B, K)\n",
    "        v = self.gap(x).flatten(1)         # (B, C)\n",
    "        h = F.relu(self.fc1(v))           # (B, hidden)\n",
    "        logits = self.fc2(h)              # (B, K)\n",
    "        a = F.softmax(logits / self.temperature, dim=1)\n",
    "        return a\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        a = self.routing(x)  # (B, K)\n",
    "\n",
    "        # Dinamik kernel üretimi: Wdyn (B, cout, cin, k, k)\n",
    "        Wdyn = torch.einsum('bk,kocij->bocij', a, self.weight)\n",
    "\n",
    "        # (Opsiyonel) bias\n",
    "        bdyn = None\n",
    "        if self.bias is not None:\n",
    "            bdyn = torch.einsum('bk,kc->bc', a, self.bias)  # (B, cout)\n",
    "\n",
    "        # Per-sample conv (anlaşılır versiyon)\n",
    "        outs = []\n",
    "        for i in range(B):\n",
    "            yi = F.conv2d(\n",
    "                x[i:i+1],\n",
    "                Wdyn[i],\n",
    "                bias=None if bdyn is None else bdyn[i],\n",
    "                stride=self.stride,\n",
    "                padding=self.padding,\n",
    "                dilation=self.dilation,\n",
    "            )\n",
    "            outs.append(yi)\n",
    "        return torch.cat(outs, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463e5f7",
   "metadata": {},
   "source": [
    "### 4.1) Hızlı Test\n",
    "- Giriş shape\n",
    "- Çıkış shape\n",
    "- Routing ağırlıkları toplamı 1 mi?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2323cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 16, 32, 32])\n",
      "y: torch.Size([2, 32, 32, 32])\n",
      "routing a: torch.Size([2, 4])\n",
      "row sums (should be 1): tensor([1., 1.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 16, 32, 32)\n",
    "dyn = DynamicConv2d(cin=16, cout=32, k=3, padding=1, K=4, reduction=4)\n",
    "y = dyn(x)\n",
    "\n",
    "a = dyn.routing(x)\n",
    "print('x:', x.shape)\n",
    "print('y:', y.shape)\n",
    "print('routing a:', a.shape)\n",
    "print('row sums (should be 1):', a.sum(dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c53da",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) DynamicConvBlock: DynamicConv + BN + ReLU\n",
    "\n",
    "Modelde tek katman değil, blok halinde kullanmak daha mantıklı.\n",
    "Burada:\n",
    "- DynamicConv2d\n",
    "- BatchNorm\n",
    "- ReLU\n",
    "\n",
    "şeklinde bir blok tanımlıyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3b5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConvBlock(nn.Module):\n",
    "    def __init__(self, cin, cout, k=3, stride=1, padding=1, K=4, reduction=4, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.dyn = DynamicConv2d(cin, cout, k=k, stride=stride, padding=padding, K=K, reduction=reduction, temperature=temperature)\n",
    "        self.bn = nn.BatchNorm2d(cout)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dyn(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1611ae",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) Tam Mini Model: DynamicNet\n",
    "\n",
    "Bu model:\n",
    "- Stem (normal conv)\n",
    "- 3 stage (DynamicConvBlock)\n",
    "- Head (GAP + FC)\n",
    "\n",
    "Amaç: Dynamic Conv’u model seviyesinde görmek.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d6a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10, K=4, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.stage1 = DynamicConvBlock(32, 64,  k=3, stride=2, padding=1, K=K, reduction=4, temperature=temperature)\n",
    "        self.stage2 = DynamicConvBlock(64, 128, k=3, stride=2, padding=1, K=K, reduction=4, temperature=temperature)\n",
    "        self.stage3 = DynamicConvBlock(128, 256, k=3, stride=2, padding=1, K=K, reduction=4, temperature=temperature)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        if verbose: print('input:', x.shape)\n",
    "        x = self.stem(x)\n",
    "        if verbose: print('stem :', x.shape)\n",
    "        x = self.stage1(x)\n",
    "        if verbose: print('s1   :', x.shape)\n",
    "        x = self.stage2(x)\n",
    "        if verbose: print('s2   :', x.shape)\n",
    "        x = self.stage3(x)\n",
    "        if verbose: print('s3   :', x.shape)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        out = self.fc(x)\n",
    "        if verbose: print('out  :', out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e617b0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 3, 64, 64])\n",
      "stem : torch.Size([2, 32, 64, 64])\n",
      "s1   : torch.Size([2, 64, 32, 32])\n",
      "s2   : torch.Size([2, 128, 16, 16])\n",
      "s3   : torch.Size([2, 256, 8, 8])\n",
      "out  : torch.Size([2, 10])\n",
      "\n",
      "final: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "model = DynamicNet(in_channels=3, num_classes=10, K=4, temperature=1.0)\n",
    "x = torch.randn(2, 3, 64, 64)\n",
    "y = model(x, verbose=True)\n",
    "print('\\nfinal:', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3397b1",
   "metadata": {},
   "source": [
    "---\n",
    "## 7) Dynamic Conv’un Artıları / Eksileri (Gerçekçi)\n",
    "\n",
    "### Artılar\n",
    "- Input’a göre adaptasyon → farklı örneklere farklı filtre davranışı\n",
    "- Mixture-of-Experts sezgisi (uzman kernel bankası)\n",
    "\n",
    "### Eksiler\n",
    "- Ek compute: routing ağı + kernel kombinasyonu\n",
    "- Kod karmaşıklığı (özellikle batch üzerinde verimli uygulama)\n",
    "- Bazı durumlarda kararsız eğitim (temperature, regularization gerekebilir)\n",
    "\n",
    "### Pratik ipuçları\n",
    "- `K` küçük başla: 4 veya 8\n",
    "- `temperature` ile softmax keskinliğini kontrol et\n",
    "  - düşük T → daha seçici\n",
    "  - yüksek T → daha karışım\n",
    "- Routing’e dropout / weight decay eklemek bazen iyi gelir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20247dd9",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) (Opsiyonel) Daha Verimli Uygulama Ne Demek?\n",
    "\n",
    "Bu notebook’ta anlaşılır olması için per-sample döngü kullandık.\n",
    "Gerçek projede batch’i tek seferde işlemek için:\n",
    "- grouped-conv trick\n",
    "- top-k routing\n",
    "- dinamikliği sadece 1×1 conv’da kullanma\n",
    "\n",
    "gibi optimizasyonlar yapılır.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
