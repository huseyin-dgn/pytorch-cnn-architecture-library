{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e025aa9",
   "metadata": {},
   "source": [
    "## Önce Routing Ağı'nı inceleyelim:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5a3fb",
   "metadata": {},
   "source": [
    "* Bu modül x (B,C,H,W) alır, a (B,K) döndürür. a her örnek için “K kernel karışım oranı”dır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f46621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6704aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RoutingMLP(nn.Module):\n",
    "    def __init__(self, cin: int, K: int = 4, reduction: int = 4, temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        hidden = max(1, cin // reduction)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(cin, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, K)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x):\n",
    "        v = self.gap(x).flatten(1)                # (B,C)\n",
    "        h = F.relu(self.fc1(v))                   # (B,hidden)\n",
    "        logits = self.fc2(h)                      # (B,K)\n",
    "        a = F.softmax(logits / self.temperature, dim=1)\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "355c24c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape: torch.Size([2, 4])\n",
      "row sums: tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 16, 32, 32)\n",
    "router = RoutingMLP(cin=16, K=4, reduction=4, temperature=1.0)\n",
    "0\n",
    "a = router(x)\n",
    "print(\"a shape:\", a.shape)        # (2,4)\n",
    "print(\"row sums:\", a.sum(dim=1))  # ~tensor([1., 1.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c6e1ce",
   "metadata": {},
   "source": [
    "## 2) Routing’i Dynamic Conv’a bağlama (okunur “eğitim” versiyonu)\n",
    "\n",
    "Burada K adet kernel var, routing ağı a üretir, sonra:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d19efb",
   "metadata": {},
   "source": [
    ">**W(x)=k∑​ak​(x)Wk​**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d1d7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Okunur DynamicConv2d:\n",
    "      - K uzman kernel saklar\n",
    "      - Routing ağı ile a (B,K) üretir\n",
    "      - Wdyn (B, cout, cin, k, k) üretir\n",
    "      - Her sample için conv uygular (anlamak için en net yol)\n",
    "    \"\"\"\n",
    "    def __init__(self, cin, cout, k=3, stride=1, padding=1, K=4, reduction=4, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.cin, self.cout = cin, cout\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.K = K\n",
    "\n",
    "        # Kernel bankası: (K, cout, cin, k, k)\n",
    "        self.weight = nn.Parameter(torch.randn(K, cout, cin, k, k) * 0.02)\n",
    "\n",
    "        # Routing ağı\n",
    "        self.router = RoutingMLP(cin=cin, K=K, reduction=reduction, temperature=temperature)\n",
    "\n",
    "    def forward(self, x, return_routing=False):\n",
    "        B, C, H, W = x.shape\n",
    "        a = self.router(x)  # (B,K)\n",
    "\n",
    "        # Wdyn: (B, cout, cin, k, k)\n",
    "        Wdyn = torch.einsum(\"bk,kocij->bocij\", a, self.weight)\n",
    "\n",
    "        # Per-sample conv (net ve anlaşılır)\n",
    "        outs = []\n",
    "        for i in range(B):\n",
    "            yi = F.conv2d(x[i:i+1], Wdyn[i], stride=self.stride, padding=self.padding)\n",
    "            outs.append(yi)\n",
    "\n",
    "        y = torch.cat(outs, dim=0)\n",
    "        return (y, a) if return_routing else y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cc6d81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: torch.Size([2, 32, 32, 32])\n",
      "a: torch.Size([2, 4])\n",
      "row sums: tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 16, 32, 32)\n",
    "dyn = DynamicConv2d(cin=16, cout=32, k=3, padding=1, K=4)\n",
    "\n",
    "y, a = dyn(x, return_routing=True)\n",
    "print(\"y:\", y.shape)     # (2,32,32,32)\n",
    "print(\"a:\", a.shape)     # (2,4)\n",
    "print(\"row sums:\", a.sum(dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a5edc",
   "metadata": {},
   "source": [
    "## Tam hatasız sürüm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af175266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: torch.Size([2, 32, 32, 32])\n",
      "a shape: torch.Size([2, 4])\n",
      "row sums: tensor([1., 1.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------\n",
    "# Routing ağı (gating)\n",
    "# ---------------------------\n",
    "class RoutingMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    x (B,C,H,W) -> GAP -> (B,C) -> MLP -> logits (B,K) -> softmax -> a (B,K)\n",
    "    \"\"\"\n",
    "    def __init__(self, cin: int, K: int = 4, reduction: int = 4, temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        hidden = max(1, cin // reduction)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(cin, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, K)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x):\n",
    "        v = self.gap(x).flatten(1)  # (B,C)\n",
    "        h = F.relu(self.fc1(v))     # (B,hidden)\n",
    "        logits = self.fc2(h)        # (B,K)\n",
    "        a = F.softmax(logits / self.temperature, dim=1)\n",
    "        return a\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dynamic Convolution\n",
    "# ---------------------------\n",
    "class DynamicConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    K uzman kernel + routing ağı ile input'a özel kernel üretir.\n",
    "    Okunurluk için per-sample conv döngüsü kullanır (eğitim/demo için ideal).\n",
    "    \"\"\"\n",
    "    def __init__(self, cin, cout, k=3, stride=1, padding=1,\n",
    "                 K=4, reduction=4, temperature=1.0, bias=False):\n",
    "        super().__init__()\n",
    "        self.cin, self.cout = cin, cout\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.K = K\n",
    "\n",
    "        # Kernel bankası: (K, cout, cin, k, k)\n",
    "        self.weight = nn.Parameter(torch.randn(K, cout, cin, k, k) * 0.02)\n",
    "\n",
    "        # (Opsiyonel) bias bankası: (K, cout)\n",
    "        self.bias_bank = nn.Parameter(torch.zeros(K, cout)) if bias else None\n",
    "\n",
    "        # Routing\n",
    "        self.router = RoutingMLP(cin=cin, K=K, reduction=reduction, temperature=temperature)\n",
    "\n",
    "    def forward(self, x, return_routing=False):\n",
    "        B, C, H, W = x.shape\n",
    "        a = self.router(x)  # (B,K)\n",
    "\n",
    "        # Dinamik kernel: Wdyn (B, cout, cin, k, k)\n",
    "        Wdyn = torch.einsum(\"bk,kocij->bocij\", a, self.weight)\n",
    "\n",
    "        # Dinamik bias: bdyn (B, cout)\n",
    "        bdyn = None\n",
    "        if self.bias_bank is not None:\n",
    "            bdyn = torch.einsum(\"bk,kc->bc\", a, self.bias_bank)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(B):\n",
    "            yi = F.conv2d(\n",
    "                x[i:i+1],\n",
    "                Wdyn[i],\n",
    "                bias=None if bdyn is None else bdyn[i],\n",
    "                stride=self.stride,\n",
    "                padding=self.padding\n",
    "            )\n",
    "            outs.append(yi)\n",
    "\n",
    "        y = torch.cat(outs, dim=0)\n",
    "        return (y, a) if return_routing else y\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Hızlı test\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(2, 16, 32, 32)\n",
    "    dyn = DynamicConv2d(cin=16, cout=32, k=3, padding=1, K=4, temperature=1.0)\n",
    "\n",
    "    y, a = dyn(x, return_routing=True)\n",
    "    print(\"y shape:\", y.shape)       # (2,32,32,32)\n",
    "    print(\"a shape:\", a.shape)       # (2,4)\n",
    "    print(\"row sums:\", a.sum(dim=1)) # ~[1, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5078bb13",
   "metadata": {},
   "source": [
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2087169",
   "metadata": {},
   "source": [
    "## 1) GAP ne?\n",
    "\n",
    "* GAP = Global Average Pooling (nn.AdaptiveAvgPool2d((1,1)))\n",
    "\n",
    "* Input: x shape = (B, C, H, W)\n",
    "\n",
    "GAP çıkışı: (B, C, 1, 1)\n",
    ">sonra flatten(1) ile (B, C)\n",
    "\n",
    "#### Ne yapıyor?\n",
    "* Her kanal için tüm uzamsal piksellerin ortalamasını alıyor:\n",
    "\n",
    "#### Neden kullanıyoruz?\n",
    "* Routing ağı “hangi kernel uygun?” kararını vermek için görüntünün global özetine ihtiyaç duyar.\n",
    "GAP bunu ucuz ve stabil şekilde sağlar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac69864",
   "metadata": {},
   "source": [
    "## 2) hidden ne, neden max kullanılıyor?\n",
    "\n",
    "Kod:\n",
    "```python\n",
    "hidden = max(1, cin // reduction)\n",
    "```\n",
    "\n",
    "* hidden = routing MLP’nin ara katman boyutu.\n",
    "\n",
    "* cin küçükse, cin // reduction 0 çıkabilir (ör: 3 // 4 = 0)\n",
    "\n",
    "* Linear katmanda 0 boyut olamaz → model patlar.\n",
    "\n",
    "O yüzden:\n",
    "\n",
    "* max(1, ...) ile en az 1 garanti ediliyor.\n",
    "\n",
    "**Amaç:**\n",
    "* Routing ağı küçük kalsın (hafif) ama geçerli boyutta olsun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4ce3b",
   "metadata": {},
   "source": [
    "## 3) Temperature (temperature) ne işe yarıyor?\n",
    "\n",
    "Softmax’a girmeden önce:\n",
    "```python\n",
    "a = softmax(logits / temperature)\n",
    "```\n",
    "\n",
    "\n",
    "* Temperature, softmax’ın “ne kadar seçici” olacağını ayarlar.\n",
    "\n",
    "* T ↓ (0.5 gibi) → dağılım keskinleşir\n",
    "> → model genelde “tek kerneli seçmeye” yaklaşır (one-hot benzeri)\n",
    "\n",
    "* T ↑ (2.0 gibi) → dağılım yumuşar\n",
    ">→ kernel’ler daha çok karışır (uniform’a yaklaşır)\n",
    "\n",
    "Kısaca:\n",
    "\n",
    "* Küçük T = daha agresif seçim\n",
    "\n",
    "* Büyük T = daha yumuşak karışım"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ad2f4",
   "metadata": {},
   "source": [
    "## 4) Softmax’ın içine koyduğumuz şey ne?\n",
    "\n",
    "* Softmax’ın içine koyduğun şey: logits.\n",
    "\n",
    "Kod:\n",
    "```python\n",
    "logits = fc2(h)  # (B, K)\n",
    "a = softmax(logits / T)\n",
    "```\n",
    "\n",
    "\n",
    "logits ne?\n",
    "* Henüz olasılık olmayan “ham skorlar”.\n",
    "\n",
    "Her örnek için K tane skor:\n",
    "\n",
    "* logit_1, logit_2, ..., logit_K\n",
    "\n",
    ">Softmax bunları pozitif ve toplamı 1 olan ağırlıklara çevirir:\n",
    "Bu yüzden a:\n",
    "\n",
    "* her elemanı 0-1 arası\n",
    "\n",
    "* satır toplamı = 1\n",
    "\n",
    "* “kernel karışım oranı” gibi davranır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65d373",
   "metadata": {},
   "source": [
    "## 5) Kernel bankası ve bias bankası ne?\n",
    "Kernel bankası\n",
    "\n",
    "Kod:\n",
    "```python\n",
    "self.weight: (K, cout, cin, k, k)\n",
    "```\n",
    "\n",
    "\n",
    "Bu şu demek:\n",
    "\n",
    "* K tane ayrı ayrı uzman conv kernel var.\n",
    "\n",
    "Her biri normal Conv2d ağırlığı gibi düşün:\n",
    "\n",
    "* W1, W2, ..., WK\n",
    "\n",
    "* Routing ağı a üretince bu kernel’leri karıştırıp tek efektif kernel yapıyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6638b1",
   "metadata": {},
   "source": [
    "### Bias bankası\n",
    "\n",
    "Kod (opsiyonel):\n",
    "```python\n",
    "self.bias_bank: (K, cout)\n",
    "```\n",
    "\n",
    "\n",
    "Eğer bias kullanırsan:\n",
    "\n",
    "* Her kernel için ayrı bias vektörü var: b1, b2, ..., bK\n",
    "\n",
    "* Routing ağıyla bunlar da karıştırılıp efektif bias oluşturulur.\n",
    "\n",
    "**Not: CNN’lerde BN kullandığın için çoğu zaman bias zaten gereksiz → o yüzden default bias=False mantıklı.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291e15f",
   "metadata": {},
   "source": [
    "## 6) torch.einsum burada ne işe yarıyor?\n",
    "\n",
    "İki tane kritik einsum var:\n",
    "\n",
    "### (A) Dinamik kernel üretimi\n",
    "```python\n",
    "Wdyn = einsum(\"bk,kocij->bocij\", a, weight)\n",
    "```\n",
    "\n",
    "\n",
    "* a: (B, K) → her örnek için kernel ağırlıkları\n",
    "\n",
    "* weight: (K, cout, cin, k, k) → kernel bankası\n",
    "\n",
    "\n",
    "### (B) Dinamik bias üretimi\n",
    "```python\n",
    "bdyn = einsum(\"bk,kc->bc\", a, bias_bank)\n",
    "```\n",
    "\n",
    "* einsum burada “ağırlıklı toplama / karışım” yapıyor.\n",
    "* Yani routing çıktısını kernel bankasına uyguluyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4b73f",
   "metadata": {},
   "source": [
    "## 7) outs ne? For içinde ne yapılıyor?\n",
    "Neden for var?\n",
    "\n",
    "PyTorch’un F.conv2d fonksiyonu:\n",
    "\n",
    "* batch içindeki her örneğe aynı kernel uygular.\n",
    "\n",
    "Ama Dynamic Conv’da:\n",
    "\n",
    "* her örnek için kernel farklı (Wdyn[i])\n",
    "\n",
    "* PyTorch tek çağrıda “batch’te herkes farklı kernel kullansın” diyemiyor (standart conv).\n",
    "\n",
    "Bu yüzden eğitim/demo amaçlı:\n",
    "\n",
    "* örnek örnek conv yapıyoruz.\n",
    "\n",
    "Kod:\n",
    "```python\n",
    "outs = []\n",
    "for i in range(B):\n",
    "    yi = F.conv2d(x[i:i+1], Wdyn[i], ...)\n",
    "    outs.append(yi)\n",
    "y = torch.cat(outs, dim=0)\n",
    "```\n",
    "\n",
    "\n",
    "* outs: her örneğin çıktısını tutan liste\n",
    "\n",
    "döngü:\n",
    "\n",
    "* i’inci örnek: x[i:i+1] (shape: 1,C,H,W)\n",
    "\n",
    "* i’inci kernel: Wdyn[i] (cout,cin,k,k)\n",
    "\n",
    "* conv sonucu: yi (1,cout,H’,W’)\n",
    "\n",
    "* sonra hepsini batch olarak birleştiriyoruz: cat → (B, cout, H’, W’)\n",
    "\n",
    "### Özet:\n",
    "\n",
    "* outs = “her sample’ın conv çıktısı”\n",
    "* for döngüsü = “her sample’a kendi kerneliyle conv uygulama”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e74517",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
