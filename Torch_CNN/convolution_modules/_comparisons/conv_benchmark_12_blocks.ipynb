{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a43782",
   "metadata": {},
   "source": [
    "# 12 Convolution / Block Benchmark (Params + MACs + Latency + (Optional) CUDA Memory)\n",
    "\n",
    "**Amaç:** Şu 12 blok için tek bir model iskeletinde karşılaştırma:\n",
    "1) Depthwise Separable  \n",
    "2) Pointwise (1×1)  \n",
    "3) Inverted Bottleneck (MBConv)  \n",
    "4) Ghost  \n",
    "5) Dilated  \n",
    "6) Deformable *(torchvision varsa gerçek; yoksa fallback)*  \n",
    "7) Shift  \n",
    "8) Octave  \n",
    "9) Dynamic Conv  \n",
    "10) RepVGG *(deploy mode ölçülür)*  \n",
    "11) Coordinate Conv (CoordConv)  \n",
    "12) Group Conv\n",
    "\n",
    "> Accuracy yok. Sadece: **parametre**, **yaklaşık MACs**, **latency** (ms/iter), **(varsa) CUDA peak memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b463dfa",
   "metadata": {},
   "source": [
    "## Notlar (kısa)\n",
    "- **MACs (approx)**: Pool/upsample/shift gibi non-conv ops’lar ihmal veya yaklaşık alınmıştır.  \n",
    "- **Deformable**: torchvision `DeformConv2d` yoksa otomatik **fallback** yapar (standard conv).  \n",
    "- **RepVGG**: burada **deploy mode** ölçülür (tek 3×3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea85231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu | input: (16, 64, 56, 56) | deformable_real: True\n",
      "DepthwiseSep       | params=      9152 | macs=    439.94 M | lat= 38.447 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "Pointwise          | params=      8448 | macs=    411.04 M | lat= 25.804 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "InvBottleneck      | params=     52736 | macs=   2581.86 M | lat=132.369 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "Ghost              | params=      4928 | macs=    234.42 M | lat= 31.326 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "Dilated            | params=     73984 | macs=   3699.38 M | lat= 34.826 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "Deformable         | params=     84370 | macs=    520.22 M | lat=479.308 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "Shift              | params=      8448 | macs=    411.04 M | lat= 28.294 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "Octave             | params=     73984 | macs=   1618.48 M | lat= 54.476 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "Dynamic            | params=    296788 | macs=      0.02 M | lat= 40.008 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "RepVGG(train)      | params=     82432 | macs=   4110.42 M | lat= 59.324 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "CoordConv          | params=     77440 | macs=   3872.78 M | lat= 39.470 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n",
      "GroupConv          | params=     18688 | macs=    924.84 M | lat= 20.848 ms | peak=    0.0 MB | out=(16, 128, 56, 56)\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------\n",
    "# Profil: Params / MACs / Latency / Peak Mem\n",
    "# -------------------------\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "class MacCounter:\n",
    "    def __init__(self):\n",
    "        self.macs = 0\n",
    "\n",
    "    def hook(self, module, inp, out):\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            out = out[0]\n",
    "\n",
    "        # Conv2d\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            x = inp[0]\n",
    "            B = x.shape[0]\n",
    "            Cin = module.in_channels\n",
    "            Cout = module.out_channels\n",
    "            kH, kW = module.kernel_size if isinstance(module.kernel_size, tuple) else (module.kernel_size, module.kernel_size)\n",
    "            groups = module.groups\n",
    "            Hout, Wout = out.shape[-2], out.shape[-1]\n",
    "            mac = B * Hout * Wout * Cout * (Cin // groups) * kH * kW\n",
    "            self.macs += int(mac)\n",
    "            return\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            x = inp[0]\n",
    "            B = x.shape[0] if x.dim() > 1 else 1\n",
    "            mac = B * module.in_features * module.out_features\n",
    "            self.macs += int(mac)\n",
    "            return\n",
    "\n",
    "def benchmark_latency(m: nn.Module, x: torch.Tensor, iters=200, warmup=50) -> float:\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = m(x)\n",
    "        if x.device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(iters):\n",
    "            _ = m(x)\n",
    "        if x.device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "    return (t1 - t0) * 1000.0 / iters\n",
    "\n",
    "def profile_block(name: str, m: nn.Module, x: torch.Tensor):\n",
    "    m = m.to(x.device).eval()\n",
    "    mac_counter = MacCounter()\n",
    "    hooks = []\n",
    "    for mod in m.modules():\n",
    "        if isinstance(mod, (nn.Conv2d, nn.Linear)):\n",
    "            hooks.append(mod.register_forward_hook(mac_counter.hook))\n",
    "\n",
    "    # Peak mem\n",
    "    if x.device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y = m(x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    params = count_params(m)\n",
    "    macs = mac_counter.macs\n",
    "    ms = benchmark_latency(m, x, iters=200, warmup=50)\n",
    "\n",
    "    peak_mb = 0.0\n",
    "    if x.device.type == \"cuda\":\n",
    "        peak_mb = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    print(f\"{name:18s} | params={params:10d} | macs={macs/1e6:10.2f} M | lat={ms:7.3f} ms | peak={peak_mb:7.1f} MB | out={tuple(y.shape)}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class PointwiseConv(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(cin, cout, 1, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(cin, cin, 3, padding=1, groups=cin, bias=False),\n",
    "            nn.BatchNorm2d(cin),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(cin, cout, 1, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class InvertedBottleneck(nn.Module):\n",
    "    def __init__(self, cin, cout, expand=4):\n",
    "        super().__init__()\n",
    "        mid = cin * expand\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(cin, mid, 1, bias=False),\n",
    "            nn.BatchNorm2d(mid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid, mid, 3, padding=1, groups=mid, bias=False),\n",
    "            nn.BatchNorm2d(mid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid, cout, 1, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class GhostConv(nn.Module):\n",
    "    def __init__(self, cin, cout, ratio=2):\n",
    "        super().__init__()\n",
    "        init = math.ceil(cout / ratio)\n",
    "        new = cout - init\n",
    "        self.primary = nn.Sequential(\n",
    "            nn.Conv2d(cin, init, 1, bias=False),\n",
    "            nn.BatchNorm2d(init),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # FIX: new=0 olursa Conv2d(.,0,...) invalid olur\n",
    "        self.cheap = None\n",
    "        if new > 0:\n",
    "            self.cheap = nn.Sequential(\n",
    "                nn.Conv2d(init, new, 3, padding=1, groups=init, bias=False),\n",
    "                nn.BatchNorm2d(new),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        self.cout = cout\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.primary(x)\n",
    "        if self.cheap is None:\n",
    "            return y\n",
    "        z = self.cheap(y)\n",
    "        out = torch.cat([y, z], dim=1)\n",
    "        return out[:, :self.cout]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class DilatedConv(nn.Module):\n",
    "    def __init__(self, cin, cout, dilation=2):\n",
    "        super().__init__()\n",
    "        p = dilation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(cin, cout, 3, padding=p, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# Deformable: torchvision varsa gerçek, yoksa fallback \n",
    "try:\n",
    "    from torchvision.ops import DeformConv2d as TVDeformConv2d\n",
    "    _HAS_DEFORM = True\n",
    "except Exception:\n",
    "    TVDeformConv2d = None\n",
    "    _HAS_DEFORM = False\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class DeformableConv(nn.Module):\n",
    "    def __init__(self, cin, cout, k=3, padding=1):\n",
    "        super().__init__()\n",
    "        if not _HAS_DEFORM:\n",
    "            self.fallback = True\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Conv2d(cin, cout, k, padding=padding, bias=False),\n",
    "                nn.BatchNorm2d(cout),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            self.fallback = False\n",
    "            self.offset = nn.Conv2d(cin, 2*k*k, 3, padding=1)  # 2*k*k offset\n",
    "            self.conv = TVDeformConv2d(cin, cout, k, padding=padding, bias=False)\n",
    "            self.bn = nn.BatchNorm2d(cout)\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.fallback:\n",
    "            return self.net(x)\n",
    "        off = self.offset(x)\n",
    "        y = self.conv(x, off)\n",
    "        return self.act(self.bn(y))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "def shift2d(x, dirs=((0,0),(1,0),(-1,0),(0,1),(0,-1))):\n",
    "    B,C,H,W = x.shape\n",
    "    G = len(dirs)\n",
    "    base = C // G\n",
    "    sizes = [base]*(G-1) + [C - base*(G-1)]\n",
    "    chunks = torch.split(x, sizes, dim=1)\n",
    "    out = []\n",
    "    for ch,(dx,dy) in zip(chunks, dirs):\n",
    "        if dx==0 and dy==0:\n",
    "            out.append(ch); continue\n",
    "        r = torch.roll(ch, shifts=(dy,dx), dims=(-2,-1))\n",
    "        if dy>0: r[..., :dy, :] = 0\n",
    "        if dy<0: r[..., dy:, :] = 0\n",
    "        if dx>0: r[..., :, :dx] = 0\n",
    "        if dx<0: r[..., :, dx:] = 0\n",
    "        out.append(r)\n",
    "    return torch.cat(out, dim=1)\n",
    "\n",
    "class ShiftConv(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        self.pw = nn.Sequential(\n",
    "            nn.Conv2d(cin, cout, 1, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.pw(shift2d(x))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class OctaveConv(nn.Module):\n",
    "    def __init__(self, cin, cout, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        ch_h_in = int(round(cin*(1-alpha)))\n",
    "        ch_l_in = cin - ch_h_in\n",
    "        ch_h_out = int(round(cout*(1-alpha)))\n",
    "        ch_l_out = cout - ch_h_out\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "        self.hh = nn.Conv2d(ch_h_in, ch_h_out, 3, padding=1, bias=False)\n",
    "        self.hl = nn.Conv2d(ch_h_in, ch_l_out, 3, padding=1, bias=False)\n",
    "        self.lh = nn.Conv2d(ch_l_in, ch_h_out, 3, padding=1, bias=False)\n",
    "        self.ll = nn.Conv2d(ch_l_in, ch_l_out, 3, padding=1, bias=False)\n",
    "\n",
    "        self.bn_h = nn.BatchNorm2d(ch_h_out)\n",
    "        self.bn_l = nn.BatchNorm2d(ch_l_out)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "class OctaveConv(nn.Module):\n",
    "    def __init__(self, cin, cout, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        ch_h_in = int(round(cin*(1-alpha)))\n",
    "        ch_l_in = cin - ch_h_in\n",
    "        ch_h_out = int(round(cout*(1-alpha)))\n",
    "        ch_l_out = cout - ch_h_out\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "        self.hh = nn.Conv2d(ch_h_in, ch_h_out, 3, padding=1, bias=False)\n",
    "        self.hl = nn.Conv2d(ch_h_in, ch_l_out, 3, padding=1, bias=False)\n",
    "        self.lh = nn.Conv2d(ch_l_in, ch_h_out, 3, padding=1, bias=False)\n",
    "        self.ll = nn.Conv2d(ch_l_in, ch_l_out, 3, padding=1, bias=False)\n",
    "\n",
    "        self.bn_h = nn.BatchNorm2d(ch_h_out)\n",
    "        self.bn_l = nn.BatchNorm2d(ch_l_out)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        ch_h = int(round(C*(1 - self.alpha)))\n",
    "        xh = x[:, :ch_h]           # (B, ch_h, H, W)\n",
    "        xl = x[:, ch_h:]           # (B, ch_l, H, W)  \n",
    "\n",
    "        if xl.numel() == 0:\n",
    "            xl = self.pool(xh)     \n",
    "        else:\n",
    "            if xl.shape[-2:] == (H, W):\n",
    "                xl = self.pool(xl)\n",
    "\n",
    "        yh = self.hh(xh) + self.up(self.lh(xl))      # HxW + up(H/2xW/2) -> HxW\n",
    "        yl = self.ll(xl) + self.hl(self.pool(xh))    # H/2xW/2 + H/2xW/2\n",
    "\n",
    "        yh = self.act(self.bn_h(yh))\n",
    "        yl = self.act(self.bn_l(yl))\n",
    "\n",
    "        return torch.cat([yh, self.up(yl)], dim=1)   # ikisi de HxW\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class DynamicConv2d(nn.Module):\n",
    "    def __init__(self, cin, cout, k=3, padding=1, K=4, reduction=4, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.temperature = temperature\n",
    "        hidden = max(1, cin // reduction)\n",
    "        self.router = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(cin, hidden, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, K, 1),\n",
    "        )\n",
    "        self.weight = nn.Parameter(torch.randn(K, cout, cin, k, k) * 0.02)\n",
    "        self.bias = nn.Parameter(torch.zeros(K, cout))\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        logits = self.router(x).flatten(1) / self.temperature   # (B,K)\n",
    "        a = torch.softmax(logits, dim=1)                        # (B,K)\n",
    "\n",
    "        # ağırlık karışımı: (B,K) @ (K,cout,cin,kh,kw) -> (B,cout,cin,kh,kw)\n",
    "        Wmix = torch.einsum(\"bk,kocij->bocij\", a, self.weight)\n",
    "        bmix = torch.einsum(\"bk,ko->bo\", a, self.bias)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(B):\n",
    "            yi = F.conv2d(x[i:i+1], Wmix[i], bmix[i], padding=self.padding)\n",
    "            outs.append(yi)\n",
    "        return torch.cat(outs, dim=0)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class RepVGGBlock(nn.Module):\n",
    "    def __init__(self, cin, cout, stride=1, deploy=False):\n",
    "        super().__init__()\n",
    "        self.deploy = deploy\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.cin, self.cout, self.stride = cin, cout, stride\n",
    "\n",
    "        if deploy:\n",
    "            self.rbr = nn.Conv2d(cin, cout, 3, stride=stride, padding=1, bias=True)\n",
    "        else:\n",
    "            self.rbr3 = nn.Sequential(nn.Conv2d(cin, cout, 3, stride=stride, padding=1, bias=False),\n",
    "                                      nn.BatchNorm2d(cout))\n",
    "            self.rbr1 = nn.Sequential(nn.Conv2d(cin, cout, 1, stride=stride, padding=0, bias=False),\n",
    "                                      nn.BatchNorm2d(cout))\n",
    "            self.idbn = nn.BatchNorm2d(cout) if (cin==cout and stride==1) else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.deploy:\n",
    "            return self.act(self.rbr(x))\n",
    "        out = self.rbr3(x) + self.rbr1(x)\n",
    "        if self.idbn is not None:\n",
    "            out = out + self.idbn(x)\n",
    "        return self.act(out)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "def cords(b, h, w, device, dtype, add_rad=True):\n",
    "    yy = torch.linspace(-1.0, 1.0, steps=h, device=device, dtype=dtype)\n",
    "    xx = torch.linspace(-1.0, 1.0, steps=w, device=device, dtype=dtype)\n",
    "    try:\n",
    "        yv, xv = torch.meshgrid(yy, xx, indexing=\"ij\")\n",
    "    except TypeError:\n",
    "        yv, xv = torch.meshgrid(yy, xx)\n",
    "\n",
    "    x_ch = xv.unsqueeze(0).expand(b, 1, h, w)\n",
    "    y_ch = yv.unsqueeze(0).expand(b, 1, h, w)\n",
    "\n",
    "    if add_rad:\n",
    "        r = torch.sqrt(x_ch**2 + y_ch**2)\n",
    "        return torch.cat([x_ch, y_ch, r], dim=1)\n",
    "    return torch.cat([x_ch, y_ch], dim=1)\n",
    "\n",
    "class CoordConv(nn.Module):\n",
    "    def __init__(self, cin, cout, with_r=True):\n",
    "        super().__init__()\n",
    "        self.with_r = with_r\n",
    "        extra = 3 if with_r else 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(cin + extra, cout, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "        coords = cords(b, h, w, x.device, x.dtype, add_rad=self.with_r)\n",
    "        return self.net(torch.cat([x, coords], dim=1))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "class GroupConv(nn.Module):\n",
    "    def __init__(self, cin, cout, groups=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(cin, cout, 3, padding=1, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# -------------------------\n",
    "# RUN: 12 conv karşılaştırma\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(16, 64, 56, 56, device=device)\n",
    "\n",
    "blocks = {\n",
    "    \"DepthwiseSep\":  DepthwiseSeparableConv(64, 128),\n",
    "    \"Pointwise\":     PointwiseConv(64, 128),\n",
    "    \"InvBottleneck\": InvertedBottleneck(64, 128, expand=4),\n",
    "    \"Ghost\":         GhostConv(64, 128, ratio=2),\n",
    "    \"Dilated\":       DilatedConv(64, 128, dilation=2),\n",
    "    \"Deformable\":    DeformableConv(64, 128, k=3, padding=1),\n",
    "    \"Shift\":         ShiftConv(64, 128),\n",
    "    \"Octave\":        OctaveConv(64, 128, alpha=0.5),\n",
    "    \"Dynamic\":       nn.Sequential(DynamicConv2d(64, 128, k=3, padding=1, K=4), nn.BatchNorm2d(128), nn.ReLU(True)),\n",
    "    \"RepVGG(train)\": RepVGGBlock(64, 128, stride=1, deploy=False),\n",
    "    \"CoordConv\":     CoordConv(64, 128, with_r=True),\n",
    "    \"GroupConv\":     GroupConv(64, 128, groups=4),\n",
    "}\n",
    "\n",
    "print(\"device:\", device, \"| input:\", tuple(x.shape), \"| deformable_real:\", _HAS_DEFORM)\n",
    "for name, blk in blocks.items():\n",
    "    profile_block(name, blk, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff90bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def export_profiles_to_csv(\n",
    "    blocks: dict,\n",
    "    x: torch.Tensor,\n",
    "    csv_path: str = \"conv_profiles.csv\",\n",
    "):\n",
    "\n",
    "    rows = []\n",
    "    for name, blk in blocks.items():\n",
    "        row = profile_block_return_row(name, blk, x) \n",
    "        rows.append(row)\n",
    "\n",
    "    fieldnames = list(rows[0].keys()) if rows else [\n",
    "        \"name\", \"params\", \"macs\", \"macs_m\", \"lat_ms\", \"peak_mb\", \"out_shape\"\n",
    "    ]\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "    print(f\"[OK] CSV yazıldı: {csv_path}\")\n",
    "\n",
    "def profile_block_return_row(name: str, m: nn.Module, x: torch.Tensor):\n",
    "    m = m.to(x.device).eval()\n",
    "\n",
    "    mac_counter = MacCounter()\n",
    "    hooks = []\n",
    "    for mod in m.modules():\n",
    "        if isinstance(mod, (nn.Conv2d, nn.Linear)):\n",
    "            hooks.append(mod.register_forward_hook(mac_counter.hook))\n",
    "\n",
    "    if x.device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y = m(x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    params = count_params(m)\n",
    "    macs = mac_counter.macs\n",
    "    lat_ms = benchmark_latency(m, x, iters=200, warmup=50)\n",
    "\n",
    "    peak_mb = 0.0\n",
    "    if x.device.type == \"cuda\":\n",
    "        peak_mb = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    out_shape = tuple(y.shape)\n",
    "\n",
    "    print(\n",
    "        f\"{name:18s} | params={params:10d} | macs={macs/1e6:10.2f} M | \"\n",
    "        f\"lat={lat_ms:7.3f} ms | peak={peak_mb:7.1f} MB | out={out_shape}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"params\": int(params),\n",
    "        \"macs\": int(macs),\n",
    "        \"macs_m\": float(macs / 1e6),\n",
    "        \"lat_ms\": float(lat_ms),\n",
    "        \"peak_mb\": float(peak_mb),\n",
    "        \"out_shape\": str(out_shape),\n",
    "        \"device\": str(x.device),\n",
    "        \"batch\": int(x.shape[0]),\n",
    "        \"in_shape\": str(tuple(x.shape)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cde11",
   "metadata": {},
   "source": [
    "### hangi durumda hangi conv mantıklı?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ff9f4",
   "metadata": {},
   "source": [
    "| Conv türü                        | Ne işe yarar (öz)                         | Ne zaman mantıklı                                              | Artı                                      | Eksi / Risk                                                |\n",
    "| -------------------------------- | ----------------------------------------- | -------------------------------------------------------------- | ----------------------------------------- | ---------------------------------------------------------- |\n",
    "| **Pointwise (1×1)**              | Kanal karıştırır, boyutu değiştirir       | Blok içinde “kanal artır/azalt”, bottleneck                    | Çok hızlı, ucuz                           | Tek başına uzamsal (spatial) desen yakalamaz               |\n",
    "| **Depthwise Separable**          | Uzamsal + kanal işlemini ayırır           | Mobil/edge, hızlı backbone (MobileNet tarzı)                   | MAC/param çok düşer                       | Aynı FLOP’ta bazen doğruluk düşebilir                      |\n",
    "| **Inverted Bottleneck (MBConv)** | Genişlet → depthwise → sıkıştır           | MobilNetV2/V3, EfficientNet benzeri                            | Verim/accuracy dengesi iyi                | Tasarım parametreleri (expand, SE vs) hassas               |\n",
    "| **GhostConv**                    | “Gerçek” kanalı az üret, kalanı ucuz üret | Hız/param çok kritikse                                         | Çok ucuz; pratikte hızlı                  | Bazı görevlerde temsil gücü düşebilir                      |\n",
    "| **Dilated Conv**                 | Daha geniş receptive field                | Segmentation, context isteyen işler                            | Downsample etmeden geniş görüş            | “Gridding” artefact; küçük objelerde zarar                 |\n",
    "| **Deformable Conv**              | Örnekleme noktaları kayar (adaptif)       | Nesne şekli değişken: detection/segmentation                   | Zor geometrilerde güçlü                   | Daha yavaş/karmaşık; her ortamda stabil değil              |\n",
    "| **ShiftConv**                    | Öğrenmesiz kaydırma + 1×1                 | Çok ucuz uzamsal etki istiyorsan                               | Neredeyse bedava spatial                  | Öğrenme kapasitesi sınırlı; doğruluk riski                 |\n",
    "| **OctaveConv**                   | Frekans ayrımı (high/low)                 | Büyük feature map’lerde verim                                  | Bellek/MAC düşebilir                      | Uygulaması/entegrasyonu daha “özel”, her yerde kazandırmaz |\n",
    "| **DynamicConv (mixture)**        | Girdiye göre kernel karışımı              | Veri çeşitliliği yüksekse, adaptif istenirse                   | Kapasite artar (aynı katmanda adaptasyon) | Latency artar; batch başına döngü pahalı                   |\n",
    "| **RepVGG (train)**               | Train’de çok dal, deploy’da tek conv      | Üretimde hız isterken train’de güçlü yapı                      | Deploy’da çok hızlı                       | Re-parameterize adımı gerekir (deploy sürümü)              |\n",
    "| **CoordConv**                    | X-Y (ve r) koordinat kanalı ekler         | Konum kritik: lokasyon/ısı haritası, keypoint, basit detection | “Konum öğrenmeyi” kolaylaştırır           | Translational invariance azalır; her yerde iyi değil       |\n",
    "| **GroupConv**                    | Kanalları gruplara böler                  | Param/MAC azaltmak, ResNeXt tarzı                              | Verim iyi, kontrol edilebilir             | Gruplar artarsa kanal etkileşimi azalır                    |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
