{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5bd16b",
   "metadata": {},
   "source": [
    "\n",
    "# Bottleneck With Expansion Residual Block (ResNet Bottleneck) — En İnceden En Üste\n",
    "\n",
    "Bu notlar “**Bottleneck + Expansion**” residual bloğun **neden ortaya çıktığını**, **ne problemi çözdüğünü**, **Basic residual** ve **Pre-activation (ResNet v2)** ile **farklarını** en dipten en tepeye kadar anlatır.\n",
    "\n",
    "> Hedef: Bir CNN/Detection backbone’u tasarlarken “**neden bottleneck?**”, “**expansion ne işe yarıyor?**”, “**pre mi post mu?**”, “**projection ne zaman?**” sorularını refleks haline getirmek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc58b81",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 0) Büyük Resim: Neyi çözmeye çalışıyoruz?\n",
    "\n",
    "Derin ağlar (çok katman) teoride daha güçlüdür ama pratikte iki büyük sıkıntı vardır:\n",
    "\n",
    "1. **Optimizasyon zorluğu**: Derinleşince eğitim zorlaşır (loss düşmez / çok yavaş düşer).\n",
    "2. **Gradient akışı**: Çok katmanda gradient sinyali zayıflar veya dengesizleşir.\n",
    "\n",
    "Residual bağlantı (skip) fikri:\n",
    "- Ağın öğrenmesi gerekeni “tam fonksiyon” yerine **düzeltme (residual)** haline getirir.\n",
    "\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "Böylece **kimlik (identity)** yol her zaman açıktır; gradient için otoyol oluşur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558be2a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1) Basic Residual Block (ResNet-18/34) — Referans Noktası\n",
    "\n",
    "### Yapı (Post-activation / ResNet v1)\n",
    "Klasik basic block (çoğunlukla):\n",
    "\n",
    "- Main path: `Conv3×3 → BN → ReLU → Conv3×3 → BN`\n",
    "- Skip path: `Identity` (boyut tutuyorsa)\n",
    "- Toplama: `y = x + F(x)`\n",
    "- Son: `ReLU(y)`  (v1’de genellikle burada)\n",
    "\n",
    "Kısaca:\n",
    "\n",
    "```bash\n",
    "x ────────────────┐\n",
    "      Conv-BN-ReLU│\n",
    "      Conv-BN     │\n",
    "                  ├─ add ─ ReLU ─→ y\n",
    "F(x) ─────────────┘\n",
    "```\n",
    "\n",
    "### Güçlü yön:\n",
    "- Derinleşmeyi pratikte mümkün kılar.\n",
    "\n",
    "### Zayıf yön (derinlik çok artınca):\n",
    "- ReLU/BN sıralaması nedeniyle **skip yol “tam identity” gibi davranmakta zorlanabilir**.\n",
    "- Çok derin ağlarda optimizasyon hâlâ zorlaşabilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0950a97",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2) Pre-Activation Residual Block (ResNet v2) — “Sıra”yı düzeltme\n",
    "\n",
    "Pre-act’in temel farkı: **BN+Act, konvdan önce** gelir.\n",
    "\n",
    "### Pre-act basic block (ResNet v2)\n",
    "- Main path: `BN → ReLU → Conv → BN → ReLU → Conv`\n",
    "- Skip path: mümkünse **pure identity**\n",
    "- Toplama: `y = x + F(BN/ReLU(x))`\n",
    "- Genellikle **add sonrası ReLU yok** (ya da blok sonunda değil, bir sonraki bloğun başında var)\n",
    "\n",
    "Kısaca:\n",
    "\n",
    "```bash\n",
    "x ────────────────┐\n",
    " BN-ReLU-Conv      │\n",
    " BN-ReLU-Conv      │\n",
    "                   ├─ add ─→ y\n",
    "F(·) ──────────────┘\n",
    "```\n",
    "\n",
    "### Neden işe yarıyor?\n",
    "- Skip yol daha “temiz identity” kalır.\n",
    "- Gradient daha rahat akar.\n",
    "- Çok derin ağlarda stabilite artar.\n",
    "\n",
    "> Özet: Basic residual “residual fikrini” getirir, pre-act ise “bu fikri çok derinde daha stabil yapan sıralama”yı getirir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b9b1c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3) Bottleneck With Expansion — “Niye çıktık buraya?”\n",
    "\n",
    "Şimdi asıl mesele: **ağ derinleşti, yetmedi; ayrıca genişlemek istiyoruz**.\n",
    "\n",
    "### Problem\n",
    "Derinliği artırmak kolaylaştı ama:\n",
    "- **Her bloğa 3×3 conv eklemek pahalı** (özellikle kanal sayısı büyüdükçe).\n",
    "- “Daha güçlü temsil” için kanalı büyütmek istersen FLOPs uçar.\n",
    "\n",
    "### Çözüm fikri\n",
    "3×3 konv **en pahalı** kısımdır. Peki 3×3 konvu **daha düşük/daha kontrollü kanalda** çalıştırıp,\n",
    "kanal karışımını **1×1 konvlarla** halledersek?\n",
    "\n",
    "Bu düşünce: **Bottleneck**.\n",
    "\n",
    "### Bottleneck (ResNet-50/101/152) yapısı\n",
    "Klasik ResNet bottleneck:\n",
    "\n",
    "- `1×1 Conv` (**reduce / inner channels**)\n",
    "- `3×3 Conv` (asıl uzamsal iş)\n",
    "- `1×1 Conv` (**expand / output channels**)\n",
    "\n",
    "Kısaca:\n",
    "\n",
    "```bash\n",
    "x ────────────────┐\n",
    " 1×1 (reduce)      │\n",
    " 3×3 (spatial)     │\n",
    " 1×1 (expand)      │\n",
    "                   ├─ add ─→ y\n",
    "F(x) ──────────────┘\n",
    "```\n",
    "\n",
    "### Expansion nedir?\n",
    "ResNet bottleneck’te genelde:\n",
    "- Bloğun “iç kanalı” = `C_mid`\n",
    "- Bloğun çıkışı = `C_mid * expansion`  (çoğunlukla **4**)\n",
    "\n",
    "Yani son 1×1 ile **kanalı büyütür**, böylece bloklar arası temsil gücü artar.\n",
    "\n",
    "> Not: Bu “ResNet bottleneck expansion”dır. MobileNetV2’deki “inverted bottleneck” başka mantık (expand→depthwise→project).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78419dfa",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4) “Neden 1×1 – 3×3 – 1×1?” (en ince sebepler)\n",
    "\n",
    "### 4.1 1×1 konv ne yapar?\n",
    "- Kanallar arası karışım (mixing).\n",
    "- Hesap ucuzdur (uzamsal kernel yok).\n",
    "\n",
    "### 4.2 3×3 konv ne yapar?\n",
    "- Uzamsal desen öğrenimi (kenar, köşe, doku, şekil).\n",
    "- Hesap pahalıdır.\n",
    "\n",
    "### 4.3 Bottleneck stratejisi\n",
    "- 3×3’ü **daha küçük iç kanalda** çalıştır (ucuzlat).\n",
    "- 1×1 ile kanalı önce “hazırla”, sonra “genişlet”.\n",
    "\n",
    "Böylece:\n",
    "- Parametre/FLOPs kontrol altında\n",
    "- Temsil gücü yüksek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657abb9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 5) Basic vs Bottleneck vs Pre-act — net farklar\n",
    "\n",
    "### 5.1 Basic Residual Block\n",
    "- 2 adet 3×3 conv\n",
    "- Daha basit\n",
    "- Küçük/orta derinlikte iyi (ResNet-18/34)\n",
    "\n",
    "### 5.2 Bottleneck With Expansion (ResNet v1)\n",
    "- 1×1 reduce + 3×3 + 1×1 expand\n",
    "- 3×3 sayısı 1’e düşer (maliyet düşer)\n",
    "- Çok derin/çok geniş ağlar için ideal (ResNet-50+)\n",
    "\n",
    "### 5.3 Pre-activation Bottleneck (ResNet v2)\n",
    "- Bottleneck yapısı aynı, ama **BN+Act konvdan önce**\n",
    "- Skip identity daha temiz\n",
    "- Çok derinlerde daha stabil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38758455",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6) Projection (Downsample) ne zaman gerekiyor?\n",
    "\n",
    "Eğer `x` ile `F(x)` aynı boyutta değilse toplama yapılamaz.\n",
    "\n",
    "Bu iki durumda olur:\n",
    "1. **Stride=2** ile uzamsal boyut küçülür (H,W yarıya iner).\n",
    "2. Kanal sayısı değişir (C_in ≠ C_out).\n",
    "\n",
    "Çözüm: skip path’e `1×1 conv (projection)` koymak:\n",
    "\n",
    "\\[\n",
    "y = W_s x + F(x)\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ee13b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 7) Parametre/FLOPs sezgisi (neden bottleneck daha mantıklı?)\n",
    "\n",
    "Aynı giriş/çıkış kanalını düşünelim: `C_out = 256`\n",
    "\n",
    "- Basic block’ta iki adet 3×3 konv 256 kanalda çalışırsa pahalı.\n",
    "- Bottleneck’te iç kanal `C_mid = 64` seçip 3×3’ü 64 kanalda çalıştırırsın.\n",
    "\n",
    "Sonuç: Aynı bütçede çok daha derin model mümkün.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8aa6aa",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 8) Kod: Basic v1, Bottleneck v1, Bottleneck v2 (Pre-act)\n",
    "\n",
    "Aşağıdaki kodlar, blok mantığını net görmek için minimal tutuldu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f65ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv1x1(in_ch, out_ch, stride=1, bias=False):\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0, bias=bias)\n",
    "\n",
    "def conv3x3(in_ch, out_ch, stride=1, bias=False):\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=bias)\n",
    "\n",
    "\n",
    "class BasicBlockV1(nn.Module):\n",
    "    # ResNet v1 style (post-act): Conv-BN-ReLU, Conv-BN, add, ReLU\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_ch, out_ch, stride)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = conv3x3(out_ch, out_ch, 1)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.proj = nn.Sequential(\n",
    "                conv1x1(in_ch, out_ch, stride),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x if self.proj is None else self.proj(x)\n",
    "\n",
    "        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        y = F.relu(identity + y, inplace=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "class BottleneckV1(nn.Module):\n",
    "    # ResNet v1 bottleneck: 1x1 -> 3x3 -> 1x1(expand), add, ReLU\n",
    "    expansion = 4\n",
    "    def __init__(self, in_ch, mid_ch, stride=1):\n",
    "        super().__init__()\n",
    "        out_ch = mid_ch * self.expansion\n",
    "\n",
    "        self.conv1 = conv1x1(in_ch, mid_ch, 1)\n",
    "        self.bn1   = nn.BatchNorm2d(mid_ch)\n",
    "\n",
    "        self.conv2 = conv3x3(mid_ch, mid_ch, stride)\n",
    "        self.bn2   = nn.BatchNorm2d(mid_ch)\n",
    "\n",
    "        self.conv3 = conv1x1(mid_ch, out_ch, 1)\n",
    "        self.bn3   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.proj = nn.Sequential(\n",
    "                conv1x1(in_ch, out_ch, stride),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x if self.proj is None else self.proj(x)\n",
    "\n",
    "        y = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        y = F.relu(self.bn2(self.conv2(y)), inplace=True)\n",
    "        y = self.bn3(self.conv3(y))\n",
    "\n",
    "        y = F.relu(identity + y, inplace=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "class BottleneckV2PreAct(nn.Module):\n",
    "    # ResNet v2 bottleneck: BN-ReLU-Conv order, add (often no ReLU here)\n",
    "    expansion = 4\n",
    "    def __init__(self, in_ch, mid_ch, stride=1):\n",
    "        super().__init__()\n",
    "        out_ch = mid_ch * self.expansion\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_ch)\n",
    "        self.conv1 = conv1x1(in_ch, mid_ch, 1)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2 = conv3x3(mid_ch, mid_ch, stride)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv3 = conv1x1(mid_ch, out_ch, 1)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.proj = conv1x1(in_ch, out_ch, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_p = F.relu(self.bn1(x), inplace=True)\n",
    "\n",
    "        identity = x if self.proj is None else self.proj(x)\n",
    "\n",
    "        y = self.conv1(x_p)\n",
    "        y = self.conv2(F.relu(self.bn2(y), inplace=True))\n",
    "        y = self.conv3(F.relu(self.bn3(y), inplace=True))\n",
    "\n",
    "        return identity + y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f87a5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 9) Hızlı doğrulama: Shape ve downsample\n",
    "\n",
    "- Basic: çıkış kanal = out_ch  \n",
    "- Bottleneck: çıkış kanal = mid_ch * 4  \n",
    "- Stride=2: H,W yarıya iner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5abdabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic v1: (2, 64, 32, 32) -> (2, 64, 32, 32)\n",
      "Basic v1 (ds): (2, 64, 32, 32) -> (2, 128, 16, 16)\n",
      "Bottleneck v1: (2, 64, 32, 32) -> (2, 256, 32, 32)\n",
      "Bottleneck v1 (ds): (2, 64, 32, 32) -> (2, 256, 16, 16)\n",
      "Bottleneck v2 pre-act: (2, 64, 32, 32) -> (2, 256, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "x = torch.randn(2, 64, 32, 32, device=device)\n",
    "\n",
    "b1 = BasicBlockV1(64, 64, stride=1).to(device)\n",
    "y1 = b1(x)\n",
    "print(\"Basic v1:\", tuple(x.shape), \"->\", tuple(y1.shape))\n",
    "\n",
    "b2 = BasicBlockV1(64, 128, stride=2).to(device)\n",
    "y2 = b2(x)\n",
    "print(\"Basic v1 (ds):\", tuple(x.shape), \"->\", tuple(y2.shape))\n",
    "\n",
    "bn1 = BottleneckV1(64, 64, stride=1).to(device)   # out = 64*4=256\n",
    "y3 = bn1(x)\n",
    "print(\"Bottleneck v1:\", tuple(x.shape), \"->\", tuple(y3.shape))\n",
    "\n",
    "bn2 = BottleneckV1(64, 64, stride=2).to(device)\n",
    "y4 = bn2(x)\n",
    "print(\"Bottleneck v1 (ds):\", tuple(x.shape), \"->\", tuple(y4.shape))\n",
    "\n",
    "v2 = BottleneckV2PreAct(64, 64, stride=1).to(device)\n",
    "y5 = v2(x)\n",
    "print(\"Bottleneck v2 pre-act:\", tuple(x.shape), \"->\", tuple(y5.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d6c5d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 10) Detection için pratik kararlar\n",
    "\n",
    "- **ResNet-18/34 tarzı**: basic block\n",
    "- **ResNet-50+ backbone**: bottleneck\n",
    "- **Çok derin + stabilite**: pre-act bottleneck (v2)\n",
    "- **Boyut değişiyorsa**: projection şart\n",
    "- **Attention ekliyorsan**: çoğunlukla `F(x)` yoluna ekle, skip’i temiz bırak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a3afb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 11) 1 paragraf final özet\n",
    "\n",
    "Bottleneck with expansion residual block, basic residual bloğun “derin ve geniş ağları aynı hesap bütçesinde çalıştırma” ihtiyacından doğmuş halidir. İki pahalı 3×3 yerine 3×3’ü tek yere indirir, kanalı 1×1 ile önce düzenler sonra expansion ile büyütür. Böylece hem temsil gücü artar hem FLOPs/parametre kontrol altında kalır. Pre-activation varyantı (ResNet v2) ise aynı tasarımı BN+Activation’ı konvlardan önce alarak daha temiz identity skip ve daha stabil gradient akışı sağlar.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
