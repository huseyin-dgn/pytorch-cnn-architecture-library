{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Residual Block (ResNet) — Baştan Sona Detaylı Notlar\n",
        "\n",
        "Bu notların amacı: **basic residual block** mantığını sıfırdan kurup, hem sezgisel hem de uygulamaya dönük şekilde öğrenmek.  \n",
        "Konu bittiğinde şu üç soruya net cevap verebiliyor olacaksın:\n",
        "\n",
        "1) *Skip connection neden var, neyi çözüyor?*  \n",
        "2) *Identity shortcut ile projection shortcut farkı ne?*  \n",
        "3) *PyTorch’ta “doğru” residual block nasıl yazılır ve hangi hatalar sık yapılır?*\n",
        "\n",
        "---\n",
        "\n",
        "## 0) Residual fikri “neden” ortaya çıktı?\n",
        "\n",
        "Derin ağları (çok katmanlı CNN’leri) büyüttükçe iki büyük problem artar:\n",
        "\n",
        "- **Optimization zorluğu**: Ağ derinleşince eğitim zorlaşır, kayıp (loss) düşmesi yavaşlar veya takılır.  \n",
        "- **Degradation (bozulma) problemi**: Daha derin bir ağ, teoride daha iyi olmalı gibi görünür; ama pratikte bazen **daha kötü** performans verir.\n",
        "\n",
        "Bu ikinci sorun çok önemli: *Bu, overfitting değil.*  \n",
        "Overfitting’de eğitim hatası düşer ama test bozulur. Degradation’da ise **eğitim hatası bile düşmeyebilir**.\n",
        "\n",
        "Residual bağlantının ana fikri şudur:\n",
        "\n",
        "> “Ağa yeni bir şey öğrenmeyi zorla; ama olmazsa en azından kimlik (identity) gibi davranıp performansı bozmamasını sağla.”\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Residual öğrenme: `H(x)` yerine `F(x)`\n",
        "\n",
        "Klasik bir blok şunu öğrenmeye çalışır:\n",
        "\n",
        "- Girdi: `x`\n",
        "- Çıkış hedefi: `H(x)`\n",
        "\n",
        "Residual blok ise şunu yapar:\n",
        "\n",
        "- Öğrenilecek şey: `F(x) = H(x) - x`\n",
        "- Sonuç: `H(x) = F(x) + x`\n",
        "\n",
        "Yani blok **doğrudan H(x)’i** öğrenmiyor; `x`’in üzerine eklenecek “düzeltmeyi” öğreniyor.\n",
        "\n",
        "Bu sayede:\n",
        "- Eğer “düzeltme” gerekmiyorsa, `F(x) ≈ 0` olur ve blok **x’i aynen geçirir**.\n",
        "- Derin ağda her katmanın “sıfırdan bir fonksiyon” öğrenmesi yerine, küçük düzeltmeler öğrenmesi daha kolay olur.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Basic residual block’un matematiksel formu\n",
        "\n",
        "En temel form: \n",
        "\n",
        "\\begin{aligned}\n",
        "y &= F(x) + x\n",
        "\\end{aligned}\n",
        "\n",
        "- `x`: blok girişi. x, residual bloktan önceki feature map, yani **bloğun girdisi (input tensor)**dur\n",
        "- `F(x)`: blok içindeki conv’lar + aktivasyonlar + normalizasyonun oluşturduğu dönüşüm\n",
        "- `+ x`: skip connection (shortcut)\n",
        "\n",
        "Bu toplama işlemi element-wise yapılır. Bu yüzden **boyutlar eşit olmalı**:\n",
        "- Kanal sayısı (C)\n",
        "- Yükseklik (H)\n",
        "- Genişlik (W)\n",
        "\n",
        "Boyut eşitliği bozulursa, `x` doğrudan eklenemez — işte burada projection shortcut devreye girer (aşağıda).\n",
        "\n",
        "**Residual’da toplama, öğrenilmesi gereken fonksiyonu sadeleştirir ve gradyanın bozulmadan akmasını sağlar.**\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Basic Residual Block yapısı (ResNet-18 / ResNet-34)\n",
        "\n",
        "“Basic” denince genelde şu blok kastedilir:\n",
        "\n",
        "- 3×3 Conv → BN → ReLU\n",
        "- 3×3 Conv → BN\n",
        "- Skip ekle (`+ x`)\n",
        "- ReLU\n",
        "\n",
        "Şematik:\n",
        "\n",
        "```bash\n",
        "x ────────────────┐\n",
        "                  │\n",
        "      3×3 Conv     │\n",
        "      BN           │\n",
        "      ReLU         │\n",
        "      3×3 Conv     │\n",
        "      BN           │\n",
        "                  (+) → ReLU → y\n",
        "                  │\n",
        "x (shortcut) ──────┘\n",
        "```\n",
        "\n",
        "Burada dikkat:\n",
        "- İlk conv genelde `stride=1` (aynı çözünürlük) veya blok geçişlerinde `stride=2` (downsample) olabilir.\n",
        "- İkinci conv genelde `stride=1` kalır.\n",
        "- Toplamadan sonra bir ReLU uygulanır (basic ResNet’in orijinal tasarımında).\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Identity shortcut vs Projection shortcut\n",
        "\n",
        "### 4.1 Identity shortcut (en basit ve en iyi senaryo)\n",
        "\n",
        "Boyutlar aynıysa:\n",
        "\n",
        "- `shortcut(x) = x`\n",
        "\n",
        "Yani skip path üzerinde hiçbir katman yok.\n",
        "\n",
        "Avantajları:\n",
        "- Parametre eklemez\n",
        "- Gradient akışı çok temiz olur\n",
        "\n",
        "### 4.2 Projection shortcut (boyut eşleme gerektiğinde)\n",
        "\n",
        "Boyutlar farklıysa (örneğin stride=2 ile H,W yarıya düşürülmüşse veya kanal sayısı değişmişse):\n",
        "\n",
        "- `shortcut(x) = W_s * x` şeklinde bir dönüşüm gerekir.\n",
        "- Pratikte çoğunlukla **1×1 convolution** kullanılır:\n",
        "\n",
        "`Conv1x1(stride=s, out_channels=...)`\n",
        "\n",
        "Bu durumda blok:\n",
        "\n",
        "\\begin{aligned}\n",
        "y = F(x) + \\text{proj}(x)\n",
        "\\end{aligned}\n",
        "\n",
        "Bu 1×1 conv hem:\n",
        "- Kanal sayısını eşler\n",
        "- Gerekirse stride ile çözünürlüğü düşürür\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Downsampling (stage geçişi) mantığı\n",
        "\n",
        "\n",
        "**Downsampling, bir feature map’in uzamsal çözünürlüğünü (H, W) bilinçli olarak düşürme işlemidir.**\n",
        "\n",
        "ResNet mimarisinde çözünürlük stage stage düşer. Örnek:\n",
        "\n",
        "- Stage1: 56×56\n",
        "- Stage2: 28×28\n",
        "- Stage3: 14×14\n",
        "- Stage4: 7×7\n",
        "\n",
        "Stage geçişinde genelde ilk blok:\n",
        "- Main path’te `stride=2` ile downsample yapar\n",
        "- Skip path’te de **projection** ile boyutu eşler\n",
        "\n",
        "Bu yüzden “basic residual block” öğrenirken şu ayrımı iyi bil:\n",
        "\n",
        "- **Same-shape block**: identity shortcut\n",
        "- **Downsample block**: projection shortcut\n",
        "\n",
        "---\n",
        "\n",
        "## 6) “Toplama” neden bu kadar işe yarıyor? (sezgi)\n",
        "\n",
        "Toplama, gradient’in geri akmasını kolaylaştırır. Kabaca:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial y}{\\partial x} = \\frac{\\partial F(x)}{\\partial x} + I\n",
        "\\end{aligned}\n",
        "\n",
        "Buradaki `+ I` (identity) kısmı şu demek:\n",
        "- `F(x)` tarafı kötü davranıp türevleri küçültse bile, en azından identity yolundan gradient geçer.\n",
        "- Bu, çok derin ağlarda “öğrenememe/tıkanma” riskini ciddi azaltır.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Basic residual block’ta sık yapılan hatalar\n",
        "\n",
        "1) **Boyut eşleşmesini unutmak**  \n",
        "   - Main path `stride=2` yapıp skip path’i identity bırakmak → toplama patlar.\n",
        "\n",
        "2) **ReLU yerleşimini karıştırmak**  \n",
        "   - Basic block’ta tipik sıra: (Conv→BN→ReLU→Conv→BN→Add→ReLU)\n",
        "\n",
        "3) **BN kullanımını kapatıp aynı davranışı beklemek**  \n",
        "   - BN yoksa daha dikkatli init / LR gerekir.\n",
        "\n",
        "4) **inplace ReLU yüzünden gereksiz bug’lar**  \n",
        "   - Çoğu durumda sorun çıkmaz, ama bazı debug senaryolarında işleri zorlaştırabilir.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) PyTorch ile implementasyon (temiz ve okunabilir)\n",
        "\n",
        "Aşağıdaki implementasyon:\n",
        "- Identity shortcut’u otomatik kullanır\n",
        "- Boyut gerektiğinde projection’ı otomatik kurar\n",
        "- ResNet-18/34 tarzına uygundur\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicResidualBlock(nn.Module):\n",
        "    expansion = 1  \n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels,\n",
        "            kernel_size=3, stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels,\n",
        "            kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        out = out + identity\n",
        "        out = self.relu(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hızlı şekil testi\n",
        "\n",
        "İki senaryoya bakıyoruz:\n",
        "\n",
        "- **Case A (identity shortcut)**: `in=64, out=64, stride=1`\n",
        "- **Case B (projection shortcut)**: `in=64, out=128, stride=2` (hem kanal hem çözünürlük değişiyor)\n",
        "\n",
        "Amaç: `out + identity` toplamasının sorunsuz çalıştığını görmek.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(2, 64, 56, 56)\n",
        "\n",
        "block_identity = BasicResidualBlock(64, 64, stride=1)\n",
        "y1 = block_identity(x)\n",
        "print(\"Identity block:\", x.shape, \"->\", y1.shape)\n",
        "\n",
        "block_proj = BasicResidualBlock(64, 128, stride=2)\n",
        "y2 = block_proj(x)\n",
        "print(\"Projection block:\", x.shape, \"->\", y2.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Basic block ne zaman tercih edilir?\n",
        "\n",
        "- ResNet-18 / ResNet-34 gibi daha “küçük/orta” modellerde kullanılır.\n",
        "- Daha derin ResNet’lerde (50/101/152) genelde **bottleneck** tercih edilir.\n",
        "- Basic block’un artısı: **basit, stabil, anlaşılır**.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 10) Kısa özet (tek paragraf)\n",
        "\n",
        "Basic residual block, iki adet 3×3 conv ile bir dönüşüm `F(x)` üretir ve bunu girdi `x` ile toplayarak `y = F(x) + x` çıkışı oluşturur.  \n",
        "Boyutlar aynıysa shortcut identity olur; farklıysa 1×1 projection ile eşlenir. Bu toplama, derin ağlarda gradient’in daha rahat akmasını sağlayarak eğitimde tıkanmayı azaltır ve çok derin yapılarda optimizasyonu kolaylaştırır.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
