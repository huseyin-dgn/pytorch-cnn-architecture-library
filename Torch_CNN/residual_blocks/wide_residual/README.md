# Wide Residual Blok (Wide ResNet YaklaÅŸÄ±mÄ±)

Bu yapÄ±, klasik residual mimarinin **derinleÅŸtirilmesi yerine geniÅŸletilmesi** fikrine dayanÄ±r. AmaÃ§: AÄŸÄ± aÅŸÄ±rÄ± derin yapmak yerine kanallarÄ± artÄ±rarak daha gÃ¼Ã§lÃ¼ temsil Ã¶ÄŸrenmek.

---

## ğŸ¯ Temel Denklem

**Ã‡Ä±kÄ±ÅŸ = F(x) + Skip(x)**

Residual mantÄ±k deÄŸiÅŸmez; fark, bloklarÄ±n **kanal sayÄ±sÄ±nÄ±n bÃ¼yÃ¼tÃ¼lmÃ¼ÅŸ olmasÄ±dÄ±r**.

---

## ğŸ§  Blok YapÄ±sÄ±

Bu blok **pre-activation residual** dÃ¼zenini kullanÄ±r:

**BN â†’ ReLU â†’ Conv â†’ (Dropout) â†’ BN â†’ ReLU â†’ Conv**

Toplama en sonda yapÄ±lÄ±r. Aktivasyon toplama sonrasÄ± ayrÄ± uygulanmaz.

Bu sÄ±ralama:

âœ” Gradyan akÄ±ÅŸÄ±nÄ± iyileÅŸtirir  
âœ” Ã‡ok katmanlÄ± geniÅŸ aÄŸlarda stabilite saÄŸlar

---

## ğŸ”€ â€œWideâ€ Ne Demek?

| Parametre            | AnlamÄ±                               |
| -------------------- | ------------------------------------ |
| **widen_factor (k)** | TÃ¼m stage kanallarÄ±nÄ± Ã§arpan katsayÄ± |
| **depth = 6n+4**     | KaÃ§ residual blok olduÄŸu             |

Kanal artÄ±ÅŸÄ±:

- Stage 1: 16 Ã— k
- Stage 2: 32 Ã— k
- Stage 3: 64 Ã— k

Yani aÄŸ derinleÅŸmez, **her katman daha geniÅŸ temsil kapasitesine sahip olur**.

---

## ğŸ” Skip Yolu

Standart residual kuralÄ± geÃ§erli:

| Durum                       | Ä°ÅŸlem           |
| --------------------------- | --------------- |
| Boyutlar aynÄ±               | Identity        |
| Kanal veya stride deÄŸiÅŸiyor | 1Ã—1 projeksiyon |

---

## ğŸ’¡ Dropout Neden Var?

Wide ResNetâ€™te geniÅŸlik arttÄ±kÃ§a overfitting riski artar.  
Blok ortasÄ±na konan dropout:

âœ” Regularization saÄŸlar  
âœ” GeniÅŸ aÄŸÄ±n aÅŸÄ±rÄ± ezberlemesini Ã¶nler

---

## ğŸ†š Klasik ResNet ile FarkÄ±

|               | ResNet       | Wide ResNet         |
| ------------- | ------------ | ------------------- |
| Derinlik      | YÃ¼ksek       | Orta                |
| Kanal sayÄ±sÄ±  | Daha dar     | Daha geniÅŸ          |
| Ã–ÄŸrenme stili | Derin temsil | GeniÅŸ temsil        |
| Performans    | Ä°yi          | Ã‡oÄŸu zaman daha iyi |

---

## ğŸš€ Neden Etkili?

âœ” Daha iyi gradyan akÄ±ÅŸÄ±  
âœ” Daha az katmanla yÃ¼ksek kapasite  
âœ” EÄŸitimi daha hÄ±zlÄ±  
âœ” CIFAR tarzÄ± veri setlerinde Ã§ok gÃ¼Ã§lÃ¼ sonuÃ§lar

---

## ğŸ”š Ã–zet

Wide residual yapÄ±:

**Pre-activation residual tasarÄ±m + kanal geniÅŸletme (k faktÃ¶rÃ¼)**  
kullanarak derinliÄŸi artÄ±rmadan model kapasitesini yÃ¼kseltir.
