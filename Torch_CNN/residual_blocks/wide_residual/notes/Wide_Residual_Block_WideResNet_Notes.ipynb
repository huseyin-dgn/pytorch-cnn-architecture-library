{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc63ebe",
   "metadata": {},
   "source": [
    "-------\n",
    "-------\n",
    "-------\n",
    "-------\n",
    "-------\n",
    "-------\n",
    "# Wide Residual, derinliği şişirmeden,geniş ve güçlü dönüşümleri residual ile eğitilebilir hale getirerek,bazı görevlerde çok derin ağlardan daha verimli sonuçlar verebilir.\n",
    "-------\n",
    "-------\n",
    "-------\n",
    "-------\n",
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09d139",
   "metadata": {},
   "source": [
    "# Wide Residual Block (Wide ResNet) — Temelden İleri Seviyeye Notlar\n",
    "\n",
    "> Bu notlar, **Wide Residual Block** (WRB) ve **Wide ResNet (WRN)** fikrini, klasik residual bağlantıdan başlayıp mimari kararlar, motivasyon ve pratik PyTorch uygulamasına kadar adım adım anlatır.  \n",
    "> Amaç: Konuyu **her baktığında hatırlayabileceğin** netlikte, **karmaşıklaştırmadan** ama **eksik bırakmadan** özetlemek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836a451",
   "metadata": {},
   "source": [
    "## 1) Önce temel: Residual bağlantı (değişmeyen iskelet)\n",
    "\n",
    "Residual blokların çekirdeği:\n",
    "\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "- **Skip/Shortcut path**: \\(x\\) (identity) veya boyutlar uymuyorsa projection.\n",
    "- **Main path**: Öğrenilebilir dönüşüm \\(F(x)\\) (Conv/BN/ReLU vb.).\n",
    "\n",
    "Bu iskelet, **Basic**, **Pre-Act**, **Bottleneck**, **ResNeXt**, **Wide** gibi tüm varyantlarda aynı kalır.  \n",
    "Farklar, yalnızca \\(F(x)\\)'in **nasıl parametrikleştirildiği** ve **kanal/stride** düzenindedir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883ad28",
   "metadata": {},
   "source": [
    "## 2) Wide Residual Block nedir?\n",
    "\n",
    "**Wide Residual Block**, residual fikrini ( \\(y=x+F(x)\\) ) bozmaz.  \n",
    "Yaptığı şey:\n",
    "\n",
    "> **Derinliği aşırı artırmak yerine kanalları (width) artırarak kapasiteyi büyütmek.**\n",
    "\n",
    "“Wide”, uzamsal genişlik değil; **kanal sayısı** demektir:\n",
    "- 64 kanal → 128/256 kanal gibi **genişletme**\n",
    "- Daha az katmanla daha yüksek temsil gücü hedefi\n",
    "\n",
    "Bu yaklaşımın yaygın formu **Wide ResNet (WRN)**’tir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e5185",
   "metadata": {},
   "source": [
    "## 3) Neden ortaya çıktı? (motivasyon)\n",
    "\n",
    "Klasik (çok derin) ResNet’lerde pratik gözlemler:\n",
    "1. Aşırı derinlikte bazı bloklar öğrenme sırasında **identity'e yakın** davranabilir.\n",
    "2. Derinlik arttıkça eğitim/çalıştırma maliyeti büyür; GPU verimliliği her zaman lineer artmaz.\n",
    "3. “Daha derin = daha iyi” yaklaşımı her zaman optimum değildir.\n",
    "\n",
    "Wide ResNet fikri:\n",
    "- Derinliği ölçülü tut (ör. 16–40 bandı),\n",
    "- **Kanal genişliğini artır** (widen factor ile),\n",
    "- Daha güçlü bloklarla daha iyi doğruluk / eğitim verimliliği hedefle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bb656",
   "metadata": {},
   "source": [
    "## 4) Wide ResNet isimlendirmesi: WRN-\\(d\\)-\\(k\\)\n",
    "\n",
    "- **\\(d\\)**: derinlik parametresi (blok/layer sayısı düzeni)\n",
    "- **\\(k\\)**: widen factor (genişlik çarpanı)\n",
    "\n",
    "Örnek: **WRN-28-10**\n",
    "- \\(d=28\\)\n",
    "- \\(k=10\\): kanal sayıları temel düzenin 10 katı ölçeğinde\n",
    "\n",
    "Pratik yorum:\n",
    "- \\(k\\) büyüdükçe kanallar artar → kapasite artar → FLOPs/parametre artar.\n",
    "- \\(d\\) büyüdükçe blok sayısı artar → maliyet artar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb748e27",
   "metadata": {},
   "source": [
    "## 5) Wide Residual Block’un tipik iç yapısı\n",
    "\n",
    "Yaygın blok formu **pre-activation** düzeniyle yazılır:\n",
    "\n",
    "```bash\n",
    "x\n",
    "│\n",
    "├─ BN → ReLU → Conv 3×3  (geniş kanallar)\n",
    "│\n",
    "├─ BN → ReLU → Conv 3×3  (geniş kanallar)\n",
    "│\n",
    "└─ Skip: identity veya 1×1 projection\n",
    "        ↓\n",
    "      Toplama (x + F(x))\n",
    "```\n",
    "\n",
    "### Neden pre-activation?\n",
    "BN/ReLU’nun toplamadan önce gelmesi, gradyan akışını pratikte iyileştirir (eğitim stabilitesi).\n",
    "\n",
    "### Dropout nerede?\n",
    "Bazı WRN varyantlarında 1. ve 2. conv arasına **dropout** eklenir (overfitting azaltmak için).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411015e",
   "metadata": {},
   "source": [
    "## 6) “Wide” tam olarak nerede? (kanal genişliği)\n",
    "\n",
    "Wide yaklaşımında ana kontrol düğmesi:\n",
    "\n",
    "\\[\n",
    "\\text{out\\_ch} = k \\times \\text{base\\_ch}\n",
    "\\]\n",
    "\n",
    "- **base_ch**: stage’in temel kanal sayısı\n",
    "- **k (widen factor)**: kanalı kaç kat büyüteceğin\n",
    "\n",
    "Örnek:\n",
    "- base_ch = 64, k = 2 → out_ch = 128\n",
    "- base_ch = 64, k = 4 → out_ch = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378f35d",
   "metadata": {},
   "source": [
    "## 7) Wide Residual Block vs diğer residual varyantlar (kısa kıyas)\n",
    "\n",
    "| Varyant | Temel fikir | \\(F(x)\\) içi | Kapasiteyi artırma yolu |\n",
    "|---|---|---|---|\n",
    "| Basic | Basit residual | 3×3 → 3×3 | Derinlik / stage kanalı |\n",
    "| Pre-Act | Optimizasyon iyileştirme | (BN+ReLU) + conv | Eğitim stabilitesi |\n",
    "| Bottleneck | Hesap verimliliği | 1×1 → 3×3 → 1×1 | Derinliği ucuza artırma |\n",
    "| ResNeXt | Cardinality | 1×1 → **grouped** 3×3 → 1×1 | **groups** ile çoklu temsil |\n",
    "| **Wide** | Width | (genelde) 3×3 → 3×3 (geniş) | **kanalları büyütme (k)** |\n",
    "\n",
    "**Özet:** Wide = “tek dönüşümü güçlendir”  \n",
    "ResNeXt = “çoklu küçük dönüşüm (cardinality)”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200b540",
   "metadata": {},
   "source": [
    "## 8) Hangi alanlarda kullanılır?\n",
    "\n",
    "### 8.1) Sınıflandırma (Classification)\n",
    "- Wide ResNet özellikle CIFAR tarzı benchmark’larda güçlü sonuçlarıyla bilinir.\n",
    "- Overfitting riskine karşı dropout/augmentasyon ile iyi çalışır.\n",
    "\n",
    "### 8.2) Detection (Object Detection)\n",
    "- Backbone olarak kullanılabilir (feature extractor).\n",
    "- Ancak kanal genişliği arttıkça FLOPs ve latency artar.\n",
    "- Detection’da residual yapılar genelde backbone’da yoğunlaşır; head tarafında aşırı residual çoğu zaman gereksizdir.\n",
    "\n",
    "### 8.3) Segmentation\n",
    "- Güçlü backbone ihtiyacında kullanılabilir; maliyet dengesi önemlidir.\n",
    "\n",
    "**Pratik kural:** Accuracy odaklıysanız wide avantajlı olabilir; gerçek zamanlı/edge odakta daha ölçülü olunmalıdır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9aa585",
   "metadata": {},
   "source": [
    "## 9) PyTorch ile Wide Residual Block (WRB) — Referans Uygulama\n",
    "\n",
    "Aşağıdaki kod:\n",
    "- Pre-activation düzeni kullanır (BN→ReLU→Conv)\n",
    "- Gerekirse skip için projection (1×1) ekler\n",
    "- İsteğe bağlı dropout içerir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WideResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p) if dropout_p and dropout_p > 0 else None\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        # Bu kanal değişimini burada yapıyor:\n",
    "                # base → wide (64 → 128)\n",
    "        self.proj = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.proj = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-activation\n",
    "        out = self.bn1(x)\n",
    "        out = F.relu(out, inplace=True)\n",
    "\n",
    "        # Skip path (identity veya projection)\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "\n",
    "        # Main path\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        #\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Residual toplama\n",
    "        out = out + skip\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff939f",
   "metadata": {},
   "source": [
    "### 9.1) Hızlı shape testi\n",
    "- `stride=1` → uzamsal boyut değişmez  \n",
    "- `stride=2` → uzamsal boyut yarıya düşer (skip projection devreye girer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa34136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x     : torch.Size([2, 64, 56, 56])\n",
      "y_same: torch.Size([2, 64, 56, 56])\n",
      "y_down: torch.Size([2, 128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "blk_same = WideResidualBlock(in_ch=64, out_ch=64, stride=1, dropout_p=0.0)\n",
    "y_same = blk_same(x)\n",
    "\n",
    "blk_down = WideResidualBlock(in_ch=64, out_ch=128, stride=2, dropout_p=0.0)\n",
    "y_down = blk_down(x)\n",
    "\n",
    "print(\"x     :\", x.shape)\n",
    "print(\"y_same:\", y_same.shape)\n",
    "print(\"y_down:\", y_down.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffaef75",
   "metadata": {},
   "source": [
    "## 10) Wide fikrini widen factor ile stage tasarımına bağlamak\n",
    "\n",
    "Basit bir stage örneği:\n",
    "- base kanal: 64\n",
    "- widen factor: \\(k=2\\) → stage kanal: 128\n",
    "\n",
    "Genişlik stratejisi: aynı derinlikte daha fazla kanal kullanarak temsil gücünü artırır.\n",
    "\n",
    "> Not: Genişlik artışı parametre/FLOPs’ı hızlı büyütebilir. Detection için dengeli seçilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239f989",
   "metadata": {},
   "source": [
    "## 11) Wide Residual Block'u modele bağlamak (mini backbone örneği)\n",
    "\n",
    "Aşağıdaki mini model:\n",
    "- Stem ile 3→64 kanal çıkarır\n",
    "- Ardından 2 adet WideResidualBlock uygular\n",
    "- Sonunda global pooling ile çıktı üretir (classifier iskeleti)\n",
    "\n",
    "Detection tarafında aynı mantıkla bloklar backbone’a yerleştirilir; üstüne neck + head eklenir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb68065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: torch.Size([2, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "class TinyWRNBackbone(nn.Module):\n",
    "    def __init__(self, widen_factor: int = 2, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        base = 64\n",
    "        wide = base * widen_factor\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, base, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # 3 kanal = renk\n",
    "\n",
    "        self.block1 = WideResidualBlock(in_ch=base, out_ch=wide, stride=1, dropout_p=dropout_p)\n",
    "        self.block2 = WideResidualBlock(in_ch=wide, out_ch=wide, stride=1, dropout_p=dropout_p)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "m = TinyWRNBackbone(widen_factor=2, dropout_p=0.0)\n",
    "inp = torch.randn(2, 3, 64, 64)\n",
    "out = m(inp)\n",
    "print(\"out:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5920b",
   "metadata": {},
   "source": [
    "## 12) Detection (YOLO) tarafında “nerede kullanılır?” (pratik rehber)\n",
    "\n",
    "YOLO gibi detector’larda mimari üç parçaya ayrılır:\n",
    "1. **Backbone**: feature extractor (residual blokların en doğal yeri)\n",
    "2. **Neck**: multi-scale feature fusion (FPN/PAN; hafif residual bazen olur)\n",
    "3. **Head**: sınıflandırma + bbox regresyonu (genelde sade olmalı)\n",
    "\n",
    "Wide Residual için öneri:\n",
    "- Backbone’un orta/derin stage’lerinde, ölçülü widen factor ile kullanmak mantıklı.\n",
    "- Çok erken stage’de (yüksek çözünürlükte) “aşırı wide” latency’yi hızlı artırır.\n",
    "- Head içinde çok derin residual çoğu zaman gereksizdir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb4b0b",
   "metadata": {},
   "source": [
    "## 13) Sık yapılan hatalar (kontrol listesi)\n",
    "\n",
    "- **Skip ve F(x) shape uyumsuzluğu**: `in_ch != out_ch` veya `stride!=1` ise projection şart.\n",
    "- **Aşırı genişlik**: Detection’da FPS düşürür; k’yı küçük tutarak başlamak daha güvenlidir.\n",
    "- **Normalization/activation yerleşimi**: Pre-act düzeni eğitim stabilitesi açısından pratikte güçlüdür.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f543f0",
   "metadata": {},
   "source": [
    "## 14) Mini özet\n",
    "\n",
    "- Residual iskelet değişmez: \\(y=x+F(x)\\)\n",
    "- Wide yaklaşımı: **derinlik yerine kanal genişliğiyle kapasite artırma**\n",
    "- En sık kullanım: sınıflandırma backbone’u\n",
    "- Detection’da kullanım: backbone içinde ölçülü (özellikle orta/derin katmanlar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
