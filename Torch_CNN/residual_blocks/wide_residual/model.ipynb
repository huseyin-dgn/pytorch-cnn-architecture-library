{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d846e4c9",
   "metadata": {},
   "source": [
    "# Aşama 1 — Wide Residual Block’u adım adım yazma\n",
    "\n",
    "### 1) Hedef: Ne üretmek istiyoruz?\n",
    "\n",
    "Wide residual block’ın omurgası değişmez:\n",
    "* **y=skip(x)+F(x)**\n",
    "Burada:\n",
    "\n",
    "* skip(x): ya x (identity), ya da projection (1×1 conv)\n",
    "\n",
    "* F(x): 2 tane 3×3 conv (wide = kanalı büyütme burada)\n",
    "## Kural: Pre-activation düzeni\n",
    "\n",
    "Wide ResNet’te yaygın düzen:\n",
    "\n",
    "* BN + ReLU → Conv (activation conv’dan önce)\n",
    "\n",
    "* sonra yine BN + ReLU → Conv\n",
    "\n",
    "* Dropout varsa genelde iki conv arasına koyulur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b546c",
   "metadata": {},
   "source": [
    "## Aşama 1 Kod — Parça parça yazıyoruz \n",
    "* 1×1 conv = her pikselde kanal vektörüne uygulanan bir lineer dönüşüm.\n",
    "Kanalları “birleştirir” çünkü yeni kanallar, eskilerin ağırlıklı toplamıdır.\n",
    "“Shape düzenler” çünkü C’yi değiştirir, stride verirsek H×W’yi de değiştirir.\n",
    "### 1.1) En sade WideResidualBlock (dropout yok)\n",
    "* Aşağıdaki kod “çekirdek blok”tur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720596d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WideResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn1   = nn.BatchNorm2d(in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.proj = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Skip\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "\n",
    "        # F(x)\n",
    "        out = self.bn1(x)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Residual sum\n",
    "        out = out + skip\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957cf46",
   "metadata": {},
   "source": [
    "### 1.2) Dropout eklemek (opsiyonel)\n",
    "\n",
    "* Dropout blok güçlü olduğu için overfitting’i azaltmak amacıyla eklenir. Tipik yer: conv1 sonrası."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb00ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn1   = nn.BatchNorm2d(in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p) if dropout_p and dropout_p > 0 else None\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.proj = nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out + skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbdeea7",
   "metadata": {},
   "source": [
    "### 1.3) “Çalışıyor mu?” shape testi (kritik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3093e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 56, 56]) torch.Size([2, 64, 56, 56]) torch.Size([2, 128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "blk_id = WideResidualBlock(64, 64, stride=1, dropout_p=0.0)\n",
    "y1 = blk_id(x)  # [2, 64, 56, 56]\n",
    "\n",
    "blk_proj = WideResidualBlock(64, 128, stride=2, dropout_p=0.0)\n",
    "y2 = blk_proj(x)  # [2, 128, 28, 28]\n",
    "\n",
    "print(x.shape, y1.shape, y2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5bb87",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21ac10",
   "metadata": {},
   "source": [
    "# Aşama 2-) Modeli profesyonel hale getirelim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5baa682",
   "metadata": {},
   "source": [
    "### 2.1) Stage builder: “N tane blok” üretmek ✅\n",
    "\n",
    "Kural:\n",
    "\n",
    "* Stage’in ilk bloğu stride alır (downsample gerekiyorsa)\n",
    "\n",
    "- Sonrakiler stride=1\n",
    "\n",
    "* İlk blokta in_ch -> out_ch dönüşümü olur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c13fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wide_layer(in_ch:int,out_ch:int,num_block:int,stride:int,dropout_p:float):\n",
    "    layers = []\n",
    "    layers.append(WideResidualBlock(in_ch,out_ch,stride=stride,dropout_p=dropout_p))\n",
    "    for _ in range(1,num_block):\n",
    "        layers.append(WideResidualBlock(out_ch,out_ch,stride=1,dropout_p=dropout_p))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc2b2b",
   "metadata": {},
   "source": [
    "### `make_wide_layer(...)` Ne Yapıyor? (Stage / Katman Grubu Üretici)\n",
    "\n",
    "Bu fonksiyonun görevi şudur:\n",
    "\n",
    "> **Tek bir “stage” (blok grubu) oluşturmak**  \n",
    "> Yani arka arkaya `num_block` tane `WideResidualBlock` dizmek ve bunları **tek bir modül** gibi döndürmek.\n",
    "\n",
    "\n",
    "\n",
    "#### 1) Girdi Parametreleri Ne Anlama Geliyor?\n",
    "\n",
    "- **`in_ch`**: Stage’e giren kanal sayısı  \n",
    "- **`out_ch`**: Stage’den çıkan kanal sayısı (bu stage’in “genişliği”)  \n",
    "- **`num_block`**: Bu stage içinde kaç tane `WideResidualBlock` olacağı  \n",
    "- **`stride`**: Stage’in *ilk bloğunun* stride’ı (downsample burada yapılır)  \n",
    "- **`dropout_p`**: Block içindeki dropout oranı\n",
    "\n",
    "####  2) Neden İlk Blok Farklı?\n",
    "\n",
    "Stage içinde **sadece ilk blok** şu işleri yapabilir:\n",
    "\n",
    "- `in_ch -> out_ch` kanal dönüşümü\n",
    "- `stride=2` ise `H×W` downsample (boyutu küçültme)\n",
    "\n",
    "Bu yüzden ilk blok:\n",
    "\n",
    "```python\n",
    "WideResidualBlock(in_ch, out_ch, stride=stride)\n",
    "```\n",
    "şeklinde kurulur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabe1cb",
   "metadata": {},
   "source": [
    "### `layers` Listesi Ne Oluyor?\n",
    "\n",
    "Fonksiyon sonunda layers şu hale gelir:\n",
    "\n",
    "* blok: in_ch -> out_ch (stride = verilen stride)\n",
    "\n",
    "* blok: out_ch -> out_ch (stride=1)\n",
    "\n",
    "* ...\n",
    "\n",
    "* num_block. blok: out_ch -> out_ch (stride=1)\n",
    "Yani:\n",
    "```bash\n",
    "[ Block(in_ch→out_ch, stride=stride),\n",
    "  Block(out_ch→out_ch, stride=1),\n",
    "  Block(out_ch→out_ch, stride=1),\n",
    "  ... (toplam num_block adet) ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d17c3f",
   "metadata": {},
   "source": [
    "### `nn.Sequential` demek:\n",
    "\n",
    "* Bu blokları sırayla çalıştır, tek bir modül gibi davran.\n",
    "\n",
    "\n",
    "\n",
    ">**make_wide_layer, num_block tane WideResidualBlock'u arka arkaya dizip,ilk blokta kanal/çözünürlük geçişini yapacak şekilde bir stage oluşturur ve bunu nn.Sequential olarak döndürür.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a31fe7",
   "metadata": {},
   "source": [
    "## 2.1-) Gerçek backbone: Stem + 3 stage ✅\n",
    "\n",
    "* Bu backbone, istersek classifier’a, istersek detection neck/head’e feature üretir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40eb1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResNetBackbone(nn.Module):\n",
    "    def __init__(self, depth:int = 128,widen_factor:int = 2,dropout_p :float = 0.0,in_channels:int = 3):\n",
    "        super().__init__()\n",
    "        assert (depth-4) % 6 == 0 , \"WRN için depth genelde 6n+4 formatındadır\"\n",
    "        n = (depth-4) // 6\n",
    "\n",
    "        base = 16\n",
    "        ch1 = base * widen_factor\n",
    "        ch2 = base * 2 * widen_factor\n",
    "        ch3 = base * 4 * widen_factor\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,base,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.stage1 = make_wide_layer(base,ch1,num_block=n,stride=1,dropout_p=dropout_p)\n",
    "        self.stage2 = make_wide_layer(ch1,ch2,num_block=n,stride=2,dropout_p=dropout_p)\n",
    "        self.stage3 = make_wide_layer(ch2,ch3,num_block=n,stride=2,dropout_p=dropout_p)\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(ch3)\n",
    "        self.out_channels = ch3\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x,inplace=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf72d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int, depth: int = 28, widen_factor: int = 2, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.backbone = WideResNetBackbone(depth=depth, widen_factor=widen_factor, dropout_p=dropout_p)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(self.backbone.out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x).flatten(1)  # [B,C,1,1] -> [B,C]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7aa1b",
   "metadata": {},
   "source": [
    "## Kullanım: “Gerçekten model oldu mu?” (shape test) ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([4, 10])\n",
      "feat: torch.Size([4, 128, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "model = WideResNetClassifier(num_classes=10, depth=28, widen_factor=2, dropout_p=0.0)\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "y = model(x)\n",
    "print(\"logits:\", y.shape)  # [4,10]\n",
    "\n",
    "feat = model.backbone(x)\n",
    "print(\"feat:\", feat.shape)  # [4, C, H', W'] -> WRN-28-2 için C=128, H'=8, W'=8 (32->16->8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6710b7b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
