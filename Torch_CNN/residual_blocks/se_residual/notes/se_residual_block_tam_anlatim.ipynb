{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16db3b05",
   "metadata": {},
   "source": [
    "------\n",
    "----\n",
    "----\n",
    "\n",
    "# SE-Residual\n",
    "\n",
    "**Residual bloğun içindeki feature’ları kanal bazında yeniden ağırlıklandıran bir\n",
    "attention ekidir.**\n",
    "\n",
    "**SE-Residual yapısında, kanal attention mekanizması\n",
    "residual bloğun dönüşüm fonksiyonu F(x) içine yerleştirilir.\n",
    "SE, F(x)’in kanal çıktıları için önem katsayıları üretir\n",
    "ve bu çıktılar residual bağlantı ile ana akışa eklenir.**\n",
    "\n",
    "----\n",
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ac41e",
   "metadata": {},
   "source": [
    "# Squeeze-and-Excitation (SE) Residual Block — Temelden Derine\n",
    "\n",
    "Bu notebook, **SE (Squeeze-and-Excitation)** fikrini sıfırdan alıp **Residual blok içine nasıl yerleştiğine** kadar götürür.\n",
    "\n",
    "İçerik akışı:\n",
    "1. CNN’lerde kanal önemlendirme problemi\n",
    "2. SE modülünün mantığı (Squeeze → Excitation → Reweight)\n",
    "3. Matematiksel sezgi + boyut takibi\n",
    "4. PyTorch ile: `SEBlock`, `SEBasicBlock`, `SEBottleneck`\n",
    "5. Nerede/niye kullanılır, pratik ayarlar ve sık hatalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4762b3a",
   "metadata": {},
   "source": [
    "## 1) Problem: Convolution kanal önemini doğrudan ayırt etmez\n",
    "\n",
    "Bir conv katmanının çıktısı genelde şu şekildedir:\n",
    "\n",
    "- **X** ∈ ℝ^(B×C×H×W)\n",
    "\n",
    "Burada:\n",
    "- **C**: kanal sayısı (feature map sayısı)\n",
    "- **H, W**: uzamsal boyutlar\n",
    "\n",
    "Conv katmanı uzamsal desenleri iyi yakalar ama “bu kanalı aç, bunu kıs” işini **explicit** yapmaz.\n",
    "İşte SE, **channel-wise attention** ile bu açığı kapatır.\n",
    "\n",
    "Kısa özet:\n",
    "- CNN: “nerede ne var” (spatial) güçlü\n",
    "- SE: “hangi kanal önemli” (channel) net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e78584",
   "metadata": {},
   "source": [
    "## 2) SE fikri tek cümleyle\n",
    "\n",
    "**SE şunu yapar:**\n",
    "> Feature map’lere bakar, her kanal için 0–1 arası bir ağırlık üretir ve önemli kanalları güçlendirir.\n",
    "\n",
    "SE’nin üç adımı var:\n",
    "1. **Squeeze**: (H×W) bilgisini tek sayıya indir (kanal başına özet)\n",
    "2. **Excitation**: Bu özetten kanal ağırlıkları öğren\n",
    "3. **Reweight**: Ağırlıkları feature map ile çarp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb42c9a",
   "metadata": {},
   "source": [
    "## 3) Squeeze: Global Average Pooling ile kanal özeti çıkarma\n",
    "\n",
    "Elimizde bir kanal olsun: X_c ∈ ℝ^(H×W)\n",
    "\n",
    "Squeeze, her kanal için:\n",
    "- `z_c = 평균(X_c)` üretir\n",
    "\n",
    "Böylece:\n",
    "- **z** ∈ ℝ^C\n",
    "\n",
    "Bu vektör şuna benzer:\n",
    "- “Her kanal genel olarak ne kadar aktif?”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5895020",
   "metadata": {},
   "source": [
    "## 4) Excitation: Küçük bir MLP ile kanal ağırlıkları üretme\n",
    "\n",
    "Squeeze sonucu gelen **z ∈ ℝ^C** vektöründen, kanal ağırlıkları **s ∈ (0,1)^C** üretilir.\n",
    "\n",
    "Tipik yapı:\n",
    "- FC: C → C/r\n",
    "- ReLU\n",
    "- FC: C/r → C\n",
    "- Sigmoid\n",
    "\n",
    "Burada **r** reduction ratio (çoğunlukla 16) — hem parametreyi düşürür, hem nonlinearity sağlar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e71a10",
   "metadata": {},
   "source": [
    "## 5) Reweight: Kanal ağırlıklarını feature map’e uygulama\n",
    "\n",
    "Kanal ağırlıkları:\n",
    "- **s** ∈ ℝ^C\n",
    "\n",
    "Feature map:\n",
    "- **X** ∈ ℝ^(B×C×H×W)\n",
    "\n",
    "Broadcast ile:\n",
    "- `Y[b, c, h, w] = X[b, c, h, w] * s[b, c]`\n",
    "\n",
    "Sonuç:\n",
    "- Kanal bazında “gain” uygulamış olursun.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02b1a0",
   "metadata": {},
   "source": [
    "## 6) SE Residual Block: Nereye takılır?\n",
    "\n",
    "Residual blok mantığı:\n",
    "- `out = F(x) + shortcut(x)`\n",
    "\n",
    "SE genelde **F(x)** yani residual branch’in **sonuna** konur:\n",
    "\n",
    "- `out = SE(F(x)) + shortcut(x)`\n",
    "\n",
    "Shortcut path çoğunlukla **dokunulmaz**, çünkü skip connection’ın temiz akması istenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b7461",
   "metadata": {},
   "source": [
    "## 7) PyTorch implementasyonu (notluk, anlaşılır)\n",
    "\n",
    "Aşağıdaki kodlar:\n",
    "- `SEBlock`: saf SE modülü\n",
    "- `SEBasicBlock`: ResNet-18/34 tarzı basic residual + SE\n",
    "- `SEBottleneck`: ResNet-50/101 tarzı bottleneck + SE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac12f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "def conv3x3(in_ch: int, out_ch: int, stride: int = 1, groups: int = 1) -> nn.Conv2d:\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False, groups=groups)\n",
    "\n",
    "def conv1x1(in_ch: int, out_ch: int, stride: int = 1) -> nn.Conv2d:\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(1, channels // reduction)\n",
    "\n",
    "        # Squeeze: Global Average Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # (B, C, 1, 1)\n",
    "\n",
    "        # Excitation: küçük MLP (1x1 conv ile FC gibi davranır)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=True)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=True)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (B, C, H, W) -> (B, C, 1, 1)\n",
    "        z = self.pool(x)\n",
    "\n",
    "        # (B, C, 1, 1) -> (B, hidden, 1, 1) -> (B, C, 1, 1)\n",
    "        s = self.fc1(z)\n",
    "        s = self.act(s)\n",
    "        s = self.fc2(s)\n",
    "        s = self.gate(s)   # 0..1 arası kanal ağırlıkları\n",
    "\n",
    "        # Reweight: broadcast çarpımı\n",
    "        return x * s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435e8b3",
   "metadata": {},
   "source": [
    "### 7.1) SE + Basic Residual (ResNet-18/34 tipi)\n",
    "\n",
    "- Residual branch: `Conv-BN-ReLU-Conv-BN`\n",
    "- Sonuna SE ekle\n",
    "- Topla + ReLU (post-act tarzı)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df9c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1, se_reduction: int = 16):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_ch, out_ch, stride=stride)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = conv3x3(out_ch, out_ch, stride=1)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.se = SEBlock(out_ch, reduction=se_reduction)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(\n",
    "                conv1x1(in_ch, out_ch, stride=stride),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # SE residual branch'e uygulanır\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7aa4c",
   "metadata": {},
   "source": [
    "### 7.2) SE + Bottleneck (ResNet-50/101 tipi)\n",
    "\n",
    "Bottleneck yapısı:\n",
    "- 1×1 reduce\n",
    "- 3×3 process\n",
    "- 1×1 expand (expansion genelde 4)\n",
    "\n",
    "SE genelde **expand’den sonra** (yani residual branch çıkışında) uygulanır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823db3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_ch: int, bottleneck_ch: int, stride: int = 1, se_reduction: int = 16):\n",
    "        super().__init__()\n",
    "        out_ch = bottleneck_ch * self.expansion\n",
    "\n",
    "        self.conv1 = conv1x1(in_ch, bottleneck_ch, stride=1)\n",
    "        self.bn1   = nn.BatchNorm2d(bottleneck_ch)\n",
    "\n",
    "        self.conv2 = conv3x3(bottleneck_ch, bottleneck_ch, stride=stride)\n",
    "        self.bn2   = nn.BatchNorm2d(bottleneck_ch)\n",
    "\n",
    "        self.conv3 = conv1x1(bottleneck_ch, out_ch, stride=1)\n",
    "        self.bn3   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.se = SEBlock(out_ch, reduction=se_reduction)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(\n",
    "                conv1x1(in_ch, out_ch, stride=stride),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)\n",
    "        out = self.conv3(out); out = self.bn3(out)\n",
    "\n",
    "        # SE residual output'ta (yani bottleneck sonunda)\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d9463",
   "metadata": {},
   "source": [
    "## 8) Boyut kontrolü (sanity check)\n",
    "\n",
    "Aşağıdaki hücre:\n",
    "- SE modülünün boyutları koruduğunu\n",
    "- Basic/Bottleneck SE blokların forward çalıştığını gösterir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283a48f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 56, 56]),\n",
       " torch.Size([2, 64, 56, 56]),\n",
       " torch.Size([2, 256, 56, 56]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "se = SEBlock(64, reduction=16)\n",
    "y = se(x)\n",
    "\n",
    "b1 = SEBasicBlock(64, 64, stride=1, se_reduction=16)\n",
    "y1 = b1(x)\n",
    "\n",
    "b2 = SEBottleneck(64, bottleneck_ch=64, stride=1, se_reduction=16)  # output: 256 ch\n",
    "y2 = b2(x)\n",
    "\n",
    "(y.shape, y1.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968a1ce",
   "metadata": {},
   "source": [
    "## 9) Pratik Notlar (gerçek hayatta işine yarayacak)\n",
    "\n",
    "### 9.1 Reduction ratio (r) nasıl seçilir?\n",
    "- Yaygın: **16**\n",
    "- Küçük kanallarda (C küçükse): **8** daha mantıklı olabilir\n",
    "- Çok küçük C’de `C//r` 0 olmasın diye `hidden=max(1, C//r)` yapılır (yukarıda yaptık)\n",
    "\n",
    "### 9.2 SE nerede en çok işe yarar?\n",
    "- Derin ağlar ve geniş kanal sayıları olan backbone’larda\n",
    "- Özellikle sınıflandırma ve detection backbone’larında (ResNet/ResNeXt türevleri)\n",
    "\n",
    "### 9.3 En sık yapılan hata\n",
    "- SE’yi shortcut’a uygulamak (genelde gereksiz / bazen zararlı)\n",
    "- Sıralamayı karıştırmak: SE, residual branch çıkışına uygulanmalı (çoğu tasarımda)\n",
    "\n",
    "### 9.4 “SE = attention mı?”\n",
    "Evet; ama **channel attention**.\n",
    "CBAM gibi modüller bunun üzerine **spatial attention** da ekler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b4a3b",
   "metadata": {},
   "source": [
    "## 10) Kısa özet\n",
    "\n",
    "- **Squeeze:** GlobalAvgPool → kanal özet vektörü\n",
    "- **Excitation:** küçük MLP → kanal ağırlıkları (0..1)\n",
    "- **Reweight:** feature map kanallarını ölçekle\n",
    "- **SE-Residual:** `out = SE(F(x)) + shortcut(x)`\n",
    "\n",
    "Bu kadar. Mantık budur. Gerisi tasarım tercihi.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
