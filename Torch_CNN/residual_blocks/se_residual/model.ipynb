{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb11962",
   "metadata": {},
   "source": [
    "\n",
    "# SE-Residual Block (Squeeze-and-Excitation + Residual) — Baştan Sona\n",
    "\n",
    "Bu notebook yalnızca **SE-Residual** (SE attention + residual bağlantı) yapısını anlatır:  \n",
    "- SE’nin ne yaptığı  \n",
    "- Residual bloğun neresine yerleştiği  \n",
    "- Pre-Act / Post-Act yerleşimleri  \n",
    "- Bir modele **temiz entegrasyon**  \n",
    "- Hızlı kontrol listesi ve shape testleri  \n",
    "\n",
    "> Notasyon  \n",
    "- `F(x)`: Residual yolun dönüşümü (conv/BN/act + SE)  \n",
    "- `skip(x)`: identity veya 1×1 projeksiyon  \n",
    "- Çıkış: `y = F(x) + skip(x)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0c222",
   "metadata": {},
   "source": [
    "\n",
    "## 1) SE nedir? (Channel attention)\n",
    "\n",
    "SE (Squeeze-and-Excitation) şunu yapar:\n",
    "\n",
    "1. **Squeeze**: Feature map’i kanal başına tek sayıya indirir (Global Average Pooling).  \n",
    "2. **Excitation**: Küçük bir MLP ile her kanal için önem katsayısı üretir.  \n",
    "3. **Sigmoid**: Katsayıları `[0,1]` aralığına sıkıştırır.  \n",
    "4. **Scale**: Feature map’i kanal bazında çarpar: `x * w`.\n",
    "\n",
    "Kritik nokta:  \n",
    "- SE **yeni uzamsal bilgi üretmez**.  \n",
    "- Var olan kanalları **yeniden ağırlıklandırır**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab928ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "def demo(m: nn.Module, x: torch.Tensor, name: str):\n",
    "    with torch.no_grad():\n",
    "        y = m(x)\n",
    "    print(f\"{name:>16} | in {tuple(x.shape)} -> out {tuple(y.shape)} | params {count_params(m):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd635e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) SE modülü (tek başına)\n",
    "\n",
    "SE modülü tek başına residual değildir.  \n",
    "SE, **residual bloğun içindeki** `F(x)` dönüşümüne takılan bir ektir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297d6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         SEBlock | in (2, 64, 20, 20) -> out (2, 64, 20, 20) | params 580\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, min_hidden: int = 4):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.fc(self.pool(x))   # (B,C,1,1)\n",
    "        return x * w                # kanal bazlı ölçekleme\n",
    "\n",
    "x = torch.randn(2, 64, 20, 20)\n",
    "demo(SEBlock(64), x, \"SEBlock\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44be1b3",
   "metadata": {},
   "source": [
    "\n",
    "## 3) SE-Residual: SE nereye konur?\n",
    "\n",
    "Pratikte en yaygın (ve en “standart”) yerleşim şudur:\n",
    "\n",
    "- `Conv/BN/Act` ile feature çıkar  \n",
    "- ikinci conv ile dönüşümü bitir  \n",
    "- **SE’yi en sona** (toplamadan önce) uygula  \n",
    "- sonra `+ skip`\n",
    "\n",
    "Şema:\n",
    "\n",
    "```bash\n",
    "x ───────────────┐\n",
    "                 ├── (+) ──> çıktı\n",
    "Conv → ... → Conv → SE\n",
    "```\n",
    "\n",
    "Bu yüzden SE çoğu zaman “residual bloğun sonuna eklenen channel attention” gibi görünür.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c37328",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Dikkat edilmesi gereken 4 nokta (SE-Residual Checklist)\n",
    "\n",
    "SE-Residual eklerken hatanın %90’ı şu 4 yerden çıkar:\n",
    "\n",
    "1. **Toplama öncesi shape uyumu**  \n",
    "   - `F(x)` ve `skip(x)` aynı `(C,H,W)` olmalı.\n",
    "\n",
    "2. **Stride/downsample senaryosu**  \n",
    "   - Stride=2 ise skip yolu da downsample edilmeli (1×1 conv + stride).\n",
    "\n",
    "3. **Kanal geçişi**  \n",
    "   - `cin != cout` ise skip yolu projeksiyon ister.\n",
    "\n",
    "4. **SE’nin konumu**  \n",
    "   - SE, genelde `F(x)` dönüşümünün **en sonuna** (toplamadan hemen önce) konur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4a85c",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Post-Act SE-Residual Block (klasik yerleşim)\n",
    "\n",
    "Bu versiyonda residual yol “post-activation” kalıbına yakındır:\n",
    "\n",
    "- `Conv → BN → Act → Conv → BN → SE → +skip → Act`\n",
    "\n",
    "Not: En sondaki aktivasyon tasarıma göre opsiyonel olabilir. Burada pratikte sık görülen şekilde ekliyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5812db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SEPostAct s1 | in (2, 32, 32, 32) -> out (2, 32, 32, 32) | params 18,852\n",
      "    SEPostAct s2 | in (2, 32, 32, 32) -> out (2, 64, 16, 16) | params 58,308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, cin, cout, k=3, s=1, p=1, act=\"silu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(cin, cout, k, stride=s, padding=p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(cout)\n",
    "        self.act = nn.SiLU(inplace=True) if act == \"silu\" else nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class SEResBlock_PostAct(nn.Module):\n",
    "    def __init__(self, cin, cout, stride=1, reduction=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNAct(cin, cout, k=3, s=stride, p=1, act=\"silu\")\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(cout, cout, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "        )\n",
    "        self.se = SEBlock(cout, reduction=reduction)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or cin != cout:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(cin, cout, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(cout),\n",
    "            )\n",
    "        self.out_act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = self.se(y)\n",
    "        y = y + skip\n",
    "        return self.out_act(y)\n",
    "\n",
    "demo(SEResBlock_PostAct(32, 32), torch.randn(2,32,32,32), \"SEPostAct s1\")\n",
    "demo(SEResBlock_PostAct(32, 64, stride=2), torch.randn(2,32,32,32), \"SEPostAct s2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1538e3",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Pre-Act SE-Residual Block (ResNet v2 çizgisi)\n",
    "\n",
    "Bu versiyonda BN+Act, conv’dan önce gelir:\n",
    "\n",
    "- `BN → Act → Conv → BN → Act → Conv → SE → +skip`\n",
    "\n",
    "Bu stil, çok derin yapılarda daha stabil optimizasyon sağlayabilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fe0953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SEPreAct s1 | in (2, 32, 32, 32) -> out (2, 32, 32, 32) | params 18,852\n",
      "     SEPreAct s2 | in (2, 32, 32, 32) -> out (2, 64, 16, 16) | params 58,116\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SEResBlock_PreAct(nn.Module):\n",
    "    def __init__(self, cin, cout, stride=1, reduction=16):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(cin)\n",
    "        self.conv1 = nn.Conv2d(cin, cout, 3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(cout)\n",
    "        self.conv2 = nn.Conv2d(cout, cout, 3, padding=1, bias=False)\n",
    "\n",
    "        self.se = SEBlock(cout, reduction=reduction)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or cin != cout:\n",
    "            self.proj = nn.Conv2d(cin, cout, 1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "\n",
    "        y = F.silu(self.bn1(x), inplace=True)\n",
    "        y = self.conv1(y)\n",
    "\n",
    "        y = F.silu(self.bn2(y), inplace=True)\n",
    "        y = self.conv2(y)\n",
    "\n",
    "        y = self.se(y)\n",
    "        return y + skip\n",
    "\n",
    "demo(SEResBlock_PreAct(32, 32), torch.randn(2,32,32,32), \"SEPreAct s1\")\n",
    "demo(SEResBlock_PreAct(32, 64, stride=2), torch.randn(2,32,32,32), \"SEPreAct s2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2a9f7",
   "metadata": {},
   "source": [
    "\n",
    "## 7) SE-Residual bir modele nasıl entegre edilir? (basitten ileriye)\n",
    "\n",
    "En kolay yol: Modeli “stage” mantığıyla kurmak.\n",
    "\n",
    "- `stem`: giriş conv\n",
    "- `stage`: aynı boyutta birden fazla SE-Residual blok\n",
    "- `stride=2`: bir stage’den diğerine geçerken çözünürlük düşürme (downsample)\n",
    "\n",
    "Aşağıdaki örnek küçük bir SE-ResNet iskeletidir (CIFAR benzeri boyutlarla).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "594ab3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEResNetTiny post | in (2, 3, 32, 32) -> out (2, 10) | params 702,986\n",
      "SEResNetTiny pre | in (2, 3, 32, 32) -> out (2, 10) | params 702,410\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_stage(block_cls, cin, cout, n_blocks, first_stride, reduction):\n",
    "    layers = [block_cls(cin, cout, stride=first_stride, reduction=reduction)]\n",
    "    for _ in range(1, n_blocks):\n",
    "        layers.append(block_cls(cout, cout, stride=1, reduction=reduction))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class SEResNetTiny(nn.Module):\n",
    "    def __init__(self, in_ch=3, num_classes=10, base=32, n=2, reduction=16, preact=False):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, base, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        block = SEResBlock_PreAct if preact else SEResBlock_PostAct\n",
    "        ch1, ch2, ch3 = base, base*2, base*4\n",
    "\n",
    "        self.stage1 = make_stage(block, ch1, ch1, n_blocks=n, first_stride=1, reduction=reduction)\n",
    "        self.stage2 = make_stage(block, ch1, ch2, n_blocks=n, first_stride=2, reduction=reduction)\n",
    "        self.stage3 = make_stage(block, ch2, ch3, n_blocks=n, first_stride=2, reduction=reduction)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch3),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(ch3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.head(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "m1 = SEResNetTiny(n=2, preact=False)\n",
    "m2 = SEResNetTiny(n=2, preact=True)\n",
    "demo(m1, torch.randn(2,3,32,32), \"SEResNetTiny post\")\n",
    "demo(m2, torch.randn(2,3,32,32), \"SEResNetTiny pre\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292c851",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Hızlı doğrulama: shape + grad\n",
    "\n",
    "SE-Residual entegrasyonunda “çalışıyor mu?” kontrolü için 2 test yeter:\n",
    "\n",
    "- **Shape testi**: forward patlıyor mu, çıktı boyutu doğru mu?\n",
    "- **Grad testi**: `.backward()` akıyor mu?\n",
    "\n",
    "Bu hücre, tek satırda kontrol eder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17af389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok | out: (2, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def quick_check_model(m: nn.Module, x: torch.Tensor):\n",
    "    m.train()\n",
    "    y = m(x)\n",
    "    loss = y.mean()\n",
    "    loss.backward()\n",
    "    print(\"ok | out:\", tuple(y.shape))\n",
    "\n",
    "x = torch.randn(2, 3, 32, 32, requires_grad=True)\n",
    "quick_check_model(SEResNetTiny(n=2, preact=False), x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb15f04",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Pratik notlar (ne zaman işe yarar?)\n",
    "\n",
    "SE-Residual çoğunlukla şuralarda faydalı olur:\n",
    "- Kanal sayısı arttıkça (orta/derin stage’ler)  \n",
    "- Özellikle “hangi feature daha önemli?” seçiminin kritik olduğu işlerde\n",
    "\n",
    "Dikkat:\n",
    "- SE, parametre ve gecikme ekler.  \n",
    "- Çok hafif modellerde (edge) bazen ECA gibi daha ucuz alternatifler tercih edilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7897b5c",
   "metadata": {},
   "source": [
    "\n",
    "## 10) En kısa özet\n",
    "\n",
    "- Residual: `y = F(x) + skip(x)` (öğrenmeyi stabil yapar)  \n",
    "- SE: kanalları ağırlıklandırır: `x * w`  \n",
    "- SE-Residual: SE’yi `F(x)` içine koyarsın (toplamadan hemen önce)\n",
    "\n",
    "En kritik entegrasyon kuralı:\n",
    "> Toplama yapılacaksa `F(x)` ve `skip(x)` aynı shape’e getirilir.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
