{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a6efb5",
   "metadata": {},
   "source": [
    "# Pattern-4 (Dual-path / Skip-aware Fusion) — Kod Odaklı Anlatım\n",
    "\n",
    "Bu notebook’un hedefi: **Pattern-4’ü sıfırdan kodlayıp**, bir attention modülüne **nasıl bağlandığını**, sonra da **normal bir modele nasıl entegre edildiğini** göstermek.\n",
    "\n",
    "- Gereksiz teori yok.\n",
    "- Kod akışı ve pratik tasarım adımları var.\n",
    "- “Near-identity skip” mantığıyla stabil kurulum var.\n",
    "\n",
    "---\n",
    "\n",
    "## Pattern-4 özeti (1 satır)\n",
    "\n",
    "\\[ y = A_s(x) + A_r(F(x)) \\]\n",
    "\n",
    "- `A_s`: **skip** yolu için attention/gate (hafif, near-identity)\n",
    "- `A_r`: **residual** yolu için attention/gate (daha güçlü olabilir)\n",
    "- `F(·)`: residual dönüşüm (Conv/BN/ReLU/Conv/BN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd05cb",
   "metadata": {},
   "source": [
    "## 1) Pattern-4’ü kodlarken izlenecek yol (checklist)\n",
    "\n",
    "1. **F(x)** dönüşümünü kur (BasicBlock benzeri)  \n",
    "2. **Skip eşitleme** (stride/kanal değişiyorsa 1×1 conv) ekle  \n",
    "3. Skip için **A_s** seç: genelde *hafif* (SE/ECA iyi gider)  \n",
    "4. Residual için **A_r** seç: daha güçlü olabilir (SE/CBAM/CoordAtt)  \n",
    "5. **Stabilite**: skip’i “tam kesen” maskeler yerine **near-identity** gate kullan  \n",
    "   - `A_s(x) = x * (1 + γ_s * m_s(x))`  (γ_s küçük/0 başlar)  \n",
    "6. Blok forward akışı:  \n",
    "   - `identity = skip(x)`  \n",
    "   - `s = A_s(identity)`  \n",
    "   - `f = F(x)`  \n",
    "   - `r = A_r(f)`  \n",
    "   - `y = s + r` (+ opsiyonel ReLU)\n",
    "\n",
    "Aşağıda bunu adım adım kuruyoruz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2673e",
   "metadata": {},
   "source": [
    "## 2) Hazır parçalar: Conv-BN-ReLU ve skip eşitleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd5c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv3x3(in_ch, out_ch, stride=1):\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_ch, out_ch, stride=1):\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def make_skip(in_ch: int, out_ch: int, stride: int):\n",
    "    if stride == 1 and in_ch == out_ch:\n",
    "        return nn.Identity()\n",
    "    return nn.Sequential(\n",
    "        conv1x1(in_ch, out_ch, stride=stride),\n",
    "        nn.BatchNorm2d(out_ch)\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15237ed4",
   "metadata": {},
   "source": [
    "## 3) Residual dönüşüm F(x) (BasicBlock tarzı)\n",
    "\n",
    "Pattern-4’te F(x) “normal residual” ile aynıdır.  \n",
    "Burada klasik tasarım:\n",
    "\n",
    "- Conv → BN → ReLU\n",
    "- Conv → BN\n",
    "- (Toplama blokta yapılacak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c859ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FxConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_ch, out_ch, stride=stride)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_ch, out_ch, stride=1)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.act(self.bn1(self.conv1(x)))\n",
    "        f = self.bn2(self.conv2(f))\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc3d3ba",
   "metadata": {},
   "source": [
    "## 4) “Attention’a bağlamak” ne demek?\n",
    "\n",
    "Pattern-4 için “attention’a bağlamak” şudur:\n",
    "\n",
    "- Skip yolundaki tensöre bir **gating/attention** uygula → `A_s(·)`\n",
    "- Residual yolundaki tensöre ayrı bir **gating/attention** uygula → `A_r(·)`\n",
    "\n",
    "Önemli nokta: çoğu attention modülü direkt “çıktı” üretmez; bir **maske** üretir ve tensörü **çarpar**.\n",
    "\n",
    "Bu notebookta:\n",
    "- `SEMask`: kanal maskesi üretir (B,C,1,1)  \n",
    "- `NearIdentityGate`: maskeyi `1 + γ*m` şeklinde uygular (skip güvenliği)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297df802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEMask(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels // reduction, 4)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(self.pool(x))\n",
    "\n",
    "class NearIdentityGate(nn.Module):\n",
    "    def __init__(self, mask: nn.Module, gamma_init: float = 0.0, gamma_learnable: bool = True):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        g = torch.tensor(float(gamma_init))\n",
    "        if gamma_learnable:\n",
    "            self.gamma = nn.Parameter(g)\n",
    "        else:\n",
    "            self.register_buffer(\"gamma\", g)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        m = self.mask(x)          \n",
    "        return x * (1.0 + self.gamma * m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799a204",
   "metadata": {},
   "source": [
    "## 5) Pattern-4 blok: en temel hali (stabil)\n",
    "\n",
    "Burada **iki gate** var:\n",
    "- `A_s`: skip için (γ_s = 0 ile başlatılır → skip saf kalır)\n",
    "- `A_r`: residual için (γ_r küçük pozitif başlayabilir)\n",
    "\n",
    "Forward akışı:\n",
    "1) identity = skip(x)  \n",
    "2) s = A_s(identity)  \n",
    "3) f = F(x)  \n",
    "4) r = A_r(f)  \n",
    "5) y = s + r  \n",
    "6) ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb783cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern4Block out: torch.Size([2, 64, 32, 32])\n",
      "gamma_s: 0.0 gamma_r: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "class Pattern4Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        out_ch: int,\n",
    "        stride: int = 1,\n",
    "        reduction_s: int = 16,\n",
    "        reduction_r: int = 16,\n",
    "        gamma_s_init: float = 0.0,   # skip: 0 ile başlat (near-identity)\n",
    "        gamma_r_init: float = 0.1,   # residual: küçük pozitif olabilir\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.skip = make_skip(in_ch, out_ch, stride)\n",
    "        self.F = FxConv(in_ch, out_ch, stride=stride)\n",
    "\n",
    "        # Skip gate (hafif)\n",
    "        self.As = NearIdentityGate(SEMask(out_ch, reduction=reduction_s), gamma_init=gamma_s_init, gamma_learnable=True)\n",
    "\n",
    "        # Residual gate (daha güçlü olabilir)\n",
    "        self.Ar = NearIdentityGate(SEMask(out_ch, reduction=reduction_r), gamma_init=gamma_r_init, gamma_learnable=True)\n",
    "\n",
    "        self.out_act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.skip(x)  # (B,out_ch,H',W')\n",
    "        s = self.As(identity)\n",
    "\n",
    "        f = self.F(x)            # (B,out_ch,H',W')\n",
    "        r = self.Ar(f)\n",
    "\n",
    "        y = s + r\n",
    "        return self.out_act(y)\n",
    "    \n",
    "x = torch.randn(2, 64, 32, 32)\n",
    "blk = Pattern4Block(64, 64, stride=1)\n",
    "y = blk(x)\n",
    "print('Pattern4Block out:', y.shape)\n",
    "print('gamma_s:', float(blk.As.gamma), 'gamma_r:', float(blk.Ar.gamma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b932fbf",
   "metadata": {},
   "source": [
    "## 6) “Daha ileri” kullanım: A_s ve A_r’yi farklı attention ile kurmak\n",
    "\n",
    "Pattern-4’te A_s ve A_r **aynı olmak zorunda değil**.\n",
    "\n",
    "Pratik kombinasyon:\n",
    "- `A_s`: hafif (SE/ECA) → stabil\n",
    "- `A_r`: daha güçlü (CBAM/CoordAtt) → ifade gücü\n",
    "\n",
    "Bu notebook minimal kalsın diye SE kullandık.  \n",
    "Ama Ar’yi CBAM ile değiştirmek istersen tek yapacağın şey:\n",
    "\n",
    "- `self.Ar = NearIdentityGate(SEMask(out_ch), gamma_init=0.05~0.1)` yerine CBAM koymak.\n",
    "\n",
    "Not: CBAM bazı implementasyonlarda “maskeyi” içeride uygular; o durumda `NearIdentityGate` yerine doğrudan `Ar(f)=CBAM(f)` kullanırsın.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fda93",
   "metadata": {},
   "source": [
    "## 7) Model entegrasyonu: “stage yok”, düz bir backbone\n",
    "\n",
    "Pattern-4 bloklarını normal bir CNN akışına böyle sokarsın:\n",
    "\n",
    "- stem\n",
    "- conv\n",
    "- Pattern4Block (same res)\n",
    "- Pattern4Block (downsample)\n",
    "- head\n",
    "\n",
    "Aşağıdaki model, “tak-çalıştır” entegrasyon örneğidir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0493ab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model out: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "class Pattern4Net(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Pattern-4 entegrasyonu\n",
    "        self.p4_1 = Pattern4Block(64, 64, stride=1, gamma_s_init=0.0, gamma_r_init=0.1)\n",
    "        self.p4_2 = Pattern4Block(64, 128, stride=2, gamma_s_init=0.0, gamma_r_init=0.1)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.stem(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.p4_1(x)\n",
    "        x = self.p4_2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "m = Pattern4Net(num_classes=10)\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "y = m(x)\n",
    "print('Model out:', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc2313",
   "metadata": {},
   "source": [
    "## 8) Pratik ayarlar (en çok iş gören)\n",
    "\n",
    "- `gamma_s_init = 0.0`  → skip başlangıçta saf kalsın ✅  \n",
    "- `gamma_r_init = 0.05–0.1` → residual gate hafif açık başlasın ✅  \n",
    "- Pattern-4’ü **her yere basma**: orta/son kısımlar daha güvenli  \n",
    "- Eğer eğitim dalgalanıyorsa:\n",
    "  - LR düşür\n",
    "  - gamma’ları küçük başlat (özellikle gamma_s)\n",
    "  - Batch çok küçükse BN yerine GN düşün\n",
    "\n",
    "Bu kadar. Pattern-4’ü “stabil ve kontrollü” kurmanın özü bu.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
