{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cedefa",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "**Attention ile F(x) HER ZAMAN ÇARPILIR.**\n",
    "\n",
    "**ASLA attention + F(x) diye toplanmaz.**\n",
    "\n",
    "**x, attention sürecine hiç girmez.x ayrı bir yoldan, doğrudan sona gider.**\n",
    "\n",
    "\n",
    "-----\n",
    "----- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f5488",
   "metadata": {},
   "source": [
    "# Residual + Attention Fusion Pattern (7) - Not Defteri\n",
    "\n",
    "**Kapsam:** Bu defter, residual bloklar ile attention modüllerini birleştirirken kullanılan **fusion pattern** (tasarım kalıpları) mantığını; nerede/niçin uygulandığını; pratikte PyTorch'ta nasıl kodlandığını; ve tasarım seçimlerinin (yerleşim, ölçekleme, norm/aktivasyon sırası) sonuçlarını **notluk** formatta toplar.\n",
    "\n",
    "**Bu bölüm ne değildir?** Stochastic Depth, ShakeDrop, DropBlock vb. eğitim-zamanı regularization teknikleri burada ana konu değildir. Burada odak: **inference-time mimari akış** ve “attention'ı residual ile nereye nasıl koyduğun” sorusu.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Kısa sözlük\n",
    "\n",
    "- **Residual branch (F(x))**: Bloğun \"öğrenen\" yolu (conv/norm/act vb.).\n",
    "- **Identity/skip path (x)**: Doğrudan taşınan yol.\n",
    "- **Add**: Birleşim işlemi: `y = x + F(x)`.\n",
    "- **Attention**: Özellik haritalarını **yeniden ağırlıklandıran** modül.\n",
    "- **Fusion pattern**: Attention’ın **hangi akışa** ve **hangi sırayla** eklendiğini tarif eden kalıp.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbdbf0",
   "metadata": {},
   "source": [
    "## 1) Daha önce işlenen bloklar ve attention'lar (mini tanım tablosu)\n",
    "\n",
    "Aşağıdaki tabloda sadece **kısa tanım** var (detaylar önceki başlıklarda). Bu defterin geri kalanı \"residual + attention\" birleşim kalıplarına odaklanır.\n",
    "\n",
    "### 1.1 Residual bloklar (özet)\n",
    "\n",
    "| Residual Blok | Mini Tanım |\n",
    "|---|---|\n",
    "| **Basic Residual Block** | 2 adet 3×3 conv (veya benzeri) + skip-add; düşük/orta kanallarda standart ResNet bloğu. |\n",
    "| **Pre-activation Residual Block** | BN+Act önce gelir; `BN→Act→Conv` sırası ile gradient akışı ve optimizasyon daha stabil. |\n",
    "| **Wide Residual Block** | Kanal sayısı artırılmış (widen factor); daha az derinlikle daha yüksek kapasite. |\n",
    "| **ResNeXt Grouped Convolution Block** | Bottleneck içinde **grouped conv** + cardinality; aynı FLOPs ile daha zengin temsiller. |\n",
    "| **Bottleneck with Expansion** | `1×1 reduce → 3×3 process → 1×1 expand` (expansion>1); derin ResNet'lerin temel bloğu. |\n",
    "| **Squeeze-Excite Residual Block** | Residual branch üzerinde **SE** ile kanal ağırlıkları öğrenilerek `F(x)` yeniden ölçeklenir. |\n",
    "\n",
    "### 1.2 Attention modülleri (özet)\n",
    "\n",
    "| Attention | Mini Tanım |\n",
    "|---|---|\n",
    "| **SE (Squeeze-and-Excitation)** | Global average pooling ile kanal istatistiği çıkarır, küçük MLP ile kanal ağırlıkları üretir. |\n",
    "| **ECA (Efficient Channel Attention)** | SE gibi kanal dikkatidir; MLP yerine 1D conv ile daha hafif kanal etkileşimi kurar. |\n",
    "| **CBAM** | Kanal attention + uzamsal (spatial) attention'ı ardışık uygular; genel amaçlı modül. |\n",
    "| **Coordinate Attention** | Uzamsal bilgiyi koordinat eksenlerinde özetleyip kanal attention ile birleştirir; konum duyarlı kanal ağırlığı üretir. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0df2f",
   "metadata": {},
   "source": [
    "## 2) Fusion Pattern nedir, neden ayrı başlık?\n",
    "\n",
    "Attention eklemek tek başına bir şey söylemez. Kritik olan: **attention nerede uygulanıyor?**\n",
    "\n",
    "Aynı attention modülü (ör. SE) farklı yere konduğunda:\n",
    "\n",
    "- Gradient akışı,\n",
    "- Representational capacity,\n",
    "- Hesap yükü,\n",
    "- Eğitim stabilitesi,\n",
    "- Düşük seviye/ yüksek seviye feature davranışı\n",
    "\n",
    "değişebilir.\n",
    "\n",
    "Bu yüzden **\"Residual + Attention Fusion Pattern\"** bir *mimari tasarım kalıbı* başlığıdır.\n",
    "\n",
    "### 2.1 Fusion pattern'i tanımlayan 3 soru\n",
    "\n",
    "1. **Konum (Where?)**: Attention, residual branch içinde mi, add'den sonra mı, skip üzerinde mi?\n",
    "2. **Neyi modüle ediyor (What?)**: Kanal mı, spatial mı, ikisi mi, yoksa etkileşim (self-attn) mi?\n",
    "3. **Kim \"dokunulmadan\" kalıyor (Safety?)**: Identity path bozuluyor mu? (Çoğu tasarım identity path’i saf tutar.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f3f8c",
   "metadata": {},
   "source": [
    "## 3) En temel 4 fusion pattern (çekirdek)\n",
    "\n",
    "Aşağıdaki şemalarda `F(·)` residual branch; `A(·)` attention modülü.\n",
    "\n",
    "> Not: Bu pattern'ler pratikte kombinlenebilir. Ancak düşünmeyi kolaylaştırmak için 4 ana forma indirgenir.\n",
    "\n",
    "----\n",
    "\n",
    "### Pattern-1: **Attention inside residual branch (en yaygın)**  fx’in önemini yeniden tarttık, sonra ekledik\n",
    "\n",
    "**Pattern-1, residual blokta üretilen F(x) feature map’ini attention ile yeniden ağırlıklandırıp, bu maskelenmiş çıktıyı dokunulmamış skip yolu x ile toplayan mimari yapıdır.**\n",
    "\n",
    "\\[ y = x + A(F(x)) \\]\n",
    "\n",
    "**α=A(F(x))**\n",
    "\n",
    "*A(⋅) = attention modülü*\n",
    "\n",
    "* Önce normal residual hesaplanır\n",
    "\n",
    "* Sonra attention, sadece F(x)’i yeniden ağırlıklandırır\n",
    "\n",
    "* Skip path dokunulmaz\n",
    "\n",
    "ASCII:\n",
    "\n",
    "```bash\n",
    "      x ────────────────┐\n",
    "      │                 │\n",
    "      │      F(·)       │\n",
    "      └─> [conv... ] -> A -> (+) -> y\n",
    "```\n",
    "\n",
    "**Neden yaygın?**\n",
    "- Identity path **saf** kalır (gradient için \"emniyet hattı\").\n",
    "- Attention, sadece öğrenen transform'u (F) şekillendirir.\n",
    "- Uygulama basit, davranış öngörülebilir.\n",
    "\n",
    "**SE-ResNet / ECA-ResNet / birçok CBAM entegrasyonu** bu kalıbı kullanır.\n",
    "\n",
    "----\n",
    "\n",
    "### Pattern-2: **Post-addition attention (birleşmiş feature üzerinde)**\n",
    "\n",
    "\\[ y = A(x + F(x)) \\]\n",
    "\n",
    "ASCII:\n",
    "\n",
    "```bash\n",
    "      x ────────────────┐\n",
    "      │                 │\n",
    "      │      F(·)       │\n",
    "      └─> [conv... ] ───┘\n",
    "              │\n",
    "             (+) -> A -> y\n",
    "```\n",
    "\n",
    "**Ne zaman mantıklı?**\n",
    "- Add sonrası feature'ın hem kimliği (x) hem dönüşümü (F) birlikte görülür.\n",
    "- \"Gating\" daha güçlüdür ama **attention artık identity etkisini de** dolaylı şekillendirir.\n",
    "\n",
    "**Risk/Trade-off:**\n",
    "- Çok agresif attention öğrenimi, identity'nin \"bedava\" geçişini zayıflatabilir.\n",
    "- Bazı görevlerde faydalı, bazı görevlerde over-suppression yapabilir.\n",
    "\n",
    "----\n",
    "\n",
    "### Pattern-3: **Pre-residual attention (input conditioning / gated input)**\n",
    "\n",
    "\\[ y = x + F(A(x)) \\]\n",
    "\n",
    "ASCII:\n",
    "\n",
    "```bash\n",
    "      x -> A -> F(·) -> (+ with x) -> y\n",
    "```\n",
    "\n",
    "**Okuma:** Attention, bloğa girecek sinyali filtreler; residual transform bu filtrelenmiş sinyal üzerinden öğrenir.\n",
    "\n",
    "**Ne zaman mantıklı?**\n",
    "- Erken aşamalarda gürültü baskılama,\n",
    "- Çok hafif backbone'larda feature seçiciliğini erkenden artırma.\n",
    "\n",
    "**Dikkat:**\n",
    "- A(x) kötü öğrenirse, F'nin öğrenebileceği bilgi azalır.\n",
    "\n",
    "----\n",
    "\n",
    "### Pattern-4: **Dual-path / skip-aware fusion (daha agresif)**\n",
    "\n",
    "En genel form:\n",
    "\n",
    "\\[ y = A_s(x) + A_r(F(x)) \\]\n",
    "\n",
    "ASCII:\n",
    "\n",
    "```bash\n",
    "      x -> A_s -> (+)\n",
    "      x -> F -> A_r -^\n",
    "```\n",
    "\n",
    "**Okuma:** Hem skip path hem residual path ayrı ayrı modüle edilir.\n",
    "\n",
    "**Avantaj:** Çok güçlü ifade gücü.\n",
    "\n",
    "**Risk:** Identity artık \"tam identity\" değildir. Stabilite için genellikle:\n",
    "- `A_s` çok hafif seçilir,\n",
    "- başlangıçta yaklaşık 1'e yakın olacak şekilde tasarlanır.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9cce5",
   "metadata": {},
   "source": [
    "## 4) Hangi pattern'i seçmeliyim? (tasarım rehberi)\n",
    "\n",
    "Aşağıdaki karar çizelgesi pratikte işe yarar:\n",
    "\n",
    "### 4.1 Default seçim (çoğu projede)\n",
    "\n",
    "- Başlangıç için **Pattern-1**.\n",
    "- Neden: davranışı öngörülebilir, identity korunur, debug kolay.\n",
    "\n",
    "### 4.2 Pattern-2 ne zaman?\n",
    "\n",
    "- Add sonrası birleşmiş feature üzerinde *tek seferde* gating istiyorsan.\n",
    "- Backbone çok derin değilse ve attention'ın identity'yi fazla bastırmayacağından eminsen.\n",
    "\n",
    "### 4.3 Pattern-3 ne zaman?\n",
    "\n",
    "- Bloğun girişini koşullandırmak istiyorsan.\n",
    "- Özellikle düşük seviyeli gürültü/arka plan baskılama hedefi varsa.\n",
    "\n",
    "### 4.4 Pattern-4 ne zaman?\n",
    "\n",
    "- Araştırma / ablation odaklı çalışma.\n",
    "- Skip’i modüle etmek riskli olduğu için: iyi initialization, dikkatli LR ve güçlü validasyon gerekir.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) \"Attention\" nereye konur? (residual bloğun içi)\n",
    "\n",
    "Bir residual blok (özellikle bottleneck) içinde attention için en yaygın yerleşimler:\n",
    "\n",
    "### 5.1 Bottleneck örneği\n",
    "\n",
    "```\n",
    "1x1 reduce -> 3x3 -> 1x1 expand -> (optional BN)\n",
    "```\n",
    "\n",
    "**Kural gibi çalışan pratik:**\n",
    "- Kanal attention (SE/ECA) çoğunlukla **expand sonrası** konur.\n",
    "  - Çünkü dikkat uygulanacak kanal sayısı, bloğun çıkış kanal sayısı ile hizalı olur.\n",
    "\n",
    "### 5.2 CBAM / Spatial attention için\n",
    "\n",
    "- Spatial attention, `H×W` üzerinde çalıştığı için genelde **son conv sonrası** konur.\n",
    "- Çok erken koyarsan compute artar ve düşük seviyeli feature'ları gereğinden fazla kesebilir.\n",
    "\n",
    "### 5.3 Pre-activation blokta dikkat\n",
    "\n",
    "Pre-activation mantığı bozulmasın diye dikkat yerleştirirken:\n",
    "- ya residual branch'in sonuna koy,\n",
    "- ya da attention'ın kendisini \"norm/act\" akışına uyumlu olacak şekilde tasarla.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acdf3a",
   "metadata": {},
   "source": [
    "## 6) Ölçekleme ve stabilite: \"gating\" patlamasın\n",
    "\n",
    "Attention modülleri pratikte **çarpan** üretir: `scale ∈ [0,1]` veya bazen `[0,∞)`.\n",
    "\n",
    "Residual ekleme:\n",
    "\n",
    "\\[ y = x + scale \\cdot F(x) \\]\n",
    "\n",
    "Bu noktada iki kritik stabilite konusu çıkar:\n",
    "\n",
    "### 6.1 Identity korunumu\n",
    "\n",
    "- Eğer `scale` sürekli çok küçükse: model residual öğrenemez (underfit gibi görünür).\n",
    "- Eğer `scale` çok agresif dalgalanırsa: eğitim kararsızlaşır.\n",
    "\n",
    "### 6.2 Zero-init / near-identity başlangıç\n",
    "\n",
    "Bazı tasarımlar, attention'ın veya residual branch'in son BN gamma'sını **0'a yakın** başlatır.\n",
    "Amaç:\n",
    "- İlk iterasyonlarda blok ≈ identity gibi davranır,\n",
    "- sonra yavaş yavaş residual katkı artar.\n",
    "\n",
    "Bu, attention'lı residual tasarımlarda sık kullanılan bir stabilite hilesidir.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b53e4f",
   "metadata": {},
   "source": [
    "## 7) Pattern'lerin pratik kod şablonları (PyTorch)\n",
    "\n",
    "Aşağıdaki kodlar \"çalışır\" örnekten ziyade **yerleşim mantığını** göstermeyi hedefler. İstediğin residual bloğa (basic/bottleneck/pre-act) uyarlayabilirsin.\n",
    "\n",
    "> Not: Aşağıda attention modülleri \"placeholder\" gibi verilmiştir. Projendeki gerçek SE/ECA/CBAM/CA implementasyonunu aynı slotlara takabilirsin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=None, groups=1, act=True):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = k // 2\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, groups=groups, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.ReLU(inplace=True) if act else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class IdentityAttn(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class SEStub(nn.Module):\n",
    "    def __init__(self, ch, r=16):\n",
    "        super().__init__()\n",
    "        hidden = max(1, ch // r)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(ch, hidden, 1)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, ch, 1)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.pool(x)\n",
    "        s = self.fc2(self.act(self.fc1(s)))\n",
    "        w = self.gate(s)\n",
    "        return x * w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern-1: y = x + A(F(x))\n",
    "class ResidualAttn_P1(nn.Module):\n",
    "    def __init__(self, ch, attn=None):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            ConvBNAct(ch, ch, 3, 1),\n",
    "            ConvBNAct(ch, ch, 3, 1, act=False),\n",
    "        )\n",
    "        self.attn = attn if attn is not None else IdentityAttn()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = self.f(x)\n",
    "        fx = self.attn(fx)\n",
    "        y = x + fx\n",
    "        return self.act(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern-2: y = A(x + F(x))\n",
    "class ResidualAttn_P2(nn.Module):\n",
    "    def __init__(self, ch, attn=None):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            ConvBNAct(ch, ch, 3, 1),\n",
    "            ConvBNAct(ch, ch, 3, 1, act=False),\n",
    "        )\n",
    "        self.attn = attn if attn is not None else IdentityAttn()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = self.f(x)\n",
    "        y = x + fx\n",
    "        y = self.attn(y)\n",
    "        return self.act(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3463f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern-3: y = x + F(A(x))\n",
    "class ResidualAttn_P3(nn.Module):\n",
    "    def __init__(self, ch, attn=None):\n",
    "        super().__init__()\n",
    "        self.attn = attn if attn is not None else IdentityAttn()\n",
    "        self.f = nn.Sequential(\n",
    "            ConvBNAct(ch, ch, 3, 1),\n",
    "            ConvBNAct(ch, ch, 3, 1, act=False),\n",
    "        )\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ax = self.attn(x)\n",
    "        fx = self.f(ax)\n",
    "        y = x + fx\n",
    "        return self.act(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern-4: y = A_s(x) + A_r(F(x))\n",
    "class ResidualAttn_P4(nn.Module):\n",
    "    def __init__(self, ch, attn_skip=None, attn_res=None):\n",
    "        super().__init__()\n",
    "        self.attn_skip = attn_skip if attn_skip is not None else IdentityAttn()\n",
    "        self.f = nn.Sequential(\n",
    "            ConvBNAct(ch, ch, 3, 1),\n",
    "            ConvBNAct(ch, ch, 3, 1, act=False),\n",
    "        )\n",
    "        self.attn_res = attn_res if attn_res is not None else IdentityAttn()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.attn_skip(x)\n",
    "        fx = self.attn_res(self.f(x))\n",
    "        y = xs + fx\n",
    "        return self.act(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331c1e2",
   "metadata": {},
   "source": [
    "## 8) Pattern seçimi için ablation planı (notluk)\n",
    "\n",
    "Bu bölüm doğrudan uygulamada “hangisi daha iyi?” sorusunu sistematik çözmek için.\n",
    "\n",
    "### 8.1 Kontrol değişkenleri\n",
    "\n",
    "Ablation yaparken aynı anda her şeyi değiştirme:\n",
    "\n",
    "- Backbone aynı kalsın (ör. ResNet-50 bottleneck).\n",
    "- Aynı attention modülü ile pattern değiştir.\n",
    "- Aynı data augmentation ve optimizer.\n",
    "\n",
    "### 8.2 Minimum ablation seti\n",
    "\n",
    "- Baseline: no attention\n",
    "- +SE Pattern-1\n",
    "- +SE Pattern-2\n",
    "- +SE Pattern-3\n",
    "\n",
    "### 8.3 Ölçüm\n",
    "\n",
    "- Top-1 / mAP (göreve göre)\n",
    "- Params / FLOPs\n",
    "- Latency (GPU/CPU hedefe göre)\n",
    "- Training stability: loss curve, grad norm\n",
    "\n",
    "### 8.4 Yorumlama ipucu\n",
    "\n",
    "- Pattern-2 iyileşiyor ama training dalgalanıyorsa: attention çok agresif olabilir.\n",
    "- Pattern-3 erken katmanlarda iyiyse ama derinlerde kötüleşiyorsa: input gating fazla filtreliyor olabilir.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70bc99e",
   "metadata": {},
   "source": [
    "## 9) Pre-activation residual ile attention: dikkat edilmesi gerekenler\n",
    "\n",
    "Pre-activation blokta tipik akış:\n",
    "\n",
    "```\n",
    "BN -> ReLU -> Conv -> BN -> ReLU -> Conv -> Add\n",
    "```\n",
    "\n",
    "Burada attention eklerken iki güvenli yaklaşım:\n",
    "\n",
    "### 9.1 Güvenli-1 (branch sonu)\n",
    "\n",
    "- Residual branch'in **son conv** çıkışına attention koy.\n",
    "- Add öncesi uygulandığı için Pattern-1 benzeri olur.\n",
    "\n",
    "### 9.2 Güvenli-2 (norm/act uyumlu)\n",
    "\n",
    "- Attention modülünü, BN/Act sırasına uyacak şekilde \"stateless\" tut.\n",
    "- Örn: CBAM/CA gibi uzamsal işlemler, pre-act'te gereksiz dağılım kaymasına sebep olmasın.\n",
    "\n",
    "> Pratikte: pre-act + SE/ECA genelde sorunsuz; pre-act + güçlü spatial attention (CBAM/CA) için validasyon şart.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a138c",
   "metadata": {},
   "source": [
    "## 10) ResNeXt (grouped conv) + attention: nerede oturur?\n",
    "\n",
    "ResNeXt bottleneck mantığı:\n",
    "\n",
    "- `1×1 reduce`\n",
    "- `3×3 grouped conv (cardinality)`\n",
    "- `1×1 expand`\n",
    "\n",
    "Attention yerleşimi için pratik kural değişmez:\n",
    "\n",
    "- SE/ECA: **expand sonrası**\n",
    "- CBAM/CA: çoğu zaman expand sonrası (veya add sonrası Pattern-2)\n",
    "\n",
    "**Neden?**\n",
    "- Grouped conv feature çeşitliliği üretir.\n",
    "- Attention, bu çeşitliliği \"seçici\" hale getirir.\n",
    "\n",
    "Trade-off:\n",
    "- Cardinality yüksekse attention compute görece az kalır; bu yüzden SE/ECA burada çok maliyet-etkin olur.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2e7a4",
   "metadata": {},
   "source": [
    "## 11) Wide residual + attention: aşırı kapasiteyi kontrol\n",
    "\n",
    "Wide ResNet'te kanal sayısı artar → capacity artar.\n",
    "\n",
    "Bu durumda attention iki rol oynar:\n",
    "\n",
    "1. **Seçicilik**: Kanallar çoksa, hepsini aynı güçte taşımak gereksiz olabilir.\n",
    "2. **Regularization benzeri etki**: Yanlış kanalları bastırarak overfit'i azaltabilir.\n",
    "\n",
    "Pratik öneri:\n",
    "- Wide backbone + SE/ECA genelde iyi.\n",
    "- Wide backbone + CBAM/CA compute artışına dikkat.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e18fb5e",
   "metadata": {},
   "source": [
    "## 12) Sık yapılan hatalar (debug checklist)\n",
    "\n",
    "### 12.1 Shape uyuşmazlığı\n",
    "- Attention modülü `C` kanal bekliyor ama bloğun output'u `C_out`.\n",
    "- Çözüm: attention'ı **output kanalına göre** kur.\n",
    "\n",
    "### 12.2 Skip path projeksiyonu unutma\n",
    "- Stride=2 veya kanal değişiminde `x` ile `F(x)` aynı shape olmaz.\n",
    "- Çözüm: skip üzerinde `1×1 conv` (projection) uygula.\n",
    "\n",
    "### 12.3 Add sonrası activation\n",
    "- Bazı implementasyonlar add sonrası ReLU yapar; bazıları pre-act.\n",
    "- Karışık kullanırsan istatistikler sapar.\n",
    "\n",
    "### 12.4 Attention'ı \"her yere\" koymak\n",
    "- Her blokta CBAM/CA: compute ve training zorlaşır.\n",
    "- Çözüm: stage bazlı seç (özellikle derin stage'lerde).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1061de2",
   "metadata": {},
   "source": [
    "## 13) Sonuç: bu başlıkta ne öğrendik?\n",
    "\n",
    "- Attention modülü \"ne\" olduğu kadar **\"nereye\" konduğu** ile de anlam kazanır.\n",
    "- 4 ana fusion pattern, pratikte tüm varyasyonların düşünce çerçevesini verir.\n",
    "- Default ve güvenli tercih çoğu senaryoda: **Pattern-1 (A(F(x)) + x)**.\n",
    "- Daha agresif tasarımlar (Pattern-2/4) daha güçlü olabilir ama stabilite/ablation gerektirir.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
