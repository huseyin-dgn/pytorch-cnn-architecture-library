{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d221f958",
   "metadata": {},
   "source": [
    "# Model.ipynb — Pattern-3 (Pre-Residual Attention / Input Conditioning)\n",
    "\n",
    "Bu notebook Pattern-3’ü **en baştan en sona** anlatır:\n",
    "- Ne yapar, amaç nedir?\n",
    "- Nasıl kodlanır? (CBAM örnekli)\n",
    "- Stabil kullanım tüyoları\n",
    "- Normal bir modele (stage yok) nasıl entegre edilir?\n",
    "\n",
    "---\n",
    "\n",
    "## Pattern-3 tek cümle\n",
    "**Önce x attention’dan geçer → çıkan filtrelenmiş temsil F(·) içine girer → en sonda skip olarak ham x eklenir.**\n",
    "\n",
    "\\[ y = x + F(A(x)) \\]\n",
    "\n",
    "ASCII:\n",
    "```bash\n",
    "x ──► A(.) ──► F(.) ──► (+) ──► y\n",
    "│\n",
    "└──── skip (x) ────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67220c53",
   "metadata": {},
   "source": [
    "## 1) Pattern-3 ne yapıyor?\n",
    "\n",
    "En basit akış:\n",
    "1) `x_att = A(x)`  → attention, x’i **ölçekler/filtreler** (maskeler)\n",
    "2) `f = F(x_att)`  → conv/bn/act gibi residual dönüşüm\n",
    "3) `y = skip(x) + f` → skip (identity) en sonda eklenir\n",
    "\n",
    "**Pattern-1 farkı:** Pattern-1’de attention `F(x)` üstünde; Pattern-3’te attention **x’in üstünde**.\n",
    "\n",
    "## 2) Amaç + Risk\n",
    "**Amaç:** Residual branch’e girecek sinyali daha baştan seçici yapmak (erken gürültü baskılama, hafif backbone’larda verim).\n",
    "\n",
    "**Risk:** Attention kötü öğrenirse residual branch’e giden bilgi azalır → `F` yanlış/eksik öğrenebilir.\n",
    "Bu yüzden pratikte Pattern-3 çoğu zaman **kontrollü** kullanılır (λ ile input mixing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085ad73",
   "metadata": {},
   "source": [
    "## 3) CBAM ile Pattern-3 (A(x))\n",
    "\n",
    "CBAM tipik olarak:\n",
    "- Channel mask (B,C,1,1)\n",
    "- Spatial mask (B,1,H,W)\n",
    "\n",
    "ve pratikte `A(x) ⊙ x` etkisini üretir (x’i ölçekler/filtreler).\n",
    "\n",
    "Aşağıda CBAM’i sıfırdan yazıyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels // reduction, 4)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.sigmoid(self.mlp(self.avg(x)) + self.mlp(self.max(x)))  # (B,C,1,1)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 7)\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)          # (B,1,H,W)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)        # (B,1,H,W)\n",
    "        cat = torch.cat([avg_map, max_map], dim=1)            # (B,2,H,W)\n",
    "        return self.sigmoid(self.conv(cat))                   # (B,1,H,W)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, spatial_kernel: int = 7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
    "        self.sa = SpatialAttention(kernel_size=spatial_kernel)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ca(x) * x\n",
    "        x = self.sa(x) * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f3220",
   "metadata": {},
   "source": [
    "## 4) Residual dönüşüm F(·) ve Skip eşitleme\n",
    "\n",
    "- `F(·)`: BasicBlock tarzı 3×3→3×3 residual dönüşüm\n",
    "- Skip yolu: stride/kanal değişince 1×1 conv + BN ile eşitlenir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FxConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.act(self.bn1(self.conv1(x)))\n",
    "        f = self.bn2(self.conv2(f))\n",
    "        return f\n",
    "\n",
    "def make_skip(in_ch: int, out_ch: int, stride: int):\n",
    "    if stride != 1 or in_ch != out_ch:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "    return nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378364b1",
   "metadata": {},
   "source": [
    "## 5) Pattern-3 Block (Temel)\n",
    "\n",
    "Akış:\n",
    "1) `x_att = CBAM(x)`\n",
    "2) `f = F(x_att)`\n",
    "3) `y = skip(x) + f`\n",
    "\n",
    "\\[ y = \\text{skip}(x) + F(\\text{CBAM}(x)) \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e8e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 basic: torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class Pattern3_PreResidualCBAM(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1,\n",
    "                 reduction: int = 16, spatial_kernel: int = 7):\n",
    "        super().__init__()\n",
    "        self.attn = CBAM(in_ch, reduction=reduction, spatial_kernel=spatial_kernel)\n",
    "        self.F = FxConv(in_ch, out_ch, stride=stride)\n",
    "        self.skip = make_skip(in_ch, out_ch, stride)\n",
    "        self.out_act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.skip(x)\n",
    "        x_att = self.attn(x)\n",
    "        f = self.F(x_att)\n",
    "        y = identity + f\n",
    "        return self.out_act(y)\n",
    "\n",
    "# quick test\n",
    "x = torch.randn(2, 64, 32, 32)\n",
    "blk = Pattern3_PreResidualCBAM(64, 64)\n",
    "print('P3 basic:', blk(x).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d6b0f",
   "metadata": {},
   "source": [
    "## 6) Stabil kullanım tüyoları (en önemlisi)\n",
    "\n",
    "Pattern-3’te risk: attention residual branch’e giden bilgiyi fazla kısabilir.\n",
    "En pratik çözüm: **λ ile input mixing**.\n",
    "\n",
    "\\[ x_{att}=A(x)\\odot x \\]\n",
    "\\[ \\tilde{x}=(1-\\lambda)x+\\lambda x_{att} \\]\n",
    "\\[ y=skip(x)+F(\\tilde{x}) \\]\n",
    "\n",
    "Öneriler:\n",
    "- `λ` küçük başlat: 0.05–0.2\n",
    "- `λ` öğrenilebilir olsun (sigmoid ile 0–1 aralığında)\n",
    "- Büyük data yoksa warm-up iyi çalışır\n",
    "- Çok erken katmanda kontrolsüz Pattern-3 agresif olabilir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 controlled: torch.Size([2, 64, 32, 32]) lambda= 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "class Pattern3_PreResidualCBAM_Controlled(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1,\n",
    "                 lam_init: float = 0.1, lam_learnable: bool = True,\n",
    "                 reduction: int = 16, spatial_kernel: int = 7):\n",
    "        super().__init__()\n",
    "        self.attn = CBAM(in_ch, reduction=reduction, spatial_kernel=spatial_kernel)\n",
    "        self.F = FxConv(in_ch, out_ch, stride=stride)\n",
    "        self.skip = make_skip(in_ch, out_ch, stride)\n",
    "        self.out_act = nn.ReLU(inplace=True)\n",
    "\n",
    "        lam_init = float(lam_init)\n",
    "        lam_init = min(max(lam_init, 1e-4), 1 - 1e-4)\n",
    "        lam_logit = torch.log(torch.tensor(lam_init) / (1 - torch.tensor(lam_init)))\n",
    "        if lam_learnable:\n",
    "            self.lam_logit = nn.Parameter(lam_logit)\n",
    "        else:\n",
    "            self.register_buffer('lam_logit', lam_logit)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.skip(x)\n",
    "        x_att = self.attn(x)\n",
    "\n",
    "        lam = torch.sigmoid(self.lam_logit)\n",
    "        x_tilde = (1.0 - lam) * x + lam * x_att\n",
    "\n",
    "        f = self.F(x_tilde)\n",
    "        y = identity + f\n",
    "        return self.out_act(y)\n",
    "\n",
    "x = torch.randn(2, 64, 32, 32)\n",
    "blk = Pattern3_PreResidualCBAM_Controlled(64, 64, lam_init=0.1)\n",
    "print('P3 controlled:', blk(x).shape, 'lambda=', float(torch.sigmoid(blk.lam_logit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b37923",
   "metadata": {},
   "source": [
    "## 7) Normal modele entegrasyon (stage yok)\n",
    "\n",
    "Düz mimari:\n",
    "- stem (conv+bn+relu)\n",
    "- conv\n",
    "- p3_1 (same resolution)\n",
    "- p3_2 (downsample + kanal artışı)\n",
    "- head (GAP + FC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97340a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model out: torch.Size([4, 10])\n",
      "lambda p3_1: 0.10000000149011612\n",
      "lambda p3_2: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN_With_Pattern3(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10, lam_init: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.p3_1 = Pattern3_PreResidualCBAM_Controlled(64, 64, stride=1, lam_init=lam_init)\n",
    "        self.p3_2 = Pattern3_PreResidualCBAM_Controlled(64, 128, stride=2, lam_init=lam_init)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.stem(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.p3_1(x)\n",
    "        x = self.p3_2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "m = SimpleCNN_With_Pattern3(num_classes=10, lam_init=0.1)\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "y = m(x)\n",
    "print('model out:', y.shape)\n",
    "print('lambda p3_1:', float(torch.sigmoid(m.p3_1.lam_logit)))\n",
    "print('lambda p3_2:', float(torch.sigmoid(m.p3_2.lam_logit)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefe6b8",
   "metadata": {},
   "source": [
    "## 8) Son mini checklist\n",
    "\n",
    "- [ ] Attention **x üstünde mi?** (Pattern-3 = evet)\n",
    "- [ ] `F` attention sonrası girdiyi mi alıyor?\n",
    "- [ ] Skip eşitleme var mı? (stride/kanal değişince)\n",
    "- [ ] Stabilite için controlled (λ) kullandın mı?\n",
    "- [ ] Çok erken katmanda agresif kullanım var mı? (gerek yoksa azalt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
