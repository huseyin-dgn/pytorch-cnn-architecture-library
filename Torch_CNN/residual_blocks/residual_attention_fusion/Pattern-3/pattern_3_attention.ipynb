{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f16b048",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Önce feature map (x) attention’a verilir, attention’dan çıkan filtrelenmiş çıktı F(·) içine sokulur ve en sonda skip yolu olarak ham x eklenir.,\n",
    "\n",
    "-----\n",
    "\n",
    "# Pattern 3 — Pre-Residual Attention (Input Conditioning / Gated Input)\n",
    "\n",
    "Bu notebook **Pattern-3**'ü baştan sona anlatır ve Pattern-1 / Pattern-2 ile **attention yerleşimi** üzerinden net karşılaştırır.\n",
    "\n",
    "---\n",
    "\n",
    "## Hızlı Özet (3 pattern yan yana)\n",
    "\n",
    "- **Pattern-1 (Inside residual):**  \\(y = x + (A(F(x)) \\odot F(x))\\)  \n",
    "- **Pattern-2 (Post-addition):**  \\(z = x + F(x),\\; y = A(z)\\odot z\\)  \n",
    "- **Pattern-3 (Pre-residual):**  \\(y = x + F(A(x))\\)\n",
    "\n",
    "Pattern-3'te attention, **residual branch'e girecek sinyali** filtreler; skip yolu ayrı akar.\n",
    "\n",
    "---\n",
    "\n",
    "## ASCII Akış\n",
    "\n",
    "```BASH\n",
    "            ┌─────────────── Skip (identity) ────────────────┐\n",
    "            │                                                │\n",
    "x ──► A(.) ─┴─► (gated input) ─► F(.) ───────────────►  (+)  ├─► y\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93814827",
   "metadata": {},
   "source": [
    "## 1) Residual temel hatırlatma\n",
    "\n",
    "Klasik residual blok:\n",
    "\n",
    "\\[ y = x + F(x) \\]\n",
    "\n",
    "- `x` (skip): referans bilgi + stabil gradient yolu  \n",
    "- `F(x)` (residual): öğrenilen katkı/düzeltme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd689329",
   "metadata": {},
   "source": [
    "## 2) Pattern-3'te ne değişiyor?\n",
    "\n",
    "Pattern-3'te residual branch'e giren sinyal **x değil**, attention ile filtrelenmiş sinyaldir:\n",
    "\n",
    "\\[ \\tilde{x} = A(x) \\odot x \\]\n",
    "\n",
    "Residual:\n",
    "\n",
    "\\[ f = F(\\tilde{x}) \\]\n",
    "\n",
    "Çıkış:\n",
    "\n",
    "\\[ y = \\text{skip}(x) + f \\]\n",
    "\n",
    "Okuma: \"Önce x'i seçici hale getir, sonra F bu seçilmiş bilgi üzerinden öğrenir.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdff41e",
   "metadata": {},
   "source": [
    "## 3) Amaç ve risk\n",
    "\n",
    "### Amaç\n",
    "- Erken gürültü baskılama\n",
    "- Hafif backbone'larda erken seçicilik\n",
    "- F'nin öğrenmesini daha \"temiz\" girdiye şartlamak\n",
    "\n",
    "### Kritik risk\n",
    "`A(x)` yanlış öğrenirse residual branch'in gördüğü bilgi azalır → `F` kötü/eksik öğrenebilir.\n",
    "\n",
    "Bu yüzden Pattern-3 çoğu zaman **kontrollü** uygulanır (lambda ile input mixing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02ea48",
   "metadata": {},
   "source": [
    "## 4) CBAM ile Pattern-3\n",
    "\n",
    "CBAM iki maske uygular:\n",
    "\n",
    "1) Channel attention: \\(\\alpha_c\\in\\mathbb{R}^{C\\times1\\times1}\\)  \n",
    "2) Spatial attention: \\(\\alpha_s\\in\\mathbb{R}^{1\\times H\\times W}\\)\n",
    "\n",
    "Sıra:\n",
    "\n",
    "\\[ x' = \\alpha_c(x)\\odot x \\]  \n",
    "\\[ \\tilde{x} = \\alpha_s(x')\\odot x' \\]\n",
    "\n",
    "Sonra:\n",
    "\n",
    "\\[ y = x + F(\\tilde{x}) \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b6749",
   "metadata": {},
   "source": [
    "## 5) Kod — CBAM (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85ef6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels // reduction, 4)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.sigmoid(self.mlp(self.avg(x)) + self.mlp(self.max(x)))  # (B,C,1,1)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 7)\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)          # (B,1,H,W)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)        # (B,1,H,W)\n",
    "        cat = torch.cat([avg_map, max_map], dim=1)            # (B,2,H,W)\n",
    "        return self.sigmoid(self.conv(cat))                   # (B,1,H,W)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, spatial_kernel: int = 7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
    "        self.sa = SpatialAttention(kernel_size=spatial_kernel)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ca(x) * x\n",
    "        x = self.sa(x) * x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafec795",
   "metadata": {},
   "source": [
    "## 6) Kod — Pattern-3 Residual Block (Pre-Residual Attention)\n",
    "\n",
    "Akış:\n",
    "\n",
    "1) `x_att = CBAM(x)`  (CBAM kendi içinde maske üretir ve çarpar, yani `A(x) ⊙ x`)  \n",
    "2) `f = F(x_att)`  \n",
    "3) `y = skip(x) + f`\n",
    "\n",
    "\\[ y = \\text{skip}(x) + F(\\text{CBAM}(x)) \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0b37e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 block output: torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def make_skip(in_ch: int, out_ch: int, stride: int):\n",
    "    if stride != 1 or in_ch != out_ch:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "    return nn.Identity()\n",
    "\n",
    "class FxConv(nn.Module):\n",
    "    # Basit F(): 3x3 -> 3x3 (ResNet Basic)\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.act(self.bn1(self.conv1(x)))\n",
    "        f = self.bn2(self.conv2(f))\n",
    "        return f\n",
    "\n",
    "class PreResidualAttentionBlock_P3(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1,\n",
    "                 reduction: int = 16, spatial_kernel: int = 7):\n",
    "        super().__init__()\n",
    "        self.attn = CBAM(in_ch, reduction=reduction, spatial_kernel=spatial_kernel)\n",
    "        self.F = FxConv(in_ch, out_ch, stride=stride)\n",
    "        self.skip = make_skip(in_ch, out_ch, stride)\n",
    "        self.out_act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.skip(x)\n",
    "        x_att = self.attn(x)\n",
    "        f = self.F(x_att)\n",
    "        y = identity + f\n",
    "        return self.out_act(y)\n",
    "\n",
    "# Shape sanity check\n",
    "x = torch.randn(2, 64, 32, 32)\n",
    "blk = PreResidualAttentionBlock_P3(64, 64, stride=1)\n",
    "print(\"P3 block output:\", blk(x).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f647f0d",
   "metadata": {},
   "source": [
    "## 7) Kontrollü Pattern-3 (önerilen): lambda ile input mixing\n",
    "\n",
    "Pattern-3'te \"kapıyı tamamen kapatma\" yaklaşımı:\n",
    "\n",
    "- `x_att = A(x) ⊙ x`\n",
    "- `x_tilde = (1-λ)·x + λ·x_att`\n",
    "- `y = skip(x) + F(x_tilde)`\n",
    "\n",
    "\\[ x_{att} = A(x)\\odot x \\]  \n",
    "\\[ \\tilde{x} = (1-\\lambda)x + \\lambda x_{att} \\]  \n",
    "\\[ y = \\text{skip}(x) + F(\\tilde{x}) \\]\n",
    "\n",
    "`λ` küçük başlatılır (örn. 0.1) ve öğrenilebilir yapılır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ab3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controlled P3: torch.Size([2, 64, 32, 32]) lambda: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ControlledPreResidualAttentionBlock_P3(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1,\n",
    "                 lam_init: float = 0.1, lam_learnable: bool = True,\n",
    "                 reduction: int = 16, spatial_kernel: int = 7):\n",
    "        super().__init__()\n",
    "        self.attn = CBAM(in_ch, reduction=reduction, spatial_kernel=spatial_kernel)\n",
    "        self.F = FxConv(in_ch, out_ch, stride=stride)\n",
    "        self.skip = make_skip(in_ch, out_ch, stride)\n",
    "        self.out_act = nn.ReLU(inplace=True)\n",
    "\n",
    "        lam_init = float(lam_init)\n",
    "        lam_init = min(max(lam_init, 1e-4), 1 - 1e-4)\n",
    "        lam_logit = torch.log(torch.tensor(lam_init) / (1 - torch.tensor(lam_init)))\n",
    "        if lam_learnable:\n",
    "            self.lam_logit = nn.Parameter(lam_logit)\n",
    "        else:\n",
    "            self.register_buffer(\"lam_logit\", lam_logit)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.skip(x)\n",
    "\n",
    "        x_att = self.attn(x)\n",
    "        lam = torch.sigmoid(self.lam_logit)          # scalar in (0,1)\n",
    "        x_tilde = (1.0 - lam) * x + lam * x_att      # controlled input\n",
    "\n",
    "        f = self.F(x_tilde)\n",
    "        y = identity + f\n",
    "        return self.out_act(y)\n",
    "\n",
    "x = torch.randn(2, 64, 32, 32)\n",
    "blk = ControlledPreResidualAttentionBlock_P3(64, 64, lam_init=0.1)\n",
    "print(\"Controlled P3:\", blk(x).shape, \"lambda:\", torch.sigmoid(blk.lam_logit).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e718b1",
   "metadata": {},
   "source": [
    "## 8) Normal modele entegrasyon (stage yok, düz akış)\n",
    "\n",
    "Aşağıdaki model: `stem -> conv -> (P3 blokları) -> head`.\n",
    "\n",
    "Pattern-3 için öneri: kontrollü versiyon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "260aeba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([4, 10])\n",
      "lambda p3_1: 0.10000000149011612\n",
      "lambda p3_2: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN_P3_Controlled(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10, lam_init: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.p3_1 = ControlledPreResidualAttentionBlock_P3(64, 64, stride=1, lam_init=lam_init)\n",
    "        self.p3_2 = ControlledPreResidualAttentionBlock_P3(64, 128, stride=2, lam_init=lam_init)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.stem(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.p3_1(x)\n",
    "        x = self.p3_2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    m = SimpleCNN_P3_Controlled(num_classes=10, lam_init=0.1)\n",
    "    x = torch.randn(4, 3, 32, 32)\n",
    "    y = m(x)\n",
    "    print(\"Output:\", y.shape)\n",
    "    print(\"lambda p3_1:\", torch.sigmoid(m.p3_1.lam_logit).item())\n",
    "    print(\"lambda p3_2:\", torch.sigmoid(m.p3_2.lam_logit).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259fbe7",
   "metadata": {},
   "source": [
    "## 9) Mini kontrol listesi (Pattern-3)\n",
    "\n",
    "- [ ] Attention girişte mi? (`x -> A(x) -> x_tilde`)\n",
    "- [ ] `F(.)` x_tilde üstünde mi?\n",
    "- [ ] Skip yolu en sonda toplanıyor mu?\n",
    "- [ ] Boyut/kanal değişiminde skip eşitleme var mı?\n",
    "- [ ] Risk için input mixing (lambda) var mı?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
