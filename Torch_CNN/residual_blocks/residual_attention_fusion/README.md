# Attentionâ€“Residual Fusion Pattern â€” KÄ±sa Ã–zet

## ğŸ¯ AmaÃ§
Residual Ã¶ÄŸrenmeyi korurken, aÄŸÄ±n **Ã¶nemli Ã¶zelliklere odaklanmasÄ±nÄ±** saÄŸlamak.

---

## ğŸ§  Temel Fikir

Residual blokta ana yol **F(x)** Ã¶ÄŸrenir, skip yol ise bilgiyi taÅŸÄ±r.  
Attention modÃ¼lÃ¼, **F(x)â€™in Ã¼rettiÄŸi Ã¶zellikleri aÄŸÄ±rlÄ±klandÄ±rÄ±r**.

**Ã‡Ä±kÄ±ÅŸ = Aktivasyon( Attention(F(x)) + Skip(x) )**

Yani:
- Residual yapÄ± korunur  
- Attention, â€œneyin Ã¶nemli olduÄŸunuâ€ sÃ¶yler

---

## ğŸ”€ Fusion (BirleÅŸim) NoktasÄ±

Attention genelde ÅŸu noktada uygulanÄ±r:

**Conv â†’ Norm â†’ Conv â†’ Norm â†’ Attention â†’ Toplama**

Yani toplama Ã¶ncesi ana yol filtrelenir.

---

## ğŸ§© Ne KazandÄ±rÄ±r?

âœ” GÃ¼rÃ¼ltÃ¼lÃ¼ featureâ€™lar bastÄ±rÄ±lÄ±r  
âœ” Ã–nemli kanallar/bÃ¶lgeler Ã¶ne Ã§Ä±kar  
âœ” Residual stabilite bozulmaz  
âœ” Ã–zellikle detection ve segmentationâ€™da etkilidir  

---

## âš™ï¸ Uygulama TÃ¼rleri

| Attention TÃ¼rÃ¼ | Ne AÄŸÄ±rlÄ±klandÄ±rÄ±r |
|---------------|--------------------|
| Channel Attention | Hangi kanal Ã¶nemli |
| Spatial Attention | Hangi bÃ¶lge Ã¶nemli |
| SE / CBAM / ECA | Hafif ve etkili modÃ¼ller |

---

## ğŸ”š Ã–zet

Bu pattern:

**Residual Ã¶ÄŸrenme + Attention odaklanmasÄ± = Daha seÃ§ici ve gÃ¼Ã§lÃ¼ feature temsili**
