{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be323f12",
   "metadata": {},
   "source": [
    "----\n",
    "-----\n",
    "----\n",
    "----\n",
    "\n",
    "## PATTERN 1 — Attention Inside Residual Branch (CBAM)\n",
    "\n",
    "**Girdi**\n",
    "\n",
    "\\[\n",
    "x \\in \\mathbb{R}^{C \\times H \\times W}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "**Akış (tek hücre – baştan sona)**\n",
    "\n",
    "```text\n",
    "x\n",
    "│\n",
    "├─ Skip (identity) yolu\n",
    "│   ├─ identity = x\n",
    "│   └─ (kanal/çözünürlük değişiyorsa)\n",
    "│       identity = Conv1×1(stride) → BatchNorm\n",
    "│\n",
    "└─ Residual branch (F(x))\n",
    "    ├─ f = Conv3×3(stride)(x)\n",
    "    ├─ f = BatchNorm(f)\n",
    "    ├─ f = ReLU(f)\n",
    "    ├─ f = Conv3×3(stride=1)(f)\n",
    "    ├─ f = BatchNorm(f)\n",
    "    │\n",
    "    ├─ Channel Attention\n",
    "    │   ├─ α_c = ChannelAttention(f)        # (C × 1 × 1)\n",
    "    │   └─ f   = α_c ⊙ f\n",
    "    │\n",
    "    ├─ Spatial Attention\n",
    "    │   ├─ α_s = SpatialAttention(f)        # (1 × H × W)\n",
    "    │   └─ f   = α_s ⊙ f\n",
    "    │\n",
    "    └─ f = A(F(x)) ⊙ F(x)\n",
    "│\n",
    "└─ Residual Add\n",
    "    ├─ y = identity + f\n",
    "    └─ y = ReLU(y)   (opsiyonel)\n",
    "```\n",
    "Tek satırlık denklem :: :: y=skip(x)+(A(F(x))⊙F(x))\n",
    "\n",
    "Net kurallar\n",
    "\n",
    "* Attention’a giren x değil, F(x)’tir\n",
    "\n",
    "* Attention yeni feature üretmez, maske (α) üretir\n",
    "\n",
    "* Maske çarpılır (⊙), toplanmaz\n",
    "\n",
    "* Toplama yalnızca en sonda, skip yolu ile yapılır\n",
    "\n",
    "* Skip yolu attention’dan etkilenmez\n",
    "\n",
    "----\n",
    "-----\n",
    "----\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170b83b",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea59de5",
   "metadata": {},
   "source": [
    "## Uygulama için önce bir CBAM Attention tanımlayalım.Eğer attention kısmına aşina değilseniz bu dosyaya göz gezdirmenizi öneririm.\n",
    "> **Torch CNN - Part_2\\Attention Mekanizmaları**\n",
    "\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "-------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24987ab0",
   "metadata": {},
   "source": [
    "# 1) CBAM’i sıfırdan yazalım (PyTorch)\n",
    "\n",
    "CBAM iki parçadan oluşur:\n",
    "\n",
    "* Channel Attention: (B,C,1,1) maske üretir\n",
    "\n",
    "* Spatial Attention: (B,1,H,W) maske üretir\n",
    "\n",
    "Sonra ikisi sırayla çarpılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d181dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels // reduction, 4)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # CBAM paper: shared MLP (biz 1x1 conv ile yapıyoruz)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, kernel_size=1, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,H,W)\n",
    "        avg_out = self.mlp(self.avg_pool(x))  # (B,C,1,1)\n",
    "        max_out = self.mlp(self.max_pool(x))  # (B,C,1,1)\n",
    "        attn = self.sigmoid(avg_out + max_out)\n",
    "        return attn  # (B,C,1,1)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 7)\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,H,W)\n",
    "        # channel-wise pooling -> (B,1,H,W) + (B,1,H,W)\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        cat = torch.cat([avg_map, max_map], dim=1)  # (B,2,H,W)\n",
    "        attn = self.sigmoid(self.conv(cat))         # (B,1,H,W)\n",
    "        return attn\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, spatial_kernel: int = 7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
    "        self.sa = SpatialAttention(kernel_size=spatial_kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        x = self.ca(x) * x\n",
    "        # Spatial attention\n",
    "        x = self.sa(x) * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1407c0",
   "metadata": {},
   "source": [
    "# 2) Pattern-1 Residual Block: “Attention inside residual branch” + CBAM\n",
    "\n",
    "Kural:\n",
    "\n",
    "* f = F(x) üret\n",
    "\n",
    "* f = CBAM(f) (maskelenmiş f döner)\n",
    "\n",
    "* y = x + f (skip path burada)\n",
    "\n",
    "Ayrıca downsample gerektiğinde (stride=2 veya kanal artışı) skip path’i 1x1 conv ile eşitliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d62a98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattern_1_Residual(nn.Module):\n",
    "    def __init__(self, in_ch:int , out_ch:int , stride :int = 1 ,reduction:int=16,spatial_kernel:int=7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        # PATTERN -- 1\n",
    "        self.cbam = CBAM(in_ch,out_ch,reduction=reduction,spatial_kernel=spatial_kernel)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(nn.Conv2d(in_ch,out_ch,kernel_size=1,stride=stride,bias=False),\n",
    "            nn.BatchNorm2d(out_ch)                              \n",
    "            )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        identity = x \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        f = self.act(self.bn1(self.conv1(x)))\n",
    "        f = self.bn2(self.conv2(x))\n",
    "\n",
    "        # attention inside residual branch: f -> cbam(f)  (çarpma CBAM içinde)\n",
    "        f = self.cbam(f) #  # f = A(F(x)) ⊙ F(x) (CBAM bunu yapmış oluyor)\n",
    "\n",
    "        y = identity +f \n",
    "        y = self.act(y)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084b438",
   "metadata": {},
   "source": [
    "> **Not: CBAM’in içindeki * x çarpmaları maskeyi uygular. Bu yüzden block içinde ayrıca alpha * f yazmıyoruz.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0a959",
   "metadata": {},
   "source": [
    "### CBAM’i conv’dan önce koyarsak ne olur?\n",
    "\n",
    "#### CBAM pre-attention olursa:\n",
    "\n",
    "* Channel + spatial mask, x’i filtreler\n",
    "\n",
    "* Conv’lar artık “seçili bölgeleri” işler\n",
    "\n",
    "Bu bazen işe yarar ama:\n",
    "\n",
    "* CBAM’in spatial maskesi erken katmanda “yanlış yerde” yoğunlaşırsa,\n",
    "model daha baştan bilgiyi kesebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce5de5",
   "metadata": {},
   "source": [
    "# 3) Bunu bir modele entegre edelim (Mini-ResNet)\n",
    "\n",
    "Aşağıdaki model:\n",
    "\n",
    "* Stem\n",
    "\n",
    "* 3 stage (channel: 64→128→256, her stage başında stride=2 downsample)\n",
    "\n",
    "* Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c02b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention_1(nn.Module):\n",
    "    def __init__(self, channels,reduction:int=16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels  // reduction,4)\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels,hidden,1,bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden,channels,1,bias=False))\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.mlp(self.avg(x)) + self.mlp(self.max(x)))\n",
    "    \n",
    "class SpatialAttention_1(nn.Module):\n",
    "    def __init__(self, kernel_size:int = 7):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = 3 if kernel_size == 7 else 1 \n",
    "        self.conv = nn.Conv2d(2,1,kernel_size,padding=padding , bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        avg_map = torch.mean(x,dim=1, keepdim=True) # B,1,H,W\n",
    "        max_map = torch.max(x,dim=1 , keepdim=True) # B,1,H,W\n",
    "\n",
    "        a = torch.cat([avg_map,max_map],dim=1)\n",
    "        return self.sigmoid(self.conv(a))\n",
    "\n",
    "class CBAM_1(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, spatial_kernel=7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention_1(channels, reduction)\n",
    "        self.sa = SpatialAttention_1(spatial_kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ca(x) * x\n",
    "        x = self.sa(x) * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36bcdcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResCBAM_Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1, reduction=16, spatial_kernel=7):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.cbam  = CBAM(out_ch, reduction=reduction, spatial_kernel=spatial_kernel)\n",
    "\n",
    "        self.skip = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x if self.skip is None else self.skip(x)\n",
    "\n",
    "        f = self.act(self.bn1(self.conv1(x)))\n",
    "        f = self.bn2(self.conv2(f))   # f = F(x)\n",
    "\n",
    "        f = self.cbam(f)              # f = A(F(x)) ⊙ F(x) (CBAM kendi içinde çarpar)\n",
    "\n",
    "        y = identity + f              # klasik residual toplama\n",
    "        y = self.act(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a267c0e",
   "metadata": {},
   "source": [
    "### MODEL - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8a50f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_With_ResCBAM(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Normal conv blok\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Buraya Pattern-1 entegre edildi \n",
    "        self.rescbam1 = ResCBAM_Block(64, 64, stride=1)     # çözünürlük aynı\n",
    "        self.rescbam2 = ResCBAM_Block(64, 128, stride=2)    # downsample + kanal artışı\n",
    "\n",
    "        # Devam: normal conv\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "\n",
    "        x = self.rescbam1(x)\n",
    "        x = self.rescbam2(x)\n",
    "\n",
    "        x = self.block2(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84e8f6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = SimpleCNN_With_ResCBAM(num_classes=10)\n",
    "    x = torch.randn(4, 3, 32, 32)\n",
    "    y = model(x)\n",
    "    print(y.shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593120e",
   "metadata": {},
   "source": [
    "---\n",
    "----\n",
    "----\n",
    "---\n",
    "\n",
    "# En net entegrasyon mantığı (kural)\n",
    "\n",
    "Eğer elimizdeki modelde herhangi bir yerde şu yapı varsa:\n",
    "\n",
    "... -> Conv/BN/ReLU -> Conv/BN -> ...\n",
    "\n",
    "\n",
    "bunu şu hale getiriyoruz:\n",
    "\n",
    "* identity = x\n",
    "* F(x) = convconv(x)\n",
    "* F_att = CBAM(F(x))   # içerde çarpma var\n",
    "* y = identity + F_att\n",
    "\n",
    "\n",
    "Kafamızdaki “ben nereye koyacağım?” sorusunun cevabı:\n",
    "\n",
    "* Conv’ların ürettiği bloğu F(x) kabul et\n",
    "\n",
    "* CBAM’i F(x)’in üstüne koy\n",
    "\n",
    "* Sonra skip ile topla\n",
    "\n",
    "---\n",
    "----\n",
    "----\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
