{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2c1e4c",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "-----\n",
    "---\n",
    "\n",
    "**Pattern-1, residual blokta üretilen F(x) feature map’ini attention ile yeniden ağırlıklandırıp, bu maskelenmiş çıktıyı dokunulmamış skip yolu x ile toplayan mimari yapıdır.**\n",
    "\n",
    "\n",
    "-----\n",
    "-----\n",
    "-----\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4a534",
   "metadata": {},
   "source": [
    "# Pattern 1 — *Attention Inside Residual Branch* (Residual + Attention Fusion)\n",
    "\n",
    "> Bu not, “**attention’ı residual bloğun içinde nereye koyuyoruz?**”, “**çarpıyor muyuz topluyor muyuz?**”, “**x nerede?**” gibi soruların üstünden ilerler.  \n",
    "> Amaç: konuyu **en temelden** başlayıp **ileri seviyeye** kadar oturtmak (kafa karışıklığı bırakmadan).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d98cd",
   "metadata": {},
   "source": [
    "## İçindekiler\n",
    "1. Residual blok: x ve F(x) ne demek?\n",
    "2. Pattern 1: “Attention inside residual branch” tam olarak ne?\n",
    "3. Çarpma mı, toplama mı? (En kritik ayrım)\n",
    "4. “x nerede?” — skip path’i gözle görünür hale getirme\n",
    "5. Boyutlar: maske (α) hangi shape’te olur?\n",
    "6. SE / ECA / CBAM / Coordinate Attention bu pattern’e nasıl oturur?\n",
    "7. İleri seviye: gradient akışı neden bozulmuyor?\n",
    "8. Sık yapılan hatalar ve doğru-yanlış karşılaştırmaları\n",
    "9. PyTorch iskeleti (minimal ve doğru)\n",
    "10. Mini kontrol listesi (debug checklist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e17dc",
   "metadata": {},
   "source": [
    "## 1) Residual blok: x ve F(x) ne demek?\n",
    "\n",
    "Bir residual blokta iki yol var:\n",
    "\n",
    "- **Kısa yol (skip / identity):** `x` aynen taşınır.\n",
    "- **Uzun yol (residual branch):** `x` üstünde konv katmanları çalışır ve modelin “ek bilgi” dediğimiz çıktısı üretilir: `F(x)`.\n",
    "\n",
    "En sade denklem:\n",
    "\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "Buradaki sezgi:\n",
    "- `x` = “zaten elimde olan temel bilgi”\n",
    "- `F(x)` = “modelin bu temel bilginin üstüne eklemek istediği düzeltme / katkı”\n",
    "\n",
    "**Önemli:** Normal residual’da model, `F(x)` içindeki her şeyi “eşit değerli” gibi ekler. Yani bir kanal gürültü bile olsa, yine eklenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c6010",
   "metadata": {},
   "source": [
    "## 2) Pattern 1: “Attention inside residual branch” tam olarak ne?\n",
    "\n",
    "Bu pattern’in cümlelik tanımı:\n",
    "\n",
    "> Attention **yalnızca residual branch’in ürettiği** `F(x)` üzerinde hesaplanır ve `F(x)`’i **yeniden ağırlıklandırır**.  \n",
    "> Sonrasında klasik residual toplaması aynen yapılır.\n",
    "\n",
    "Yani akış şu:\n",
    "\n",
    "1) `F(x)` üret  \n",
    "2) `F(x)` → attention modülüne ver  \n",
    "3) attention bir **maske/ağırlık** üretir: `α = A(F(x))`  \n",
    "4) bu maske `F(x)` ile **çarpılır**: `F_att(x) = α ⊙ F(x)`  \n",
    "5) en son: `y = x + F_att(x)`\n",
    "\n",
    "Denklem olarak:\n",
    "\n",
    "\\[\n",
    "y = x + \\big(A(F(x)) \\odot F(x)\\big)\n",
    "\\]\n",
    "\n",
    "Burada `x` hiçbir noktada attention’a girmez. `x` sadece sonda “+” noktasında devreye girer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84e084",
   "metadata": {},
   "source": [
    "## 3) Çarpma mı, toplama mı? (En kritik ayrım)\n",
    "\n",
    "\n",
    "### Doğrusu: **ÇARPMA**\n",
    "Attention’ın çıktısı genelde **maske** (ağırlık) olduğu için, feature map’i **açar/kısar**.\n",
    "\n",
    "\\[\n",
    "F_{att}(x) = A(F(x)) \\odot F(x)\n",
    "\\]\n",
    "\n",
    "- `⊙` = eleman bazlı çarpma (broadcast ile kanal/uzam boyunca uygulanır)\n",
    "- Bu, “ses ayarı” gibi: ses (`F(x)`) var, düğme (`A(F(x))`) sesi açıp kapatıyor.\n",
    "\n",
    "### Neden toplama değil?\n",
    "Eğer `F(x) + A(F(x))` gibi toplasaydık:\n",
    "- attention “yeni bir feature” gibi davranırdı\n",
    "- maskenin anlamı bozulurdu\n",
    "- pratikte beklenen etkiyi vermez (ve çoğu modül böyle tanımlanmaz)\n",
    "\n",
    "**Kısa cümle:**  \n",
    "> Pattern 1’de attention **ek bilgi üretmez**; `F(x)`’in içindeki bilgiyi **seçici hale getirir**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf110ff0",
   "metadata": {},
   "source": [
    "## 4) “x nerede?” — skip path’i görünür hale getirelim\n",
    "\n",
    "Bu pattern’de `x` iki yerde “görünür”:\n",
    "\n",
    "1) **Başlangıçta:** Bloğun girdisi olarak var.\n",
    "2) **Sonda:** `+` düğümünde tekrar devreye girer.\n",
    "\n",
    "Şöyle düşün:\n",
    "\n",
    "```bash\n",
    "x --------------------------┐\n",
    "                             ├─  y\n",
    "x → (Conv…Conv) → F(x) → A → ⊙ ┘\n",
    "```\n",
    "\n",
    "- Üst çizgi: **skip path** (x hiç bozulmadan taşınıyor)\n",
    "- Alt çizgi: residual branch (F(x) üretiliyor, attention ile ağırlıklandırılıyor)\n",
    "- Birleşim: **toplama** (residual’ın ruhu)\n",
    "\n",
    "**Bu yüzden** pattern 1 çok stabil: attention yanlış öğrense bile `x` yolu “hayatta kalma hattı” gibi çalışır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab42794",
   "metadata": {},
   "source": [
    "## 5) Boyutlar: maske (α) hangi shape’te olur?\n",
    "\n",
    "Giriş `F(x)` tipik olarak:\n",
    "\n",
    "\\[\n",
    "F(x) \\in \\mathbb{R}^{C \\times H \\times W}\n",
    "\\]\n",
    "\n",
    "Attention modülüne göre `α` farklı shape’lerde çıkar:\n",
    "\n",
    "### (a) Channel attention (SE / ECA)\n",
    "\\[\n",
    "\\alpha \\in \\mathbb{R}^{C \\times 1 \\times 1}\n",
    "\\]\n",
    "- Her **kanal** için bir ağırlık\n",
    "- Broadcast ile tüm `H×W` noktalarına yayılır\n",
    "\n",
    "### (b) Spatial attention (CBAM’ın spatial kısmı)\n",
    "\\[\n",
    "\\alpha \\in \\mathbb{R}^{1 \\times H \\times W}\n",
    "\\]\n",
    "- Her **uzamsal konum** için bir ağırlık\n",
    "- Broadcast ile tüm kanallara yayılır\n",
    "\n",
    "### (c) Channel + Spatial (CBAM)\n",
    "Genelde sırayla uygulanır:\n",
    "- önce kanal maskesi, sonra uzamsal maske\n",
    "- iki çarpma üst üste gelir\n",
    "\n",
    "\\[\n",
    "F' = \\alpha_c \\odot F,\\quad F_{att} = \\alpha_s \\odot F'\n",
    "\\]\n",
    "\n",
    "**Not:** Bu maskeler genelde `sigmoid` ile 0–1 aralığına çekilir (tam kapatma değil, yumuşak seçim).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0ad75",
   "metadata": {},
   "source": [
    "## 6) SE / ECA / CBAM / Coordinate Attention bu pattern’e nasıl oturur?\n",
    "\n",
    "Hepsinin ortak tarafı:  \n",
    "> `F(x)`’e bakıp bir ağırlık/mask çıkarırlar ve `F(x)` ile çarparlar.\n",
    "\n",
    "### SE (Squeeze-and-Excitation)\n",
    "- “Squeeze”: `F(x)`’i global average pooling ile **kanal özetine** indirger\n",
    "- “Excitation”: küçük MLP ile kanal ağırlıkları üretir\n",
    "- Uygulama: `F_att = α_c ⊙ F(x)`\n",
    "\n",
    "### ECA (Efficient Channel Attention)\n",
    "- SE’ye benzer ama MLP yerine 1D conv ile hafif kanal etkileşimi\n",
    "- Uygulama yine aynı: kanal maskesi × feature\n",
    "\n",
    "### CBAM\n",
    "- Kanal maskesi + uzamsal maske\n",
    "- Uygulama: iki aşamalı çarpma\n",
    "\n",
    "### Coordinate Attention (CA)\n",
    "- Uzamsal bilgiyi (x-y yönleri) daha yapılandırılmış taşır\n",
    "- Çıktısı yine maskeler; sonuçta `F(x)` ile çarpılır\n",
    "\n",
    "**Özet:** Modül farklı, fusion pattern aynı: `F(x)` üzerinde maske üret → `F(x)` ile çarp → `x` ile topla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fec731",
   "metadata": {},
   "source": [
    "## 7) İleri seviye: Gradient akışı neden bozulmuyor?\n",
    "\n",
    "Bu pattern’in “güvenli” olmasının nedeni şu:\n",
    "\n",
    "- `x` yolu **tamamen açık** (identity)\n",
    "- Attention yalnızca `F(x)`’i etkiliyor\n",
    "- Dolayısıyla geri yayılımda (backprop) `∂y/∂x` içinde “1” gibi direkt bir bileşen kalıyor\n",
    "\n",
    "Denklem:\n",
    "\n",
    "\\[\n",
    "y = x + (\\alpha \\odot F(x))\n",
    "\\]\n",
    "\n",
    "Burada `x` doğrudan var → gradient yolu kapanmıyor.  \n",
    "Bu, çok derin ağlarda eğitimi kolaylaştıran ana mekanizma.\n",
    "\n",
    "**Pratik sezgi:**  \n",
    "> Attention bir şeyleri “kısabilir”, ama modeli “nefessiz bırakamaz” çünkü skip yol hep açık.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842adb46",
   "metadata": {},
   "source": [
    "## 8) Sık yapılan hatalar (doğru-yanlış)\n",
    "\n",
    "### Hata 1: “(x + F(x)) + A(x)” gibi düşünmek\n",
    "- Bu pattern değil.\n",
    "- Attention burada ayrı bir feature map gibi eklenmiş olur → konsept kayar.\n",
    "\n",
    "✅ Doğru:  \n",
    "\\[\n",
    "y = x + (A(F(x)) \\odot F(x))\n",
    "\\]\n",
    "\n",
    "### Hata 2: Attention’ı `x` üzerinde hesaplamak (bu pattern’de)\n",
    "- Pattern 1’in tanımı gereği attention `F(x)` üzerinde hesaplanır.\n",
    "- `x`’i de sokarsan farklı fusion pattern’e geçersin (daha agresif kontrol).\n",
    "\n",
    "### Hata 3: Maskeyi toplamak\n",
    "- Attention çıktısı çoğunlukla maske → toplamak yerine çarpmak gerekir.\n",
    "\n",
    "### Hata 4: Broadcast şekillerini yanlış yapmak\n",
    "- `C×1×1` ile `C×H×W` çarpılır (kanal attention)\n",
    "- `1×H×W` ile `C×H×W` çarpılır (spatial attention)\n",
    "\n",
    "Broadcast yanlışsa model sessizce “yanlış şeyi” öğrenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf8ae3",
   "metadata": {},
   "source": [
    "## 9) PyTorch iskeleti (minimal ve doğru)\n",
    "\n",
    "Aşağıdaki kod, pattern 1’in mantığını doğrudan gösterir:\n",
    "- Residual branch: `F(x)`\n",
    "- Attention: `α = A(F(x))`\n",
    "- Re-weight: `F_att = α ⊙ F(x)`\n",
    "- Skip add: `y = x + F_att`\n",
    "\n",
    "Aşağıdaki attention (kanal maskesi) sadece örnek. SE/CBAM/CA modülünü `SimpleChannelAttention` yerine takarsın.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b92e89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 32, 32])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleChannelAttention(nn.Module):\n",
    "    # Basit örnek: F(x) -> GAP -> 1x1 conv -> ReLU -> 1x1 conv -> Sigmoid -> (C,1,1) maske\n",
    "    # Amaç: pattern'i göstermek.\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels // reduction, 4)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, kernel_size=1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, kernel_size=1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, f):          # f: (B,C,H,W)\n",
    "        w = self.gap(f)            # (B,C,1,1)\n",
    "        w = self.net(w)            # (B,C,1,1)  -> alpha\n",
    "        return w\n",
    "\n",
    "class ResidualBlock_AttnInside(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # Pattern 1: attention residual branch içinde, F(x) üstünde\n",
    "        self.attn  = SimpleChannelAttention(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x               # x yolu (skip) korunuyor\n",
    "\n",
    "        f = self.act(self.bn1(self.conv1(x)))\n",
    "        f = self.bn2(self.conv2(f))      # f = F(x)\n",
    "\n",
    "        alpha = self.attn(f)             # alpha = A(F(x))\n",
    "        f_att = alpha * f                # F_att(x) = alpha ⊙ f\n",
    "\n",
    "        y = identity + f_att             # y = x + F_att(x)\n",
    "        y = self.act(y)\n",
    "        return y\n",
    "\n",
    "# mini test\n",
    "B, C, H, W = 2, 64, 32, 32\n",
    "x = torch.randn(B, C, H, W)\n",
    "block = ResidualBlock_AttnInside(C)\n",
    "y = block(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cea83a",
   "metadata": {},
   "source": [
    "## 10) Mini kontrol listesi (debug checklist)\n",
    "\n",
    "Pattern 1’i doğru uyguladım mı?\n",
    "\n",
    "- [ ] Attention’a **F(x)** verdim mi? (x değil)\n",
    "- [ ] Attention çıktısı **maske** mi? (shape doğru mu?)\n",
    "- [ ] Maske ile `F(x)` arasında **çarpma** yaptım mı?\n",
    "- [ ] En sonda `x + (...)` toplaması var mı?\n",
    "- [ ] Skip path’i (identity) hiç bozmadım mı?\n",
    "- [ ] Broadcast doğru mu? (`C×1×1` veya `1×H×W`)\n",
    "\n",
    "Bu maddeler tamam ise “inside residual branch” fusion doğru kurulmuştur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a0685",
   "metadata": {},
   "source": [
    "---\n",
    "## Kısa özet (tek paragraf)\n",
    "\n",
    "Pattern 1’de attention, residual bloğun ürettiği `F(x)` üzerinde bir maske `α` üretir ve `F(x)`’i **çarpma** ile yeniden ağırlıklandırır. Skip yolu olan `x` bu sırada **hiç dokunulmadan** taşınır ve en sonda `y = x + (α ⊙ F(x))` şeklinde birleşir. Bu yüzden pattern 1 hem sezgisel hem de eğitim açısından en stabil attention entegrasyonlarından biridir.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
