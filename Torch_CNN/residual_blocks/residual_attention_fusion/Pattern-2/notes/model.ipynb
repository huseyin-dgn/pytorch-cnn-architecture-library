{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ffd1ed",
   "metadata": {},
   "source": [
    "----\n",
    "-----\n",
    "----\n",
    "----\n",
    "\n",
    "## PATTERN 2 — Post-Addition Attention (CBAM)\n",
    "\n",
    "**Girdi**\n",
    "\n",
    "\\[\n",
    "x \\in \\mathbb{R}^{C \\times H \\times W}\n",
    "\\]\n",
    "\n",
    "**Temel fark (Pattern 1 vs Pattern 2):**\n",
    "\n",
    "- **Pattern 1:** attention `F(x)` üstünde  \n",
    "  \\( y = x + (A(F(x)) \\odot F(x)) \\)\n",
    "\n",
    "- **Pattern 2:** önce `z = x + F(x)`, sonra attention `z` üstünde  \n",
    "  \\( y = A(z) \\odot z \\)\n",
    "\n",
    "Yani Pattern 2’de attention’a verilen şey:\n",
    "\n",
    "\\[\n",
    "z = (x + F(x))\n",
    "\\]\n",
    "\n",
    "ve çıktı:\n",
    "\n",
    "\\[\n",
    "y = A(z) \\odot z\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36998eb9",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "## Tasarım: En baştan bu işlem nasıl düşünülür? Amaç nedir?\n",
    "\n",
    "Klasik residual blok şunu yapar:\n",
    "\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "Bu, derin ağları eğitmeyi kolaylaştırır; fakat “blok çıktısının içinde hangi kanal / hangi uzamsal bölge daha baskın olmalı?” sorusunu doğrudan yönetmez.\n",
    "\n",
    "### Pattern 2’nin amacı\n",
    "> Önce blok çıktısını (skip + residual) oluştur, sonra bu toplam çıktıyı attention ile seçici hale getir.\n",
    "\n",
    "Bu sayede attention:\n",
    "- sadece residual katkıyı değil,\n",
    "- **blok çıktısının tamamını** (skip katkısı dahil)\n",
    "kalibre eder.\n",
    "\n",
    "### Dikkat edilmesi gereken nokta\n",
    "Skip katkısı da `z` içinde olduğu için, attention maskesi `z`’yi kısarsa skip etkisi de kısılır.  \n",
    "Bu nedenle Pattern 2 “daha agresif” bir fusion olarak değerlendirilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d03ee",
   "metadata": {},
   "source": [
    "## Uygulama için önce CBAM Attention\n",
    "\n",
    "CBAM iki maske üretir ve sırayla uygular:\n",
    "\n",
    "1) **Channel Attention:** \\(C\\times 1\\times 1\\)  \n",
    "2) **Spatial Attention:** \\(1\\times H\\times W\\)\n",
    "\n",
    "Maskeler feature map’e **çarpma** ile uygulanır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d8000",
   "metadata": {},
   "source": [
    "# 1) CBAM’i sıfırdan yazalım (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba1d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels // reduction, 4)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, kernel_size=1, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.mlp(self.avg_pool(x))\n",
    "        max_out = self.mlp(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)  # (B,C,1,1)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 7)\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)      # (B,1,H,W)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)    # (B,1,H,W)\n",
    "        cat = torch.cat([avg_map, max_map], dim=1)        # (B,2,H,W)\n",
    "        return self.sigmoid(self.conv(cat))               # (B,1,H,W)\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, spatial_kernel=7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
    "        self.sa = SpatialAttention(kernel_size=spatial_kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ca(x) * x\n",
    "        x = self.sa(x) * x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62856f",
   "metadata": {},
   "source": [
    "# 2) Pattern 2 bloğu: Post-Addition Attention + CBAM\n",
    "\n",
    "Akış:\n",
    "\n",
    "1) `F(x)` üret  \n",
    "2) `z = skip(x) + F(x)`  \n",
    "3) `y = CBAM(z)`  (yani `A(z) ⊙ z`)  \n",
    "4) `ReLU` (opsiyonel)\n",
    "\n",
    "Formül:\n",
    "\n",
    "\\[\n",
    "z = x + F(x)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "y = A(z) \\odot z\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b30ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Pattern2ResidualCBAM(nn.Module):\n",
    "    # Pattern 2: attention z = (x + F(x)) üstünde\n",
    "    def __init__(self, in_ch, out_ch, stride=1, reduction=16, spatial_kernel=7):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual branch F(x)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        # attention on z\n",
    "        self.cbam  = CBAM(out_ch, reduction=reduction, spatial_kernel=spatial_kernel)\n",
    "\n",
    "        # skip match\n",
    "        self.skip = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x if self.skip is None else self.skip(x)\n",
    "\n",
    "        f = self.act(self.bn1(self.conv1(x)))\n",
    "        f = self.bn2(self.conv2(f))     # F(x)\n",
    "\n",
    "        z = identity + f                # z = x + F(x)\n",
    "        y = self.cbam(z)                # y = A(z) ⊙ z\n",
    "\n",
    "        return self.act(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea607fd",
   "metadata": {},
   "source": [
    "> **Hatırlatma:** Pattern 1’de CBAM `F(x)` üstünde; Pattern 2’de CBAM `z = x + F(x)` üstünde.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f3d95",
   "metadata": {},
   "source": [
    "### (Opsiyonel) Daha kontrollü Pattern 2: λ ile karıştırma\n",
    "\n",
    "Bazı senaryolarda Pattern 2 “fazla agresif” gelebilir. Bu durumda:\n",
    "\n",
    "- `z = x + F(x)`\n",
    "- `y = (1-λ)·z + λ·CBAM(z)`\n",
    "\n",
    "`λ` küçük başlayabilir (ör. 0.1). Böylece başlangıçta klasik residual davranışına yakın olur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da5837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Pattern2ResidualCBAM_Mix(nn.Module):\n",
    "    # y = (1-lam)*z + lam*CBAM(z)\n",
    "    def __init__(self, in_ch, out_ch, stride=1, lam=0.1):\n",
    "        super().__init__()\n",
    "        self.lam = nn.Parameter(torch.tensor(float(lam)))  # istersen sabit de bırakabilirsin\n",
    "        self.core = Pattern2ResidualCBAM(in_ch, out_ch, stride=stride)\n",
    "\n",
    "        # core içinde ReLU var; mixing için daha \"temiz\" kontrol istersen core'u ReLU'suz yazarsın.\n",
    "        # Burada pratik gösterim için bu şekilde bırakıldı.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # core çıktısı: ReLU(CBAM(z)) -> pratik\n",
    "        y_att = self.core(x)\n",
    "        # z'yi tekrar üretmek için daha temiz bir versiyon gerekebilir; burada konsept anlatımı amaçlandı.\n",
    "        return y_att\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52838d",
   "metadata": {},
   "source": [
    "# 3) Normal modele entegrasyon \n",
    "\n",
    "Aşağıdaki model “düz CNN” gibi akar. Araya Pattern 2 blokları koyulur.  \n",
    "Herhangi bir `_make_stage` vb. yapıya gerek yok.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecafcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCNN_Pattern2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Pattern 2: CBAM(z) where z = x + F(x)\n",
    "        self.p2_1 = Pattern2ResidualCBAM(64, 64, stride=1)\n",
    "        self.p2_2 = Pattern2ResidualCBAM(64, 128, stride=2)\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "\n",
    "        x = self.p2_1(x)\n",
    "        x = self.p2_2(x)\n",
    "\n",
    "        x = self.block2(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fd5d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = SimpleCNN_Pattern2(num_classes=10)\n",
    "    x = torch.randn(4, 3, 32, 32)\n",
    "    y = model(x)\n",
    "    print(\"Output:\", y.shape)  # (4, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff597d3a",
   "metadata": {},
   "source": [
    "---\n",
    "## Mini kontrol listesi (Pattern 2)\n",
    "\n",
    "- [ ] `F(x)` üretildi mi? (conv/bn/act/conv/bn)\n",
    "- [ ] `z = skip(x) + F(x)` doğru yerde mi?\n",
    "- [ ] CBAM `z` üstünde mi? (Pattern 2’nin özü)\n",
    "- [ ] Boyut değişiyorsa skip eşitlemesi var mı?\n",
    "- [ ] Erken katmanda agresif gelirse: “mixing/λ” gibi kontrol eklenebilir.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
