{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c16dcfb",
   "metadata": {},
   "source": [
    "# Pattern 2 — *Post-Addition Attention* (Residual + Attention Fusion)\n",
    "\n",
    "Bu not, **Pattern 2**’yi “en temel → en ileri” sırayla anlatır ve **Pattern 1** ile net şekilde karşılaştırır.  \n",
    "Odak: *Attention’ı nereye koyduğun* (fusion), *çarpma-toplama*, *skip güvenliği*, *stabilizasyon*.\n",
    "\n",
    "---\n",
    "## İçindekiler\n",
    "1. Residual blok hatırlatma: `x`, `F(x)` ve `y`\n",
    "2. Pattern 1 (Inside residual branch) — kısa tekrar\n",
    "3. Pattern 2 (Post-addition attention) — temel tanım\n",
    "4. Neden “daha agresif”? Skip güvenliği ne olur?\n",
    "5. Pattern 2’nin artıları / eksileri (pratik sezgi)\n",
    "6. Stabil hale getirme yöntemleri (λ, identity-preserving, init)\n",
    "7. CBAM ile Pattern 2 (channel + spatial) akışı\n",
    "8. Kod: Pattern 1 vs Pattern 2 block (PyTorch)\n",
    "9. Hızlı debug checklist (sık hatalar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f1074",
   "metadata": {},
   "source": [
    "## 1) Residual blok hatırlatma: `x`, `F(x)` ve `y`\n",
    "\n",
    "Bir residual blok iki akıştan oluşur:\n",
    "\n",
    "- **Skip/identity yolu:** `x` (referans bilgi, kısa yol)\n",
    "- **Residual yolu:** `F(x)` (modelin öğrendiği “düzeltme/katkı”)\n",
    "\n",
    "En klasik form:\n",
    "\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "Burada kritik nokta: Skip yolu sayesinde **gradient yolu açık kalır**. Bu, derin ağlarda eğitim stabilitesinin temelidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498aee3",
   "metadata": {},
   "source": [
    "## 2) Pattern 1 (Inside residual branch) — kısa tekrar\n",
    "\n",
    "Pattern 1’de attention **sadece** residual yolun ürettiği `F(x)` üzerinde çalışır:\n",
    "\n",
    "\\[\n",
    "y = x + \\big(A(F(x)) \\odot F(x)\\big)\n",
    "\\]\n",
    "\n",
    "- Attention’a giren: `F(x)`\n",
    "- Attention çıktısı: maske/weights `A(F(x))`\n",
    "- İşlem: **çarpma** (⊙) ile `F(x)` yeniden ağırlıklandırılır\n",
    "- Skip yolu `x`: **dokunulmaz**, en sonda toplanır\n",
    "\n",
    "**Sezgi:** “Eklediğim şeyi seç; referansa dokunma.”  \n",
    "Bu yüzden Pattern 1 genelde **en güvenli default** kabul edilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db2f1e",
   "metadata": {},
   "source": [
    "## 3) Pattern 2 (Post-Addition Attention) — temel tanım\n",
    "\n",
    "Pattern 2’de önce **klasik residual toplaması yapılır**, sonra attention bu toplamın üstüne uygulanır.\n",
    "\n",
    "Adımlar:\n",
    "\n",
    "1) Klasik birleşim:\n",
    "\\[\n",
    "z = x + F(x)\n",
    "\\]\n",
    "\n",
    "2) Attention, birleşik temsil üzerinde:\n",
    "\\[\n",
    "y = A(z) \\odot z\n",
    "\\]\n",
    "\n",
    "Yani Pattern 2’nin net formu:\n",
    "\n",
    "\\[\n",
    "y = A(x + F(x)) \\odot (x + F(x))\n",
    "\\]\n",
    "\n",
    "**Sezgi:** “Önce toplam sonucu oluştur; sonra toplam sonucu seç/filtrele.”\n",
    "\n",
    "Buradaki büyük fark:\n",
    "- Pattern 1: attention yalnızca `F(x)`’i etkiler\n",
    "- Pattern 2: attention **hem** `F(x)` hem `x` (skip katkısı) üstünde etkilidir çünkü ikisi `z` içinde birleşmiştir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90297242",
   "metadata": {},
   "source": [
    "## 4) Neden “daha agresif”? Skip güvenliği ne olur?\n",
    "\n",
    "Pattern 2’de attention, **skip yolunun katkısını da ölçekler**. Çünkü skip katkısı artık `z` içinde.\n",
    "\n",
    "- Pattern 1’de `x` her zaman “garanti hat” gibi akar.\n",
    "- Pattern 2’de `x` doğrudan korunmaz; `A(z)` maskesi `z`’yi kısarsa, skip’in etkisi de kısılır.\n",
    "\n",
    "Bu yüzden Pattern 2:\n",
    "- temsil gücünü artırabilir (toplam sonucu seçer)\n",
    "- ama yanlış yerde/yanlış şekilde kullanılırsa eğitim dinamiğini daha hassas yapabilir.\n",
    "\n",
    "**Basit düşünce:**  \n",
    "Pattern 1 = “eklenen kısmı ayarla”  \n",
    "Pattern 2 = “toplamı ayarla” (skip dahil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281aa153",
   "metadata": {},
   "source": [
    "## 5) Pattern 2’nin artıları / eksileri (pratik sezgi)\n",
    "\n",
    "### Artıları\n",
    "- **Daha güçlü seçicilik:** Attention, toplam temsilin tamamını düzenler.\n",
    "- **Gürültü bastırma:** Bazı katmanlarda (özellikle daha üst seviyelerde) birleşik temsil temizlenebilir.\n",
    "- **Feature yeniden kalibrasyonu:** Sadece residual katkı değil, “nihai blok çıktısı” kalibre edilir.\n",
    "\n",
    "### Eksileri\n",
    "- **Skip güvenliği azalır:** `A(z)` küçük değerler öğrenirse, `x` katkısı da azalır.\n",
    "- **Daha hassas eğitim:** Öğrenme oranı, init, sigmoid saturasyonu gibi detaylara daha duyarlı olabilir.\n",
    "- **Erken katmanlarda risk:** İlk katmanlarda temsil hamdır; agresif attention erken bilgi kaybına yol açabilir.\n",
    "\n",
    "Kısa kural:\n",
    "- Pattern 1: genelde güvenli “default”\n",
    "- Pattern 2: daha agresif; genelde “later stages” veya kontrollü gating ile daha mantıklı\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce70b3b",
   "metadata": {},
   "source": [
    "## 6) Stabil hale getirme yöntemleri (λ, identity-preserving, init)\n",
    "\n",
    "Pattern 2’yi pratikte “kontrollü” yapmak için birkaç yaygın yaklaşım vardır.\n",
    "\n",
    "### (a) Öğrenilebilir/ayarlı ölçek (λ) ile karıştırma (mixing)\n",
    "Önce `z = x + F(x)` sonra:\n",
    "\n",
    "\\[\n",
    "y = (1-\\lambda)\\, z + \\lambda \\, (A(z)\\odot z)\n",
    "\\]\n",
    "\n",
    "- \\(\\lambda=0\\) → tamamen klasik residual (y=z)\n",
    "- \\(\\lambda=1\\) → tam Pattern 2\n",
    "- Pratikte \\(\\lambda\\) küçük başlatılabilir (ör. 0.1) veya öğrenilebilir.\n",
    "\n",
    "Bu yöntem şu anlama gelir:  \n",
    "> “Attention’a ip bağla; başta az etki etsin, sonra aç.”\n",
    "\n",
    "### (b) Identity-preserving attention (sapmayı ölçekle)\n",
    "\\[\n",
    "y = z + \\lambda \\, (A(z)\\odot z - z)\n",
    "\\]\n",
    "\n",
    "Bu form, “attention uygulanmış hâl” ile “orijinal hâl” arasındaki farkı ölçekler. Yine \\(\\lambda\\) küçük başlatılabilir.\n",
    "\n",
    "### (c) Initialization ile güvenli başlangıç\n",
    "- Maskeyi üreten katmanların bias/init ayarı ile `A(z) ≈ 1` başlangıcı hedeflenir.\n",
    "- Böylece model başta “normal residual gibi” davranır, attention yavaşça öğrenilir.\n",
    "\n",
    "**Özet:** Pattern 2 kullanılacaksa, çoğu zaman “tam serbest bırakmak” yerine **kontrollü açmak** daha güvenlidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c52eb",
   "metadata": {},
   "source": [
    "## 7) CBAM ile Pattern 2 (channel + spatial) akışı\n",
    "\n",
    "CBAM iki maskeyi ardışık uygular:\n",
    "- Channel maskesi: \\(\\alpha_c \\in \\mathbb{R}^{C\\times 1\\times 1}\\)\n",
    "- Spatial maskesi: \\(\\alpha_s \\in \\mathbb{R}^{1\\times H\\times W}\\)\n",
    "\n",
    "Pattern 2’de CBAM’in girdiği şey `z`’dir:\n",
    "\n",
    "1) Birleşim:\n",
    "\\[\n",
    "z = x + F(x)\n",
    "\\]\n",
    "\n",
    "2) Channel attention (z üstünde):\n",
    "\\[\n",
    "z' = \\alpha_c(z) \\odot z\n",
    "\\]\n",
    "\n",
    "3) Spatial attention (z' üstünde):\n",
    "\\[\n",
    "y = \\alpha_s(z') \\odot z'\n",
    "\\]\n",
    "\n",
    "Bu durumda CBAM, skip katkısını da içerdiği için **blok çıkışını komple kalibre etmiş** olur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3dfba",
   "metadata": {},
   "source": [
    "## 8) Kod: Pattern 1 vs Pattern 2 block (PyTorch)\n",
    "\n",
    "Aşağıdaki kod iki bloğu yan yana verir:\n",
    "\n",
    "- **Pattern 1:** CBAM yalnızca `F(x)` üstünde\n",
    "- **Pattern 2:** önce `z = x + F(x)`, sonra CBAM `z` üstünde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1: torch.Size([2, 64, 32, 32])\n",
      "P2: torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        hidden = max(channels // reduction, 4)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.mlp(self.avg(x)) + self.mlp(self.max(x)))\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, k=7):\n",
    "        super().__init__()\n",
    "        pad = 3 if k == 7 else 1\n",
    "        self.conv = nn.Conv2d(2, 1, k, padding=pad, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        cat = torch.cat([avg_map, max_map], dim=1)\n",
    "        return self.sigmoid(self.conv(cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, k=7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, reduction)\n",
    "        self.sa = SpatialAttention(k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ca(x) * x\n",
    "        x = self.sa(x) * x\n",
    "        return x\n",
    "\n",
    "class FxConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.act   = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.act(self.bn1(self.conv1(x)))\n",
    "        f = self.bn2(self.conv2(f))\n",
    "        return f\n",
    "\n",
    "def make_skip(in_ch, out_ch, stride):\n",
    "    if stride != 1 or in_ch != out_ch:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "    return nn.Identity()\n",
    "\n",
    "class Block_P1(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.F = FxConv(in_ch, out_ch, stride=stride)\n",
    "        self.skip = make_skip(in_ch, out_ch, stride)\n",
    "        self.cbam = CBAM(out_ch)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        f = self.F(x)       # F(x)\n",
    "        f = self.cbam(f)    # A(F(x)) ⊙ F(x)\n",
    "        y = identity + f\n",
    "        return self.act(y)\n",
    "\n",
    "class Block_P2(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.F = FxConv(in_ch, out_ch, stride=stride)\n",
    "        self.skip = make_skip(in_ch, out_ch, stride)\n",
    "        self.cbam = CBAM(out_ch)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        f = self.F(x)            # F(x)\n",
    "        z = identity + f         # z = x + F(x)\n",
    "        y = self.cbam(z)         # y = A(z) ⊙ z\n",
    "        return self.act(y)\n",
    "\n",
    "x = torch.randn(2, 64, 32, 32)\n",
    "print(\"P1:\", Block_P1(64,64)(x).shape)\n",
    "print(\"P2:\", Block_P2(64,64)(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dceb13",
   "metadata": {},
   "source": [
    "## 9) Hızlı debug checklist (sık hatalar)\n",
    "\n",
    "### Pattern 2\n",
    "- [ ] Önce `z = x + F(x)` var mı?\n",
    "- [ ] Attention’a giren `z` mi?\n",
    "- [ ] Attention sonucu **çarpma** ile mi uygulanıyor? (`A(z) ⊙ z`)\n",
    "- [ ] Boyut/kanal değişiyorsa skip eşitlemesi var mı?\n",
    "\n",
    "### Pattern 1 vs Pattern 2\n",
    "- **Pattern 1:** attention `F(x)` üstünde → skip korunur  \n",
    "- **Pattern 2:** attention `z` üstünde → skip de kalibre edilir\n",
    "\n",
    "### Stabilizasyon (öneri)\n",
    "- [ ] Pattern 2’de \\(\\lambda\\) ile karıştırma/ölçek kullanıyor muyum?\n",
    "- [ ] Başlangıçta `A(z) ≈ 1` olacak init/bias stratejim var mı?\n",
    "\n",
    "---\n",
    "## Tek cümlelik fark\n",
    "- **Pattern 1:** “Sadece residual katkıyı seç; skip’i koru.”  \n",
    "- **Pattern 2:** “Toplam sonucu seç; skip dahil hepsini kalibre et.”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
