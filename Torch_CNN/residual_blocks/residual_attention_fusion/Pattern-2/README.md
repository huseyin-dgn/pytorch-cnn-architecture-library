# Controlled Attentionâ€“Residual Fusion (Pattern-2)

Bu yapÄ±, klasik **Attentionâ€“Residual fusion**â€™Ä±n daha kontrollÃ¼ bir versiyonudur.  
AmaÃ§: Attentionâ€™Ä±n residual temsili **ne kadar etkileyeceÄŸini Ã¶ÄŸrenilebilir bir katsayÄ±yla ayarlamak**.

---

## ğŸ¯ Temel Denklem

Ã–nce klasik residual Ã§Ä±ktÄ± Ã¼retilir:

**z = Skip(x) + F(x)**

ArdÄ±ndan attention uygulanmÄ±ÅŸ versiyon elde edilir:

**z_att = A(z) âŠ™ z**

Son olarak iki temsil karÄ±ÅŸtÄ±rÄ±lÄ±r:

**Ã‡Ä±kÄ±ÅŸ = Aktivasyon( (1âˆ’Î»)Â·z + Î»Â·z_att )**

---

## ğŸ§  Patternâ€™in MantÄ±ÄŸÄ±

Bu tasarÄ±mda attention doÄŸrudan zorla uygulanmaz.  
Bunun yerine model ÅŸuna karar verir:

> â€œResidual temsil mi daha Ã¶nemli, yoksa attention ile filtrelenmiÅŸ temsil mi?â€

Bu dengeyi **Î» (lambda)** belirler.

---

## ğŸ”€ Pattern-2 Fusion Ã–zelliÄŸi

Bu pattern, klasik â€œattention â†’ residual toplamaâ€ yerine:

**Ã–nce residual oluÅŸtur â†’ sonra attention ile yeniden aÄŸÄ±rlÄ±klandÄ±r â†’ iki temsili karÄ±ÅŸtÄ±r**

ÅŸeklinde Ã§alÄ±ÅŸÄ±r.

Bu yÃ¼zden adÄ±:

> **Post-Residual Controlled Attention Fusion**

---

## âš™ï¸ Î» (Lambda) Nedir?

- 0â€™a yakÄ±n â†’ Model daha Ã§ok saf residualâ€™a gÃ¼venir  
- 1â€™e yakÄ±n â†’ Model attention filtreli temsile daha Ã§ok gÃ¼venir  
- Ã–ÄŸrenilebilir ise â†’ EÄŸitim sÄ±rasÄ±nda en iyi dengeyi bulur  

Bu, attentionâ€™Ä±n aÅŸÄ±rÄ± baskÄ±n olup Ã¶ÄŸrenmeyi bozmasÄ±nÄ± engeller.

---

## ğŸ§© Attention TÃ¼rÃ¼

CBAM kullanÄ±lÄ±r:

| TÃ¼r | Ne SeÃ§er |
|-----|----------|
| Channel Attention | Hangi feature kanallarÄ± Ã¶nemli |
| Spatial Attention | Hangi uzamsal bÃ¶lgeler Ã¶nemli |

Ama bu attention artÄ±k **zorunlu deÄŸil**, Î» ile kontrollÃ¼.

---

## ğŸš€ Neden Bu Pattern GÃ¼Ã§lÃ¼?

âœ” Attentionâ€™Ä±n aÅŸÄ±rÄ± agresif etkisi kontrol edilir  
âœ” Residual stabilite korunur  
âœ” Model, hangi temsile gÃ¼veneceÄŸini Ã¶ÄŸrenir  
âœ” GÃ¼rÃ¼ltÃ¼lÃ¼ veri senaryolarÄ±nda daha dengeli Ã§alÄ±ÅŸÄ±r  

---

## ğŸ”š Ã–zet

Bu pattern:

**Residual Ã¶ÄŸrenme + Attention + Ã–ÄŸrenilebilir karÄ±ÅŸÄ±m katsayÄ±sÄ±**

yapÄ±sÄ±nÄ± kullanÄ±r ve klasik attention-residual fusionâ€™dan daha esnek ve stabil bir tasarÄ±m sunar.
