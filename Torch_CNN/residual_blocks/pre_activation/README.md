# Pre-Activation Residual YapÄ± (ResNet v2) â€” Tek YapÄ± Ã–zeti

## ğŸ¯ AmaÃ§
Ã‡ok derin aÄŸlarda **gradyan akÄ±ÅŸÄ±nÄ± en temiz hÃ¢le getirmek** ve eÄŸitimi daha stabil yapmak.

---

## ğŸ§  YapÄ±nÄ±n MantÄ±ÄŸÄ±

Bu tasarÄ±mda kritik fark ÅŸudur:

> **Normalizasyon ve aktivasyon, konvolÃ¼syondan Ã–NCE uygulanÄ±r.**

Blok ÅŸu akÄ±ÅŸÄ± izler:

**Norm â†’ Aktivasyon â†’ Conv â†’ Norm â†’ Aktivasyon â†’ Conv â†’ Skip ile toplama**

---

## ğŸ” Residual Toplama

- GiriÅŸ (x) doÄŸrudan kÄ±sa yoldan taÅŸÄ±nÄ±r.  
- Ana yol F(x) Ã¶ÄŸrenilen dÃ¶nÃ¼ÅŸÃ¼mdÃ¼r.  
- Ã‡Ä±kÄ±ÅŸ: **x + F(x)**  
- Toplama sonrasÄ± genellikle ek aktivasyon uygulanmaz.

---

## ğŸ“Œ Neden Bu SÄ±ra KullanÄ±lÄ±r?

âœ” Skip yolu tamamen lineer kalÄ±r  
âœ” Gradyan, aktivasyon bariyerine takÄ±lmaz  
âœ” Ã‡ok derin aÄŸlar daha kolay eÄŸitilir  
âœ” Degradation problemi azalÄ±r  

---

## âš™ï¸ Skip Yol DavranÄ±ÅŸÄ±

| Durum | Ä°ÅŸlem |
|------|-------|
| Boyutlar aynÄ± | GiriÅŸ direkt eklenir |
| Kanal/Ã§Ã¶zÃ¼nÃ¼rlÃ¼k farklÄ± | 1Ã—1 projeksiyonla eÅŸitlenir |

---

## ğŸ”¬ Post-Act ile FarkÄ±

| | Post-Activation | Pre-Activation |
|--|----------------|----------------|
| Aktivasyon yeri | Conv sonrasÄ± | Conv Ã¶ncesi |
| Toplama sonrasÄ± ReLU | Var | Yok (genelde) |
| Gradyan akÄ±ÅŸÄ± | Daha sÄ±nÄ±rlÄ± | Daha temiz |
| Derin aÄŸ uyumu | Orta | Ã‡ok yÃ¼ksek |

---

## ğŸ”š Ã–zet

Pre-activation residual yapÄ±:

**Aktivasyonu Ã¶ne alÄ±r, residual hattÄ± lineer tutar ve derin CNNâ€™lerde maksimum eÄŸitim stabilitesi saÄŸlar.**
