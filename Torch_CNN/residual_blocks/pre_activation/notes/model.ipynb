{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9fddb4",
   "metadata": {},
   "source": [
    "## 1) Residual blok kod mantığı (en temel)\n",
    "\n",
    "Residual blok şu fikri kodlar:\n",
    "\n",
    "**y=x+F(x)**\n",
    "\n",
    "* x: skip/identity yol (dokunmadan geçirir)\n",
    "\n",
    "* F(x): öğrenilebilir dönüşüm yolu (conv/BN/ReLU vs.)\n",
    "\n",
    "* Sonda: out = identity + residual\n",
    "\n",
    "En minimal PyTorch örneği:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc2fb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualToy(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ch,ch,kernel_size=3,padding=1,bias=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        residual = self.conv(x)\n",
    "        out = residual + identity\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d4093",
   "metadata": {},
   "source": [
    "* Bu kadar. Geri kalan her şey “F(x)’in içini” daha iyi tasarlamak.\n",
    "\n",
    "## 2) ResNet v1 (Post-Activation) — blok içi sıralama\n",
    "\n",
    "v1 basic block (en yaygın):\n",
    "\n",
    "#### Main path:\n",
    "\n",
    "* **Conv -> BN -> ReLU -> Conv -> BN**\n",
    "\n",
    "Sonra:\n",
    "\n",
    "* **out = out + identity**\n",
    "\n",
    "* **out = ReLU(out) ✅ (post-activation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c83251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlockV1(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.down is not None:\n",
    "            identity = self.down(x)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)   # <-- v1'in olayı: toplama SONRA ReLU\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796886cf",
   "metadata": {},
   "source": [
    "## 3) ResNet v2 (Pre-Activation) — blok içi sıralama\n",
    "\n",
    "v2 pre-act basic block:\n",
    "\n",
    "#### Main path:\n",
    "\n",
    "* **BN -> ReLU -> Conv -> BN -> ReLU -> Conv**\n",
    "\n",
    "#### Sonra:\n",
    "\n",
    "* **out = out + identity**\n",
    "\n",
    "blok sonunda ReLU yok ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActBasicBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.bn1  = nn.BatchNorm2d(in_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.bn2  = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(x))   # <-- v2'nin olayı: aktivasyon ÖNCE\n",
    "\n",
    "        identity = x\n",
    "        if self.down is not None:\n",
    "            identity = self.down(out)  # pratikte çoğu implementasyon preact üzerinden projeksiyon yapar\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "\n",
    "        out = out + identity           # <-- blok biter, ekstra ReLU yok\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15431fe5",
   "metadata": {},
   "source": [
    "## 4) Kodda en çok karıştırılan yer: downsample (shape eşleme)\n",
    "\n",
    "Toplama yapacağın için bu zorunlu:\n",
    "\n",
    "* x.shape == F(x).shape olmalı.\n",
    "\n",
    "Bu bozulur:\n",
    "\n",
    "* stride=2 ile küçültürsen\n",
    "\n",
    "* kanal sayısını değiştirirsen\n",
    "\n",
    "Çözüm:\n",
    "\n",
    "* skip yoluna 1x1 conv koyup aynı şekle getirirsin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119acc8",
   "metadata": {},
   "source": [
    "----\n",
    "-----\n",
    "------\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f83a94",
   "metadata": {},
   "source": [
    "# Genel olarak işleyişi sağlam tutmaya çalışalım.Burdaki amacımız bir veri seti üzerinden bu residual mimarisinin testlerini gerçekleştirmek.CIFAR-10 Veri Setini kullanacağız"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc8a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb8344",
   "metadata": {},
   "source": [
    "### 1) Dataset + DataLoader (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c950f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [01:23<00:00, 2.05MB/s] \n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "testset  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "testloader  = DataLoader(testset,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463ef2c",
   "metadata": {},
   "source": [
    "### 2) Yardımcı: Parametre ve Grad Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139bcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / total\n",
    "\n",
    "def grad_norm_l2(model):\n",
    "    total = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        g = p.grad.data.norm(2).item()\n",
    "        total += g*g\n",
    "    return total ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee231afc",
   "metadata": {},
   "source": [
    "### 3) Bloklar (Plain / ResNet v1 / ResNet v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c43cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainBlock(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,stride=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,out_ch,kernel_size=3,stride=stride,bias = False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch,out_ch,kernel_size=3,padding = 1,bias = False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a91648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlockV1(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv2d(in_ch,out_ch,1,stride=stride,bias=False),\n",
    "                nn.BatchNorm2d(out_ch))\n",
    "            \n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(x)\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f48a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.bn1  = nn.BatchNorm2d(in_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.bn2  = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "\n",
    "        self.down = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.down = nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.relu(self.bn1(x))\n",
    "        identity = x \n",
    "        if self.down is not None:\n",
    "            identity = self.down(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b7df9",
   "metadata": {},
   "source": [
    "### Aynı “Genel CNN Mimarisi” (3 model için aynı iskelet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624b9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, block, base=32, layers=(2,2,2), num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_ch = base\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, base, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.stage1 = self._make_stage(block, base,   layers[0], stride=1)  # 32x32\n",
    "        self.stage2 = self._make_stage(block, base*2, layers[1], stride=2)  # 16x16\n",
    "        self.stage3 = self._make_stage(block, base*4, layers[2], stride=2)  # 8x8\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.in_ch, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_stage(self, block, out_ch, n, stride):\n",
    "        layers = [block(self.in_ch, out_ch, stride=stride)]\n",
    "        self.in_ch = out_ch * getattr(block, \"expansion\", 1)\n",
    "        for _ in range(n-1):\n",
    "            layers.append(block(self.in_ch, out_ch, stride=1))\n",
    "            self.in_ch = out_ch * getattr(block, \"expansion\", 1)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2254b2",
   "metadata": {},
   "source": [
    "### Eğitim döngüsü (aynı ayarlar, aynı epoch)\n",
    "\n",
    "Her epoch sonunda:\n",
    "\n",
    "* train loss\n",
    "\n",
    "* test accuracy\n",
    "\n",
    "* grad-norm (ortalama) yazdırıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=5, lr=0.1):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[int(0.6*epochs), int(0.85*epochs)], gamma=0.1)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        total_loss = 0.0\n",
    "        total_gn = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x,y in trainloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            gn = grad_norm_l2(model)\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_gn += gn\n",
    "            n_batches += 1\n",
    "\n",
    "        sched.step()\n",
    "\n",
    "        train_loss = total_loss / max(1, n_batches)\n",
    "        avg_gn = total_gn / max(1, n_batches)\n",
    "        test_acc = accuracy(model, testloader)\n",
    "\n",
    "        print(f\"epoch {ep:02d} | loss {train_loss:.4f} | test_acc {test_acc*100:.2f}% | grad_norm {avg_gn:.2f} | time {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f52fa",
   "metadata": {},
   "source": [
    "### Üç modeli çalıştır (Plain vs v1 vs v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49790f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain params: 685994\n",
      "ResNet v1 params: 696618\n",
      "ResNet v2 params: 696042\n",
      "\n",
      "=== Plain CNN ===\n",
      "epoch 01 | loss 1.8301 | test_acc 37.53% | grad_norm 3.98 | time 199.4s\n",
      "\n",
      "=== ResNet v1 (post-act) ===\n",
      "epoch 01 | loss 1.7850 | test_acc 43.98% | grad_norm 3.30 | time 320.7s\n",
      "\n",
      "=== ResNet v2 (pre-act) ===\n",
      "epoch 01 | loss 1.9388 | test_acc 34.37% | grad_norm 2.87 | time 280.9s\n"
     ]
    }
   ],
   "source": [
    "plain = TinyNet(PlainBlock, base=32, layers=(2,2,2))\n",
    "v1    = TinyNet(BasicBlockV1, base=32, layers=(2,2,2))\n",
    "v2    = TinyNet(PreActBasicBlock, base=32, layers=(2,2,2))\n",
    "\n",
    "print(\"Plain params:\", count_params(plain))\n",
    "print(\"ResNet v1 params:\", count_params(v1))\n",
    "print(\"ResNet v2 params:\", count_params(v2))\n",
    "\n",
    "print(\"\\n=== Plain CNN ===\")\n",
    "train_model(plain, epochs=1, lr=0.1)\n",
    "\n",
    "print(\"\\n=== ResNet v1 (post-act) ===\")\n",
    "train_model(v1, epochs=1, lr=0.1)\n",
    "\n",
    "print(\"\\n=== ResNet v2 (pre-act) ===\")\n",
    "train_model(v2, epochs=1, lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8f9e3",
   "metadata": {},
   "source": [
    "## 1 Epoch Sonuçlarının Yorumu (Plain vs ResNet v1 vs ResNet v2)\n",
    "\n",
    "Bu deney yalnızca **1 epoch** çalıştırılmıştır.  \n",
    "Bu nedenle sonuçlar **erken davranış** (initial convergence) gösterir, nihai performans değildir.\n",
    "\n",
    "\n",
    "### Model Parametre Sayıları\n",
    "\n",
    "- **Plain CNN:** 685,994  \n",
    "- **ResNet v1 (Post-Activation):** 696,618  \n",
    "- **ResNet v2 (Pre-Activation):** 696,042  \n",
    "\n",
    "> Parametre sayıları neredeyse aynıdır.  \n",
    "> Performans farkları mimari tercihlerden kaynaklanmaktadır.\n",
    "\n",
    "\n",
    "### Epoch 1 Sonuçları\n",
    "\n",
    "| Model | Train Loss | Test Accuracy | Grad Norm |\n",
    "|------|-----------|---------------|-----------|\n",
    "| Plain CNN | 1.8301 | 37.53% | 3.98 |\n",
    "| ResNet v1 (Post-Act) | **1.7850** | **43.98%** | 3.30 |\n",
    "| ResNet v2 (Pre-Act) | 1.9388 | 34.37% | **2.87** |\n",
    "\n",
    "\n",
    "### Sonuçların Anlamı\n",
    "\n",
    "#### Plain CNN\n",
    "- En yüksek gradient norm.\n",
    "- Öğrenme agresif fakat kontrolsüz.\n",
    "- Derinlik arttıkça tıkanmaya yatkın.\n",
    "\n",
    "#### ResNet v1 (Post-Activation)\n",
    "- İlk epoch’ta en iyi accuracy.\n",
    "- Daha hızlı başlangıç (fast warm-up).\n",
    "- Gradient akışı residual sayesinde daha stabil.\n",
    "\n",
    "#### ResNet v2 (Pre-Activation)\n",
    "- En düşük gradient norm.\n",
    "- Daha temkinli ve stabil optimizasyon.\n",
    "- İlk epoch’ta yavaş başlaması **beklenen bir davranış**.\n",
    "\n",
    "### Kritik Nokta\n",
    "\n",
    "> **Pre-activation mimariler genellikle daha yavaş başlar,  \n",
    "> ancak epoch sayısı arttıkça daha stabil ve sürdürülebilir öğrenme sağlar.**\n",
    "\n",
    "Bu nedenle 1 epoch’ta v2’nin geride olması:\n",
    "- Mimari hatası değildir\n",
    "- Pre-activation tasarımının doğal sonucudur\n",
    "\n",
    "### Yorum\n",
    "\n",
    "- **Post-activation (v1):** Kısa vadede avantajlı  \n",
    "- **Pre-activation (v2):** Uzun vadede daha güvenilir\n",
    "\n",
    "> 1 epoch yalnızca “ilk ısınma”yı gösterir.  \n",
    "> Mimari karşılaştırması için **en az 5–10 epoch** gereklidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503b0e8",
   "metadata": {},
   "source": [
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc5def",
   "metadata": {},
   "source": [
    "## Eğitmeden Mimari Karşılaştırma: Ne Ölçüyoruz, Ne Anlama Geliyor?\n",
    "\n",
    "Bu bölümde **accuracy yok**. Eğitim yoksa accuracy zaten anlamsızdır.\n",
    "\n",
    "Bunun yerine, mimarinin “sağlıklı” olup olmadığını şu 4 şeyle ölçeriz:\n",
    "\n",
    "### (A) Parametre / FLOPs (kapasite ve maliyet)\n",
    "- Parametre sayısı yakınsa, fark “kapasite”den değil, **akış (flow)** tasarımından gelir.\n",
    "\n",
    "### (B) Aktivasyon istatistikleri (forward sağlığı)\n",
    "Her modelin **rastgele girişte** ürettiği ara/son feature’lara bakarız:\n",
    "- `mean` çok uçsa → kayma var\n",
    "- `std` çok küçükse → sinyal sönüyor (collapse)\n",
    "- `std` çok büyükse → sinyal şişiyor\n",
    "- `zero_frac` (ReLU sonrası sıfır oranı) çok yüksekse → aşırı kesme/sparsity\n",
    "\n",
    "### (C) Gradient akışı (backward sağlığı)\n",
    "Sahte bir loss ile backprop yaparız ve şu ölçülere bakarız:\n",
    "- `grad_norm_first` (en erken katman gradi)\n",
    "- `grad_norm_last`  (en son katman gradi)\n",
    "- `ratio = grad_norm_first / grad_norm_last`\n",
    "  - Çok küçükse → gradient başa ulaşamıyor (vanishing)\n",
    "  - Çok büyükse → dengesiz akış (patlama/instability)\n",
    "\n",
    "### (D) Input duyarlılığı (basit stabilite sinyali)\n",
    "Random girişte küçük bir perturbation ekleyip çıktı ne kadar değişiyor ölçeriz:\n",
    "- Çok yüksekse → model “sert” (unstable olabilir)\n",
    "- Daha makulse → stabil dönüşüm\n",
    "\n",
    "Beklenti (genel):\n",
    "- Plain CNN: gradient daha dengesiz, aktivasyon dağılımı daha oynak\n",
    "- ResNet v1: daha iyi akış\n",
    "- ResNet v2 (pre-act): genelde **en temiz gradient highway** (özellikle derinlik arttıkça)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c87b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>layers_tracked</th>\n",
       "      <th>mean(mean)</th>\n",
       "      <th>mean(std)</th>\n",
       "      <th>min(std)</th>\n",
       "      <th>max(std)</th>\n",
       "      <th>relu_zero_frac_mean</th>\n",
       "      <th>relu_zero_frac_max</th>\n",
       "      <th>loss(dummy)</th>\n",
       "      <th>grad_norm_first_conv</th>\n",
       "      <th>grad_norm_last_linear</th>\n",
       "      <th>grad_norm_global</th>\n",
       "      <th>first/last_ratio</th>\n",
       "      <th>eps</th>\n",
       "      <th>logit_diff_L2_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plain</td>\n",
       "      <td>685994</td>\n",
       "      <td>39</td>\n",
       "      <td>0.127289</td>\n",
       "      <td>0.648104</td>\n",
       "      <td>0.342056</td>\n",
       "      <td>1.186445</td>\n",
       "      <td>0.490220</td>\n",
       "      <td>0.566022</td>\n",
       "      <td>2.650428</td>\n",
       "      <td>3.763592</td>\n",
       "      <td>0.906656</td>\n",
       "      <td>19.266334</td>\n",
       "      <td>4.151070</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ResNet_v1_post</td>\n",
       "      <td>696618</td>\n",
       "      <td>43</td>\n",
       "      <td>0.187074</td>\n",
       "      <td>0.804139</td>\n",
       "      <td>0.368699</td>\n",
       "      <td>1.440660</td>\n",
       "      <td>0.447537</td>\n",
       "      <td>0.525585</td>\n",
       "      <td>2.357425</td>\n",
       "      <td>1.399056</td>\n",
       "      <td>1.970564</td>\n",
       "      <td>7.185342</td>\n",
       "      <td>0.709977</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ResNet_v2_pre</td>\n",
       "      <td>696042</td>\n",
       "      <td>41</td>\n",
       "      <td>0.122712</td>\n",
       "      <td>0.730211</td>\n",
       "      <td>0.406472</td>\n",
       "      <td>1.175450</td>\n",
       "      <td>0.520208</td>\n",
       "      <td>0.685784</td>\n",
       "      <td>2.539158</td>\n",
       "      <td>2.945943</td>\n",
       "      <td>0.431766</td>\n",
       "      <td>12.798392</td>\n",
       "      <td>6.823003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  params  layers_tracked  mean(mean)  mean(std)  min(std)  \\\n",
       "0           Plain  685994              39    0.127289   0.648104  0.342056   \n",
       "1  ResNet_v1_post  696618              43    0.187074   0.804139  0.368699   \n",
       "2   ResNet_v2_pre  696042              41    0.122712   0.730211  0.406472   \n",
       "\n",
       "   max(std)  relu_zero_frac_mean  relu_zero_frac_max  loss(dummy)  \\\n",
       "0  1.186445             0.490220            0.566022     2.650428   \n",
       "1  1.440660             0.447537            0.525585     2.357425   \n",
       "2  1.175450             0.520208            0.685784     2.539158   \n",
       "\n",
       "   grad_norm_first_conv  grad_norm_last_linear  grad_norm_global  \\\n",
       "0              3.763592               0.906656         19.266334   \n",
       "1              1.399056               1.970564          7.185342   \n",
       "2              2.945943               0.431766         12.798392   \n",
       "\n",
       "   first/last_ratio    eps  logit_diff_L2_mean  \n",
       "0          4.151070  0.001            0.002972  \n",
       "1          0.709977  0.001            0.001979  \n",
       "2          6.823003  0.001            0.002209  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def _layer_name(module):\n",
    "    return module.__class__.__name__\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_activation_stats(model, input_shape=(32,3,32,32), max_layers=999):\n",
    "    model.eval()\n",
    "    stats = []\n",
    "    hooks = []\n",
    "    seen = {\"n\": 0}\n",
    "\n",
    "    def hook_fn(name):\n",
    "        def _fn(m, inp, out):\n",
    "            if seen[\"n\"] >= max_layers:\n",
    "                return\n",
    "            # out bazen tuple olabilir\n",
    "            t = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            if not torch.is_tensor(t):\n",
    "                return\n",
    "            t = t.detach()\n",
    "            # only 4D (N,C,H,W) istatistikler daha anlamlı\n",
    "            if t.dim() < 2:\n",
    "                return\n",
    "            seen[\"n\"] += 1\n",
    "\n",
    "            # sıfır oranı (özellikle ReLU sonrası anlamlı)\n",
    "            zero_frac = float((t == 0).float().mean().cpu())\n",
    "\n",
    "            stats.append({\n",
    "                \"layer\": name,\n",
    "                \"type\": _layer_name(m),\n",
    "                \"shape\": tuple(t.shape),\n",
    "                \"mean\": float(t.mean().cpu()),\n",
    "                \"std\":  float(t.std(unbiased=False).cpu()),\n",
    "                \"min\":  float(t.min().cpu()),\n",
    "                \"max\":  float(t.max().cpu()),\n",
    "                \"zero_frac\": zero_frac,\n",
    "            })\n",
    "        return _fn\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.BatchNorm2d, nn.ReLU, nn.SiLU, nn.GroupNorm)):\n",
    "            hooks.append(m.register_forward_hook(hook_fn(name)))\n",
    "\n",
    "    x = torch.randn(*input_shape, device=device)\n",
    "    _ = model(x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "def grad_flow_stats(model, input_shape=(32,3,32,32), num_classes=10):\n",
    "    model.train()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    x = torch.randn(*input_shape, device=device)\n",
    "    y = torch.randint(0, num_classes, (input_shape[0],), device=device)\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "\n",
    "    first_conv = None\n",
    "    last_linear = None\n",
    "    for m in model.modules():\n",
    "        if first_conv is None and isinstance(m, nn.Conv2d):\n",
    "            first_conv = m\n",
    "        if isinstance(m, nn.Linear):\n",
    "            last_linear = m\n",
    "\n",
    "    def norm_param(p):\n",
    "        if p is None or p.grad is None:\n",
    "            return float(\"nan\")\n",
    "        return float(p.grad.data.norm(2).item())\n",
    "\n",
    "    first_gn = norm_param(first_conv.weight if first_conv is not None else None)\n",
    "    last_gn  = norm_param(last_linear.weight if last_linear is not None else None)\n",
    "\n",
    "    total = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        g = p.grad.data.norm(2).item()\n",
    "        total += g*g\n",
    "    global_gn = total ** 0.5\n",
    "\n",
    "    ratio = first_gn / (last_gn + 1e-12)\n",
    "\n",
    "    return {\n",
    "        \"loss(dummy)\": float(loss.item()),\n",
    "        \"grad_norm_first_conv\": first_gn,\n",
    "        \"grad_norm_last_linear\": last_gn,\n",
    "        \"grad_norm_global\": float(global_gn),\n",
    "        \"first/last_ratio\": float(ratio),\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def input_sensitivity(model, input_shape=(32,3,32,32), eps=1e-3):\n",
    "    model.eval()\n",
    "    x = torch.randn(*input_shape, device=device)\n",
    "    noise = torch.randn_like(x) * eps\n",
    "\n",
    "    out1 = model(x)\n",
    "    out2 = model(x + noise)\n",
    "\n",
    "    # logit farkının ortalama L2 normu\n",
    "    diff = (out2 - out1).flatten(1)\n",
    "    sens = diff.norm(p=2, dim=1).mean().item()\n",
    "    return {\"eps\": eps, \"logit_diff_L2_mean\": float(sens)}\n",
    "\n",
    "def summarize_activation_df(df: pd.DataFrame):\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    relu_df = df[df[\"type\"].isin([\"ReLU\", \"SiLU\"])]\n",
    "    return {\n",
    "        \"layers_tracked\": int(len(df)),\n",
    "        \"mean(mean)\": float(df[\"mean\"].mean()),\n",
    "        \"mean(std)\": float(df[\"std\"].mean()),\n",
    "        \"min(std)\": float(df[\"std\"].min()),\n",
    "        \"max(std)\": float(df[\"std\"].max()),\n",
    "        \"relu_zero_frac_mean\": float(relu_df[\"zero_frac\"].mean()) if len(relu_df) else float(\"nan\"),\n",
    "        \"relu_zero_frac_max\": float(relu_df[\"zero_frac\"].max()) if len(relu_df) else float(\"nan\"),\n",
    "    }\n",
    "def analyze_model(name, model):\n",
    "    model = model.to(device)\n",
    "    p = count_params(model)\n",
    "\n",
    "    act_df = forward_activation_stats(model)\n",
    "    act_summary = summarize_activation_df(act_df)\n",
    "\n",
    "    g = grad_flow_stats(model)\n",
    "    s = input_sensitivity(model, eps=1e-3)\n",
    "\n",
    "    row = {\"model\": name, \"params\": p, **act_summary, **g, **s}\n",
    "    return row, act_df\n",
    "results = []\n",
    "details = {}\n",
    "\n",
    "for name, m in [(\"Plain\", plain), (\"ResNet_v1_post\", v1), (\"ResNet_v2_pre\", v2)]:\n",
    "    row, act_df = analyze_model(name, m)\n",
    "    results.append(row)\n",
    "    details[name] = act_df\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4181e8",
   "metadata": {},
   "source": [
    "## Eğitimsiz Mimari Karşılaştırma — Sayıların Anlamı ve Güvenli Aralıklar\n",
    "\n",
    "Bu analiz **eğitim yapılmadan**, rastgele başlatılmış ağırlıklarla yapılmıştır.  \n",
    "Amaç: Performans değil, **mimari sağlığı** ölçmektir.\n",
    "\n",
    "\n",
    "## 1) Aktivasyon İstatistikleri (Forward Sağlığı)\n",
    "\n",
    "### `mean(std)` — Ortalama aktivasyon enerjisi\n",
    "- **Güvenli aralık:** `0.6 – 0.9`\n",
    "- Çok küçük → sinyal sönüyor  \n",
    "- Çok büyük → sinyal şişiyor\n",
    "\n",
    "**Sonuçlar:**\n",
    "- Plain: 0.65 → sınırda\n",
    "- ResNet v1: 0.80 → agresif ama iyi\n",
    "- ResNet v2: 0.73 → dengeli\n",
    "\n",
    "> Yorum: v2 kontrollü, v1 hızlı ısınan yapı.\n",
    "\n",
    "\n",
    "### `min(std)` / `max(std)` — Katmanlar arası stabilite\n",
    "- **min(std) güvenli:** `> 0.3`\n",
    "- **max(std) güvenli:** `< 1.3`\n",
    "\n",
    "**Sonuçlar:**\n",
    "- v2: en dar aralık → **en stabil sinyal**\n",
    "- v1: max(std) yüksek → bazı katmanlar şişiyor\n",
    "- Plain: min(std) düşük → sönme riski\n",
    "\n",
    "> Dar aralık = derinlikte daha güvenli mimari\n",
    "\n",
    "### `relu_zero_frac_mean` — ReLU kesme oranı\n",
    "- **Güvenli ortalama:** `0.4 – 0.6`\n",
    "- `> 0.7` → aşırı bilgi kaybı\n",
    "- `< 0.3` → yetersiz non-linearity\n",
    "\n",
    "**Sonuçlar:**\n",
    "- v2: 0.52 → normal\n",
    "- v1: 0.45 → daha az kesme\n",
    "- Plain: 0.49 → orta\n",
    "\n",
    "> v2’de kesme **main path’te**, skip yolu korunuyor (pre-act avantajı).\n",
    "\n",
    "## 2) Gradient Akışı (Backward Sağlığı)\n",
    "\n",
    "### `grad_norm_global`\n",
    "- **Güvenli aralık:** `5 – 15`\n",
    "- `< 3` → öğrenme zayıf\n",
    "- `> 20` → patlama riski\n",
    "\n",
    "**Sonuçlar:**\n",
    "- Plain: 19.27 → riskli\n",
    "- v1: 7.18 → çok stabil\n",
    "- v2: 12.80 → güçlü ama kontrollü\n",
    "\n",
    "### `first/last_ratio` — Gradientin başa ulaşma gücü\n",
    "- **İdeal:** `~1`\n",
    "- **Kabul edilebilir:** `0.5 – 5`\n",
    "- `> 6` → çok güçlü akış (dikkat)\n",
    "- `< 0.3` → vanishing\n",
    "\n",
    "**Sonuçlar:**\n",
    "- v1: 0.71 → dengeli\n",
    "- v2: 6.82 → skip yolu çok güçlü (pre-act imzası)\n",
    "- Plain: 4.15 → kontrolsüz güç\n",
    "\n",
    "> v2’nin yüksek oranı, derinlikte avantaj sağlar.\n",
    "\n",
    "\n",
    "## 3) Giriş Duyarlılığı (Stabilite Sinyali)\n",
    "\n",
    "### `logit_diff_L2_mean` (küçük daha iyi)\n",
    "- **Güvenli:** `< 0.003`\n",
    "- `< 0.002` → çok stabil\n",
    "- `> 0.004` → aşırı hassas\n",
    "\n",
    "**Sonuçlar:**\n",
    "- v1: 0.00198 → en stabil\n",
    "- v2: 0.00221 → stabil\n",
    "- Plain: 0.00297 → sınırda\n",
    "\n",
    "> v1 bastırıcı, v2 dengeli, plain oynak.\n",
    "\n",
    "\n",
    "## 4) Genel Mimari Kararı (Bu Deneyden Çıkan Net Kural)\n",
    "\n",
    "- **Plain CNN:**  \n",
    "  - Kontrolsüz, derinlikte riskli  \n",
    "- **ResNet v1 (Post-Activation):**  \n",
    "  - Erken epoch’ta hızlı  \n",
    "  - Çıkışa yakın veya sığ ağlarda iyi  \n",
    "- **ResNet v2 (Pre-Activation):**  \n",
    "  - En temiz skip yolu  \n",
    "  - En stabil sinyal aralığı  \n",
    "  - Derinlik ve attention için en güvenilir seçim  \n",
    "-\n",
    "\n",
    "## 5) Pratik Kullanım Kuralı (Ezber)\n",
    "\n",
    "> **Ara bloklar + attention + derin ağ → PRE-ACTIVATION**  \n",
    "> **Çıkışa yakın, bilinçli bastırma → POST-ACTIVATION**\n",
    "\n",
    "Bu deney sonuçları, teorik ResNet v1/v2 farklarıyla birebir örtüşmektedir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e48adf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
