{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c736c4",
   "metadata": {},
   "source": [
    "# Pre-Activation + Attention Entegrasyonu (Adım Adım, Tane Tane)\n",
    "\n",
    "Bu bölümde hedefimiz şudur:\n",
    "\n",
    "1. **Attention bloğu içindeki residual’ı kapatacağız** → attention artık `F(x)` üretecek.  \n",
    "2. **Residual’ı dışarıda (CNN blok seviyesinde) uygulayacağız** → “double residual” olmayacak.  \n",
    "3. **Pre-activation (ResNet v2 mantığı)** ile attention’ı bir CNN bloğuna bağlayacağız.\n",
    "\n",
    "> Kural:  \n",
    "> **Attention karar verir (F(x)), residual taşır (skip).**  \n",
    "> Pre-activation, skip yolunu “ham” tutar.\n",
    "\n",
    "---\n",
    "\n",
    "## Adım 0 — Mevcut durumun teşhisi\n",
    "\n",
    "Bizim `CBAMChannelPlusCoord` modülünde iki kullanım modu var:\n",
    "\n",
    "- `residual=True`  → modül kendi içinde şu tarz karışım yapar:  \n",
    "  `out = x + alpha*(y - x)`  \n",
    "- `residual=False` → modül **saf attention** gibi davranır:  \n",
    "  `out = y` (skip yok)  \n",
    "  yani **F(x)** üretir.\n",
    "\n",
    "Bizim entegrasyon hedefimiz:\n",
    "- `CBAMChannelPlusCoord(residual=False)` kullanıp **F(x)** almak\n",
    "- Residual’ı CNN blokta yapmak.\n",
    "\n",
    "---\n",
    "\n",
    "## Adım 1 — Attention bloğu “F(x)” olacak şekilde ayarlamak\n",
    "\n",
    "### Yapılacak\n",
    "`CBAMChannelPlusCoord` çağrılırken:\n",
    "- `residual=False`\n",
    "- (istersek) `return_maps=False`, `monitor=False` gibi debuglar kapalı olabilir\n",
    "\n",
    "### Amaç\n",
    "Attention bloğu artık **skip karıştırmayacak**, sadece input’u refine edecek:\n",
    "\n",
    "\\[\n",
    "F(x) = \\text{Attention}(x)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Adım 2 — Pre-Activation CNN bloğu yazalım (ResNet v2 mantığı)\n",
    "\n",
    "Pre-activation blok prensibi:\n",
    "\n",
    "1) **Önce** Normalize + Aktivasyon:  \n",
    "\\[\n",
    "x_{main} = \\text{Act}(\\text{Norm}(x))\n",
    "\\]\n",
    "\n",
    "2) Main path bu `x_main` ile çalışır:\n",
    "- Conv\n",
    "- Conv\n",
    "- Attention (**residual kapalı**)\n",
    "\n",
    "3) Skip path: **ham x** (dokunma)\n",
    "\n",
    "4) Topla:  \n",
    "\\[\n",
    "out = skip + F(x_{main})\n",
    "\\]\n",
    "5) Blok sonunda ReLU yok (v2 ruhu)\n",
    "\n",
    "---\n",
    "\n",
    "## Adım 3 — Downsample (stride/channel değişimi) kuralı\n",
    "\n",
    "Eğer:\n",
    "- stride != 1  (çözünürlük değişiyorsa)\n",
    "- veya kanal değişiyorsa\n",
    "\n",
    "Skip yolunu projekte etmemiz gerekir:\n",
    "- 1x1 Conv (stride ile)\n",
    "- (isteğe bağlı) norm\n",
    "\n",
    "Bu projeksiyon **skip’i boyut eşleştirmek içindir**.\n",
    "\n",
    "---\n",
    "\n",
    "## Adım 4 — PreAct + Attention’ı tek bir CNN bloğunda birleştirelim\n",
    "\n",
    "Aşağıdaki bileşenler olacak:\n",
    "\n",
    "- **PreAct kısmı:** `Norm + Act`\n",
    "- **Conv1:** 3x3\n",
    "- **Conv2:** 3x3\n",
    "- **Attention:** `CBAMChannelPlusCoord(residual=False)` (F(x))\n",
    "- **Skip:** Identity veya 1x1 projection\n",
    "- **Toplama:** `out = skip + y`\n",
    "\n",
    "> Sonraki adımda bu bloğu CNN içine koyacağız ve test edeceğiz.\n",
    "\n",
    "---\n",
    "\n",
    "## Adım 5 — Kontrol listesi (Doğru entegrasyonu yaptık mı?)\n",
    "\n",
    "Aşağıdaki maddeler doğruysa entegrasyonu doğru yaptık demektir:\n",
    "\n",
    "- [ ] Attention içinde `x + ...` yok (residual kapalı)\n",
    "- [ ] `Norm+Act` attention’dan önce (pre-act)\n",
    "- [ ] Skip ham `x` (pre-act’in ana fikri)\n",
    "- [ ] Residual toplamayı **tek yerde** yapıyorsun (block seviyesinde)\n",
    "- [ ] Stride/channel değişirse skip projeksiyonu var\n",
    "\n",
    "---\n",
    "\n",
    "# Sonraki Hücre: Pre-Activation + Attention CNN Bloğu (Kod)\n",
    "\n",
    "Bir sonraki hücrede şunu yazacağız:\n",
    "\n",
    "- `PreActAttnBlock` sınıfı\n",
    "- içine `CBAMChannelPlusCoord(residual=False)` entegre\n",
    "- stride/channel durumunda projection\n",
    "\n",
    "Sonra bunu mini bir `TinyNet` içine koyup forward/shape test yapacağız.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3357483",
   "metadata": {},
   "source": [
    "-----\n",
    "------\n",
    "-------\n",
    "-------\n",
    "-------\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0c3c2",
   "metadata": {},
   "source": [
    "## Hücre 0 -- CA + CBAM Attention Bloğu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce15ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def softplus_inverse(y: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return torch.log(torch.clamp(torch.exp(y) - 1.0, min=eps))\n",
    "\n",
    "def _get_gate(gate: str):\n",
    "    g = gate.lower()\n",
    "    if g == \"sigmoid\":\n",
    "        return torch.sigmoid\n",
    "    if g == \"hardsigmoid\":\n",
    "        return F.hardsigmoid\n",
    "    raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "def _get_act(act: str):\n",
    "    a = act.lower()\n",
    "    if a == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    if a == \"silu\":\n",
    "        return nn.SiLU(inplace=True)\n",
    "    raise ValueError(\"act 'relu' veya 'silu' olmalı.\")\n",
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",\n",
    "        gate: str = \"sigmoid\",\n",
    "        temperature: float = 0.9,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        act: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "        fusion_router_hidden: int = 16,\n",
    "        return_fusion_weights: bool = False,\n",
    "        t_min: float = 0.5,\n",
    "        t_max: float = 3.0,\n",
    "        router_temperature: float = 1.5,\n",
    "        beta_ca: float = 0.35,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalı.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalı.\")\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalı.\")\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if fusion == \"softmax\" and fusion_router_hidden < 1:\n",
    "            raise ValueError(\"fusion_router_hidden >= 1 olmalı.\")\n",
    "        if t_min <= 0 or t_max <= 0 or t_min > t_max:\n",
    "            raise ValueError(\"T clamp aralığı hatalı.\")\n",
    "        if router_temperature <= 0:\n",
    "            raise ValueError(\"router_temperature pozitif olmalı.\")\n",
    "        if beta_ca < 0:\n",
    "            raise ValueError(\"beta_ca >= 0 olmalı.\")\n",
    "\n",
    "        self.eps = float(eps)\n",
    "        self.fusion = fusion\n",
    "        self.return_fusion_weights = bool(return_fusion_weights)\n",
    "        self.gate_fn = _get_gate(gate)\n",
    "\n",
    "        self.t_min = float(t_min)\n",
    "        self.t_max = float(t_max)\n",
    "        self.Tr = float(router_temperature)\n",
    "        self.beta_ca = float(beta_ca)\n",
    "\n",
    "        hidden = max(int(min_hidden), int(channels) // int(reduction))\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=bias)\n",
    "        self.act = _get_act(act) # silu , relu\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        if self.fusion == \"softmax\":\n",
    "            self.fusion_router = nn.Sequential(\n",
    "                nn.Conv2d(2 * channels, fusion_router_hidden, kernel_size=1, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(fusion_router_hidden, 2, kernel_size=1, bias=True),\n",
    "            )\n",
    "            last = self.fusion_router[-1] # [-1] = “en sondaki eleman\" \n",
    "            nn.init.zeros_(last.weight)\n",
    "            nn.init.zeros_(last.bias)\n",
    "        else:\n",
    "            self.fusion_router = None\n",
    "\n",
    "        self.learnable_temperature = bool(learnable_temperature)\n",
    "        if self.learnable_temperature:\n",
    "            t0 = float(temperature)\n",
    "            lo = self.t_min + self.eps\n",
    "            hi = self.t_max - self.eps\n",
    "            if lo >= hi:\n",
    "                lo = self.t_min\n",
    "                hi = self.t_max\n",
    "            t0 = min(max(t0, lo), hi)\n",
    "            t_inv = softplus_inverse(torch.tensor(t0), eps=self.eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def get_T(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            T = F.softplus(self.t_raw) + self.eps\n",
    "        else:\n",
    "            T = self.T\n",
    "        T = T.to(device=x.device, dtype=x.dtype)\n",
    "        return T.clamp(self.t_min, self.t_max)\n",
    "\n",
    "    def mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_s = self.avg_pool(x)\n",
    "        max_s = self.max_pool(x)\n",
    "\n",
    "        a = self.mlp(avg_s)\n",
    "        m = self.mlp(max_s)\n",
    "\n",
    "        fusion_w = None\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            s_cat = torch.cat([avg_s, max_s], dim=1)\n",
    "            logits = self.fusion_router(s_cat).flatten(1)\n",
    "            fusion_w = torch.softmax(logits / self.Tr, dim=1)\n",
    "            # Softmax: “bu sample’da avg mi daha önemli max mı?” sorusuna öğrenilen ağırlık\n",
    "            # Tr :: :: Softmax’ın “keskinliğini” ayarlar\n",
    "            w0 = fusion_w[:, 0].view(-1, 1, 1, 1)\n",
    "            w1 = fusion_w[:, 1].view(-1, 1, 1, 1)\n",
    "            z = w0 * a + w1 * m\n",
    "\n",
    "        T = self.get_T(x)\n",
    "        ca = self.gate_fn(z / T)\n",
    "        # T gate’in agresifliğini kontrol eden “ısı” parametresi.\n",
    "\n",
    "        scale_ca = 1.0 + self.beta_ca * (ca - 1.0)\n",
    "        y = x * scale_ca\n",
    "\n",
    "        if self.return_fusion_weights:\n",
    "            return y, ca, fusion_w\n",
    "        return y, ca\n",
    "\n",
    "\n",
    "class HSwish(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0\n",
    "\n",
    "def make_norm(norm: str, ch: int):\n",
    "    norm = norm.lower()\n",
    "    if norm == \"bn\":\n",
    "        return nn.BatchNorm2d(ch)\n",
    "    if norm == \"gn\":\n",
    "        g = min(32, ch)\n",
    "        while ch % g != 0 and g > 2:\n",
    "            g //= 2\n",
    "        if ch % g != 0:\n",
    "            g = 2 if (ch % 2 == 0) else 1\n",
    "        return nn.GroupNorm(g, ch)\n",
    "    if norm == \"none\":\n",
    "        return nn.Identity()\n",
    "    raise ValueError(\"norm 'none', 'bn', 'gn' dışında olamaz.\")\n",
    "\n",
    "class CoordinateAttPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        reduction: int = 32,\n",
    "        min_mid_channels: int = 8,\n",
    "        act: str = \"hswish\",\n",
    "        init_alpha: float = 0.7,\n",
    "        learnable_alpha: bool = True,\n",
    "        beta: float = 0.35,\n",
    "        dilation: int = 2,\n",
    "        norm: str = \"gn\",\n",
    "        use_spatial_gate: bool = False,\n",
    "        spatial_gate_beta: float = 0.35,\n",
    "        scale_min: float = 0.6,\n",
    "        scale_max: float = 1.6,\n",
    "        head_init_std: float = 0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if in_channels < 1:\n",
    "            raise ValueError(\"in_channels >= 1 olmalı.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalı.\")\n",
    "        if dilation < 1:\n",
    "            raise ValueError(\"dilation >= 1 olmalı.\")\n",
    "        if scale_min <= 0 or scale_max <= 0 or scale_min > scale_max:\n",
    "            raise ValueError(\"scale clamp aralığı hatalı.\")\n",
    "        if head_init_std <= 0:\n",
    "            raise ValueError(\"head_init_std pozitif olmalı.\")\n",
    "        if beta < 0:\n",
    "            raise ValueError(\"beta >= 0 olmalı.\")\n",
    "        if spatial_gate_beta < 0:\n",
    "            raise ValueError(\"spatial_gate_beta >= 0 olmalı.\")\n",
    "\n",
    "        self.beta = float(beta)\n",
    "        self.scale_min = float(scale_min)\n",
    "        self.scale_max = float(scale_max)\n",
    "\n",
    "        mid_floor = max(8, min(32, int(in_channels) // 4))\n",
    "        mid = max(int(min_mid_channels), int(in_channels) // int(reduction))\n",
    "        mid = max(mid, int(mid_floor))\n",
    "\n",
    "        act_l = act.lower()\n",
    "        if act_l == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        elif act_l == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act_l == \"silu\":\n",
    "            self.act = nn.SiLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act 'hswish', 'relu', 'silu' olmalı.\")\n",
    "\n",
    "        self.shared_bottleneck_proj = nn.Conv2d(in_channels, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_norm = make_norm(norm, mid)\n",
    "        self.shared_bottleneck_refine = nn.Conv2d(mid, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_refine_norm = make_norm(norm, mid)\n",
    "\n",
    "        self.h_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(3, 1), padding=(1, 0),\n",
    "            groups=in_channels, bias=False\n",
    "        )\n",
    "        self.w_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(1, 3), padding=(0, 1),\n",
    "            groups=in_channels, bias=False\n",
    "        )\n",
    "\n",
    "        d = int(dilation)\n",
    "        self.h_dilated_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(3, 1), padding=(d, 0),\n",
    "            dilation=(d, 1), groups=in_channels, bias=False\n",
    "        )\n",
    "        self.w_dilated_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(1, 3), padding=(0, d),\n",
    "            dilation=(1, d), groups=in_channels, bias=False\n",
    "        )\n",
    "\n",
    "        self.h_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        self.w_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self.h_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        self.w_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "\n",
    "        nn.init.normal_(self.h_attention_head.weight, mean=0.0, std=float(head_init_std))\n",
    "        nn.init.normal_(self.w_attention_head.weight, mean=0.0, std=float(head_init_std))\n",
    "        if self.h_attention_head.bias is not None:\n",
    "            nn.init.zeros_(self.h_attention_head.bias)\n",
    "        if self.w_attention_head.bias is not None:\n",
    "            nn.init.zeros_(self.w_attention_head.bias)\n",
    "\n",
    "        eps = 1e-6\n",
    "        a0 = float(init_alpha)\n",
    "        a0 = min(max(a0, eps), 1.0 - eps)\n",
    "        raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "        # başlangıçta alpha=init_alpha olacak şekilde raw parametreyi ayarlamak”.\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha_h_raw = nn.Parameter(raw0.clone())\n",
    "            self.alpha_w_raw = nn.Parameter(raw0.clone())\n",
    "        else:\n",
    "            self.register_buffer(\"alpha_h_raw\", raw0.clone())\n",
    "            self.register_buffer(\"alpha_w_raw\", raw0.clone())\n",
    "\n",
    "        self.use_spatial_gate = bool(use_spatial_gate)\n",
    "        self.spatial_gate_beta = float(spatial_gate_beta)\n",
    "        if self.use_spatial_gate:\n",
    "            self.spatial_gate_dw = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False)\n",
    "            self.spatial_gate_pw = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self._last_ah = None\n",
    "        self._last_aw = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        _, _, H, W = x.shape\n",
    "\n",
    "        h_profile = 0.5 * (x.mean(dim=3, keepdim=True) + x.amax(dim=3, keepdim=True))\n",
    "        w_profile = 0.5 * (x.mean(dim=2, keepdim=True) + x.amax(dim=2, keepdim=True))\n",
    "\n",
    "        h_ms = self.h_channel_mixer(self.h_local_dw(h_profile) + self.h_dilated_dw(h_profile))\n",
    "        w_ms = self.w_channel_mixer(self.w_local_dw(w_profile) + self.w_dilated_dw(w_profile))\n",
    "        w_ms = w_ms.permute(0, 1, 3, 2)\n",
    "\n",
    "        hw = torch.cat([h_ms, w_ms], dim=2)\n",
    "\n",
    "        mid = self.act(self.shared_bottleneck_norm(self.shared_bottleneck_proj(hw)))\n",
    "        mid = self.act(self.shared_bottleneck_refine_norm(self.shared_bottleneck_refine(mid)))\n",
    "\n",
    "        mid_h, mid_w = torch.split(mid, [H, W], dim=2)\n",
    "        mid_w = mid_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        attn_h = F.hardsigmoid(self.h_attention_head(mid_h), inplace=False)\n",
    "        attn_w = F.hardsigmoid(self.w_attention_head(mid_w), inplace=False)\n",
    "\n",
    "        self._last_ah = attn_h.detach()\n",
    "        self._last_aw = attn_w.detach()\n",
    "\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw).to(device=x.device, dtype=x.dtype)\n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        scale_h = (1.0 - alpha_h) + alpha_h * attn_h\n",
    "        scale_w = (1.0 - alpha_w) + alpha_w * attn_w\n",
    "\n",
    "        scale = scale_h * scale_w\n",
    "        scale = 1.0 + self.beta * (scale - 1.0)\n",
    "        scale = scale.clamp(self.scale_min, self.scale_max)\n",
    "        out = x * scale\n",
    "\n",
    "        if self.use_spatial_gate:\n",
    "            sg = self.spatial_gate_pw(self.spatial_gate_dw(x))\n",
    "            sg = F.hardsigmoid(sg, inplace=False)\n",
    "            sg = 1.0 + self.spatial_gate_beta * (sg - 1.0)\n",
    "            out = out * sg\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def last_mask_stats(self):\n",
    "        if (self._last_ah is None) or (self._last_aw is None):\n",
    "            return None\n",
    "        ah = self._last_ah\n",
    "        aw = self._last_aw\n",
    "        return {\n",
    "            \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "            \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "        }\n",
    "\n",
    "\n",
    "class CBAMChannelPlusCoord(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        ca_reduction: int = 16,\n",
    "        ca_min_hidden: int = 4,\n",
    "        ca_fusion: str = \"softmax\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        ca_temperature: float = 0.9,\n",
    "        ca_act: str = \"relu\",\n",
    "        ca_fusion_router_hidden: int = 16,\n",
    "        learnable_temperature: bool = False,\n",
    "        ca_t_min: float = 0.5,\n",
    "        ca_t_max: float = 3.0,\n",
    "        ca_router_temperature: float = 1.5,\n",
    "        beta_ca: float = 0.35,\n",
    "        coord_reduction: int = 32,\n",
    "        coord_min_mid: int = 8,\n",
    "        coord_act: str = \"hswish\",\n",
    "        coord_init_alpha: float = 0.7,\n",
    "        coord_learnable_alpha: bool = True,\n",
    "        coord_beta: float = 0.35,\n",
    "        coord_dilation: int = 2,\n",
    "        coord_norm: str = \"gn\",\n",
    "        coord_use_spatial_gate: bool = False,\n",
    "        coord_spatial_gate_beta: float = 0.35,\n",
    "        coord_scale_min: float = 0.6,\n",
    "        coord_scale_max: float = 1.6,\n",
    "        coord_head_init_std: float = 0.01,\n",
    "        residual: bool = True,\n",
    "        alpha_baslangic: float = 0.75,\n",
    "        alpha_ogrenilsin: bool = False,\n",
    "        monitor: bool = False,\n",
    "        r_min: float = 0.45,\n",
    "        ema_momentum: float = 0.95,\n",
    "        min_kurtarma_orani: float = 0.2,\n",
    "        alpha_etkin_min: float = 0.2,\n",
    "        kurtarma_modu: str = \"ratio_floor\",\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalı.\")\n",
    "        if not (0.0 < ema_momentum < 1.0):\n",
    "            raise ValueError(\"ema_momentum (0,1) aralığında olmalı.\")\n",
    "        if r_min <= 0:\n",
    "            raise ValueError(\"r_min pozitif olmalı.\")\n",
    "        if not (0.0 <= min_kurtarma_orani <= 1.0):\n",
    "            raise ValueError(\"min_kurtarma_orani [0,1] aralığında olmalı.\")\n",
    "        if not (0.0 <= alpha_etkin_min <= 1.0):\n",
    "            raise ValueError(\"alpha_etkin_min [0,1] aralığında olmalı.\")\n",
    "        if kurtarma_modu not in (\"ratio_floor\", \"alpha_floor\"):\n",
    "            raise ValueError(\"kurtarma_modu 'ratio_floor' veya 'alpha_floor' olmalı.\")\n",
    "\n",
    "        self.return_maps = bool(return_maps)\n",
    "        self.residual = bool(residual)\n",
    "\n",
    "        self.monitor = bool(monitor)\n",
    "        self.r_min = float(r_min)\n",
    "        self.ema_m = float(ema_momentum)\n",
    "        self.min_kurtarma_orani = float(min_kurtarma_orani)\n",
    "        self.alpha_etkin_min = float(alpha_etkin_min)\n",
    "        self.kurtarma_modu = str(kurtarma_modu)\n",
    "\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=ca_reduction,\n",
    "            min_hidden=ca_min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            eps=1e-6,\n",
    "            act=ca_act,\n",
    "            bias=True,\n",
    "            fusion_router_hidden=ca_fusion_router_hidden,\n",
    "            return_fusion_weights=self.return_maps,\n",
    "            t_min=ca_t_min,\n",
    "            t_max=ca_t_max,\n",
    "            router_temperature=ca_router_temperature,\n",
    "            beta_ca=beta_ca,\n",
    "        )\n",
    "\n",
    "        self.coord = CoordinateAttPlus(\n",
    "            in_channels=channels,\n",
    "            reduction=coord_reduction,\n",
    "            min_mid_channels=coord_min_mid,\n",
    "            act=coord_act,\n",
    "            init_alpha=coord_init_alpha,\n",
    "            learnable_alpha=coord_learnable_alpha,\n",
    "            beta=coord_beta,\n",
    "            dilation=coord_dilation,\n",
    "            norm=coord_norm,\n",
    "            use_spatial_gate=coord_use_spatial_gate,\n",
    "            spatial_gate_beta=coord_spatial_gate_beta,\n",
    "            scale_min=coord_scale_min,\n",
    "            scale_max=coord_scale_max,\n",
    "            head_init_std=coord_head_init_std,\n",
    "        )\n",
    "\n",
    "        if self.residual:\n",
    "            # “alpha 0..1 aralığında garanti kalsın ama öğrenilebilir olsun.”\n",
    "            eps = 1e-6\n",
    "            a0 = float(alpha_baslangic)\n",
    "            a0 = min(max(a0, eps), 1.0 - eps)\n",
    "            raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "            if alpha_ogrenilsin:\n",
    "                self.alpha_ogrenilen_raw = nn.Parameter(raw0)\n",
    "            else:\n",
    "                self.register_buffer(\"alpha_ogrenilen_raw\", raw0)\n",
    "\n",
    "        self.register_buffer(\"r_ema\", torch.tensor(1.0))\n",
    "\n",
    "    def alpha_temiz(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if (not self.residual) or (not hasattr(self, \"alpha_ogrenilen_raw\")):\n",
    "            return x.new_tensor(1.0)\n",
    "        return torch.sigmoid(self.alpha_ogrenilen_raw).to(device=x.device, dtype=x.dtype)\n",
    "    \n",
    "    @staticmethod\n",
    "    def std_batch_ort(x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.float().flatten(1).std(dim=1, unbiased=False).mean()\n",
    "    # STD == “bu tensorde enerji var mı, dağılım canlı mı?\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def r_ema_guncelle(self, r_out: torch.Tensor):\n",
    "        r_det = r_out.detach().to(device=self.r_ema.device, dtype=self.r_ema.dtype)\n",
    "        self.r_ema.mul_(self.ema_m).add_((1.0 - self.ema_m) * r_det)\n",
    "    # residual karışım uygulanmış çıkış, girişe göre ne kadar bastırıldı?\n",
    "\n",
    "    def alpha_etkin(self, x: torch.Tensor, alpha_temiz: torch.Tensor) -> torch.Tensor:\n",
    "        ratio = (self.r_ema.detach() / max(self.r_min, 1e-12)).clamp(0.0, 1.0)\n",
    "        ratio = ratio.to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if self.kurtarma_modu == \"ratio_floor\":\n",
    "            ratio = ratio.clamp(self.min_kurtarma_orani, 1.0)\n",
    "            return alpha_temiz * ratio\n",
    "\n",
    "        alpha_eff = alpha_temiz * ratio\n",
    "        return alpha_eff.clamp(self.alpha_etkin_min, 1.0)\n",
    "    # Bu blok, residual karışım katsayısı alpha’yı training sırasında otomatik kısıyor.\n",
    "    # r_min = “izin verdiğim minimum enerji oranı”\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        monitor_stats = None\n",
    "\n",
    "        if self.return_maps:\n",
    "            y_ca, ca_map, fusion_w = self.ca(x)\n",
    "            y = self.coord(y_ca)\n",
    "\n",
    "            if not self.residual:\n",
    "                coord_stats = self.coord.last_mask_stats()\n",
    "                return y, ca_map, fusion_w, coord_stats, None\n",
    "\n",
    "            alpha_temiz = self.alpha_temiz(x)\n",
    "            alpha_etkin = alpha_temiz\n",
    "\n",
    "            if self.training and self.monitor:\n",
    "                x_std = self.std_batch_ort(x)\n",
    "                y_std = self.std_batch_ort(y)\n",
    "\n",
    "                out_tmp = x + alpha_temiz * (y - x)\n",
    "                out_std = self.std_batch_ort(out_tmp)\n",
    "\n",
    "                r_block = (y_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "                r_out = (out_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "\n",
    "                self.r_ema_guncelle(r_out)\n",
    "                alpha_etkin = self.alpha_etkin(x, alpha_temiz)\n",
    "\n",
    "                monitor_stats = {\n",
    "                    \"x_std\": float(x_std.detach()),\n",
    "                    \"y_std\": float(y_std.detach()),\n",
    "                    \"out_std_pre\": float(out_std.detach()),\n",
    "                    \"r_block\": float(r_block.detach()),\n",
    "                    \"r_out_pre\": float(r_out.detach()),\n",
    "                    \"r_ema\": float(self.r_ema.detach()),\n",
    "                    \"alpha_temiz\": float(alpha_temiz.detach()),\n",
    "                    \"alpha_etkin\": float(alpha_etkin.detach()),\n",
    "                    \"kurtarma_modu\": self.kurtarma_modu,\n",
    "                }\n",
    "\n",
    "            out = x + alpha_etkin * (y - x)\n",
    "            coord_stats = self.coord.last_mask_stats()\n",
    "            return out, ca_map, fusion_w, coord_stats, monitor_stats\n",
    "\n",
    "        y_ca, _ = self.ca(x)\n",
    "        y = self.coord(y_ca)\n",
    "\n",
    "        if not self.residual:\n",
    "            return y\n",
    "\n",
    "        alpha_temiz = self.alpha_temiz(x)\n",
    "        alpha_etkin = alpha_temiz\n",
    "\n",
    "        if self.training and self.monitor:\n",
    "            x_std = self.std_batch_ort(x)\n",
    "            out_tmp = x + alpha_temiz * (y - x)\n",
    "            out_std = self.std_batch_ort(out_tmp)\n",
    "            r_out = (out_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "\n",
    "            self.r_ema_guncelle(r_out)\n",
    "            alpha_etkin = self.alpha_etkin(x, alpha_temiz)\n",
    "\n",
    "        out = x + alpha_etkin * (y - x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ba769",
   "metadata": {},
   "source": [
    "## Hücre 1 — PreAct + Attention CNN Bloğu (asıl entegrasyon)\n",
    "\n",
    "Bu blok şunu yapacak:\n",
    "* x_main = Act(Norm(x)) (pre-act)\n",
    "* → Conv → Conv → Attention(residual=False)\n",
    "* → skip + y (residual dışarıda, tek yerde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694a33ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def extract_y(attn_out):\n",
    "    # return_maps=True ise tuple döner; y ilk elemandır\n",
    "    return attn_out[0] if isinstance(attn_out, (tuple, list)) else attn_out\n",
    "\n",
    "class PreActAttnBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        out_ch: int,\n",
    "        stride: int = 1,\n",
    "        norm: str = \"bn\",       # \"bn\" / \"gn\" / \"none\"\n",
    "        act: str = \"relu\",      # \"relu\" / \"silu\"\n",
    "        attention: nn.Module = None,\n",
    "        skip_norm: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = make_norm(norm, in_ch)\n",
    "        self.act1  = nn.ReLU(inplace=True) if act == \"relu\" else nn.SiLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.norm2 = make_norm(norm, out_ch)\n",
    "        self.act2  = nn.ReLU(inplace=True) if act == \"relu\" else nn.SiLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.attn = attention if attention is not None else nn.Identity()\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            layers = [nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)]\n",
    "            if skip_norm:\n",
    "                layers.append(make_norm(norm, out_ch))\n",
    "            self.proj = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pre-act sadece main path'e\n",
    "        x_main = self.act1(self.norm1(x))\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "        y = self.conv1(x_main)\n",
    "        y = self.conv2(self.act2(self.norm2(y)))\n",
    "        y = extract_y(self.attn(y))  # attention tuple dönerse ilkini al\n",
    "        out = skip + y\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc490608",
   "metadata": {},
   "source": [
    "## Hücre 2 — Attention’ı F(x) olarak kur (residual=False)\n",
    "\n",
    "DİKKAT: channels burada out_ch ile aynı olmalı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9446f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "out_ch = 64\n",
    "\n",
    "attn_fx = CBAMChannelPlusCoord(\n",
    "    channels=out_ch,\n",
    "    residual=False,       # <<< F(x) modu\n",
    "    return_maps=False,    # testte map istemiyoruz\n",
    "    monitor=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b1a39",
   "metadata": {},
   "source": [
    "## Hücre 3 — Forward testi (shape + NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a18287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (8, 64, 32, 32)\n",
      "y: (8, 64, 32, 32)\n",
      "nan? False | inf? False\n"
     ]
    }
   ],
   "source": [
    "block = PreActAttnBlock(\n",
    "    in_ch=64,\n",
    "    out_ch=64,\n",
    "    stride=1,\n",
    "    norm=\"bn\",\n",
    "    act=\"relu\",\n",
    "    attention=attn_fx\n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(8, 64, 32, 32, device=device)\n",
    "y = block(x)\n",
    "\n",
    "print(\"x:\", tuple(x.shape))\n",
    "print(\"y:\", tuple(y.shape))\n",
    "print(\"nan?\", torch.isnan(y).any().item(), \"| inf?\", torch.isinf(y).any().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ef447",
   "metadata": {},
   "source": [
    "## Hücre 4 — Downsample testi (stride=2, kanal değişimi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a81840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (8, 64, 32, 32)\n",
      "y: (8, 128, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "attn_fx_128 = CBAMChannelPlusCoord(\n",
    "    channels=128,\n",
    "    residual=False,\n",
    "    return_maps=False,\n",
    "    monitor=False,\n",
    ").to(device)\n",
    "\n",
    "block_ds = PreActAttnBlock(\n",
    "    in_ch=64,\n",
    "    out_ch=128,\n",
    "    stride=2,\n",
    "    norm=\"bn\",\n",
    "    act=\"relu\",\n",
    "    attention=attn_fx_128\n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(8, 64, 32, 32, device=device)\n",
    "y = block_ds(x)\n",
    "\n",
    "print(\"x:\", tuple(x.shape))\n",
    "print(\"y:\", tuple(y.shape))   # beklenen: (8, 128, 16, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553672ed",
   "metadata": {},
   "source": [
    "## Hücre 5 — Backward testi (gradient akıyor mu?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6397dc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.01794297620654106\n",
      "grad conv1: 0.01534736342728138\n",
      "grad conv2: 0.7625274658203125\n",
      "input grad: 0.0013965800171718001\n"
     ]
    }
   ],
   "source": [
    "block.train()\n",
    "x = torch.randn(8, 64, 32, 32, device=device, requires_grad=True)\n",
    "y = block(x)\n",
    "\n",
    "loss = y.mean()\n",
    "loss.backward()\n",
    "\n",
    "print(\"loss:\", float(loss.item()))\n",
    "print(\"grad conv1:\", float(block.conv1.weight.grad.norm().item()))\n",
    "print(\"grad conv2:\", float(block.conv2.weight.grad.norm().item()))\n",
    "print(\"input grad:\", float(x.grad.norm().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae467683",
   "metadata": {},
   "source": [
    "Beklenen:\n",
    "\n",
    "* grad normlar 0 değil\n",
    "\n",
    "* nan/inf yok\n",
    "\n",
    "* loss değeri y.mean() kullanıldığı için - yada düşük gelebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccab1a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
