{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93761792",
   "metadata": {},
   "source": [
    "\n",
    "# Residual Blok Rehberi (Sıfırdan İleri Seviyeye)\n",
    "\n",
    "Bu notlar, **residual (skip) bağlantının** mantığını en temelden başlayıp pratik entegrasyona kadar götürür.  \n",
    "Amaç: Bir CNN modelini adım adım büyütürken **residual blokları** doğru tanımlamak ve **mevcut bir mimariye** temiz şekilde entegre edebilmek.\n",
    "\n",
    "> Kısa sözlük  \n",
    "- **F(x)**: Bloğun öğrenmeye çalıştığı dönüşüm (conv/BN/act/attention vb.)  \n",
    "- **skip(x)**: Kimlik (identity) ya da projeksiyon (1×1 conv) yoluyla taşınan bilgi  \n",
    "- **Çıkış**: `y = F(x) + skip(x)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b666a7",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Residual bağlantı neyi çözer?\n",
    "\n",
    "Derin ağlarda iki klasik problem ortaya çıkar:\n",
    "\n",
    "1. **Optimizasyon zorlaşır** (derinlik arttıkça öğrenme “takılabilir”).  \n",
    "2. **Gradyan akışı zayıflayabilir** (çok katmandan geri akış zorlaşır).\n",
    "\n",
    "Residual bağlantı şunu garanti eder:\n",
    "\n",
    "- Eğer `F(x)` başlangıçta işe yaramıyorsa bile model en azından **skip(x)** üzerinden “akabilir”.  \n",
    "- Eğitim ilerledikçe `F(x)` “düzeltme” (residual) öğrenir.\n",
    "\n",
    "Bu yüzden residual bloklar pratikte şu sezgiyle çalışır:\n",
    "\n",
    "> “Tamamen yeni bir şey üretmek yerine, **mevcut temeli** küçük düzeltmelerle iyileştir.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284d0fc",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Residual blok yazarken dikkat edilecek 4 şey (Checklist)\n",
    "\n",
    "Aşağıdaki dört maddeyi kontrol ettiğinde residual blokların %90’ı düzgün çalışır:\n",
    "\n",
    "1. **Şekil uyumu (shape)**  \n",
    "   - `F(x)` ve `skip(x)` toplanacağı için `(B, C, H, W)` boyutlarının uyumlu olması gerekir.\n",
    "\n",
    "2. **Downsample / stride**  \n",
    "   - Stride=2 gibi bir durum varsa `skip(x)` yolu da aynı şekilde downsample edilmelidir (genelde 1×1 conv + stride).\n",
    "\n",
    "3. **Kanal geçişi (in_ch → out_ch)**  \n",
    "   - Kanal sayısı değişiyorsa yine 1×1 projeksiyon gerekir.\n",
    "\n",
    "4. **Norm/Activation sırası (Pre-Act vs Post-Act)**  \n",
    "   - Post-Act (klasik): `Conv → BN → ReLU`  \n",
    "   - Pre-Act (v2): `BN → ReLU → Conv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13fd7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ortam: PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "def demo_shapes(m: nn.Module, x: torch.Tensor, name: str = \"\"):\n",
    "    with torch.no_grad():\n",
    "        y = m(x)\n",
    "    print(f\"{name:>18}  in: {tuple(x.shape)} -> out: {tuple(y.shape)} | params: {count_params(m):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f6692",
   "metadata": {},
   "source": [
    "\n",
    "## 3) En basit blok: Conv-BN-Act\n",
    "\n",
    "Residual’a geçmeden önce temel bir yapı taşı kuralım: `Conv + BN + Activation`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51721e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ConvBNAct  in: (2, 3, 32, 32) -> out: (2, 16, 32, 32) | params: 464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, cin, cout, k=3, s=1, p=None, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = k // 2\n",
    "        self.conv = nn.Conv2d(cin, cout, k, stride=s, padding=p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(cout)\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"silu\":\n",
    "            self.act = nn.SiLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act: 'relu' veya 'silu' destekleniyor.\")\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "x = torch.randn(2, 3, 32, 32)\n",
    "demo_shapes(ConvBNAct(3, 16), x, \"ConvBNAct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2fc32",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Post-Activation Basic Residual Block (ResNet v1 tarzı)\n",
    "\n",
    "Klasik şekil:\n",
    "\n",
    "- Residual yol (F): `Conv → BN → ReLU → Conv → BN`\n",
    "- Skip yol: `x` ya da `1×1 conv` (boyut uyumu için)\n",
    "- Çıkış: `ReLU(F(x) + skip(x))`\n",
    "\n",
    "> Not: Bazı implementasyonlarda en sondaki ReLU kaldırılabilir, ancak v1’de yaygın olan hali aşağıdaki gibidir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3681cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   BasicPostAct s1  in: (2, 16, 32, 32) -> out: (2, 16, 32, 32) | params: 4,672\n",
      "   BasicPostAct s2  in: (2, 16, 32, 32) -> out: (2, 32, 16, 16) | params: 14,528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BasicBlock_PostAct(nn.Module):\n",
    "    def __init__(self, cin, cout, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNAct(cin, cout, k=3, s=stride, act=\"relu\")\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(cout, cout, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "        )\n",
    "        self.proj = None\n",
    "        if stride != 1 or cin != cout:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(cin, cout, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(cout),\n",
    "            )\n",
    "        self.out_act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = y + skip\n",
    "        return self.out_act(y)\n",
    "\n",
    "demo_shapes(BasicBlock_PostAct(16, 16), torch.randn(2,16,32,32), \"BasicPostAct s1\")\n",
    "demo_shapes(BasicBlock_PostAct(16, 32, stride=2), torch.randn(2,16,32,32), \"BasicPostAct s2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a8fa0",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Pre-Activation Basic Residual Block (ResNet v2 tarzı)\n",
    "\n",
    "Buradaki kritik fark: **BN+ReLU, conv’dan önce** gelir.\n",
    "\n",
    "- Residual yol (F): `BN → ReLU → Conv → BN → ReLU → Conv`\n",
    "- Skip yol: `x` ya da `1×1 conv` (boyut uyumu için)\n",
    "- Çıkış: genelde `F(x) + skip(x)` (en sonda ekstra ReLU şart değil)\n",
    "\n",
    "Bu tasarım, özellikle derin ağlarda optimizasyonu kolaylaştırmasıyla bilinir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae146c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BasicPreAct s1  in: (2, 16, 32, 32) -> out: (2, 16, 32, 32) | params: 4,672\n",
      "    BasicPreAct s2  in: (2, 16, 32, 32) -> out: (2, 32, 16, 16) | params: 14,432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BasicBlock_PreAct(nn.Module):\n",
    "    def __init__(self, cin, cout, stride=1):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(cin)\n",
    "        self.conv1 = nn.Conv2d(cin, cout, 3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(cout)\n",
    "        self.conv2 = nn.Conv2d(cout, cout, 3, padding=1, bias=False)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or cin != cout:\n",
    "            self.proj = nn.Conv2d(cin, cout, 1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "\n",
    "        y = F.relu(self.bn1(x), inplace=True)\n",
    "        y = self.conv1(y)\n",
    "\n",
    "        y = F.relu(self.bn2(y), inplace=True)\n",
    "        y = self.conv2(y)\n",
    "\n",
    "        return y + skip\n",
    "\n",
    "demo_shapes(BasicBlock_PreAct(16, 16), torch.randn(2,16,32,32), \"BasicPreAct s1\")\n",
    "demo_shapes(BasicBlock_PreAct(16, 32, stride=2), torch.randn(2,16,32,32), \"BasicPreAct s2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20191794",
   "metadata": {},
   "source": [
    "\n",
    "## 6) “Wide” fikri nerede? (Widen Factor k)\n",
    "\n",
    "Önemli nokta:\n",
    "\n",
    "- **Wide** olmak, bloğun “özel bir forward yapması” değildir.  \n",
    "- Wide olmak, bloğun **kanal planıyla** ilgilidir.\n",
    "\n",
    "Yani `out_ch` değerlerini `k` ile çarparak stage’leri baştan geniş kurarsın:\n",
    "- stage1: `16k`\n",
    "- stage2: `32k`\n",
    "- stage3: `64k`\n",
    "\n",
    "Bu yüzden `BasicBlock_PreAct` aynı kalıp, sadece `cout` değerleri büyür.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2039b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TinyWideResNet  in: (2, 3, 32, 32) -> out: (2, 10) | params: 1,467,610\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_stage(block_cls, cin, cout, n_blocks, first_stride):\n",
    "    layers = [block_cls(cin, cout, stride=first_stride)]\n",
    "    for _ in range(1, n_blocks):\n",
    "        layers.append(block_cls(cout, cout, stride=1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class TinyWideResNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, num_classes=10, depth=28, k=2):\n",
    "        super().__init__()\n",
    "        assert (depth - 4) % 6 == 0\n",
    "        n = (depth - 4) // 6\n",
    "        base = 16\n",
    "        ch1, ch2, ch3 = base*k, base*2*k, base*4*k\n",
    "\n",
    "        self.stem = nn.Conv2d(in_ch, base, 3, padding=1, bias=False)\n",
    "\n",
    "        # Pre-Act blok kullanıyoruz\n",
    "        self.stage1 = make_stage(BasicBlock_PreAct, base, ch1, n, first_stride=1)\n",
    "        self.stage2 = make_stage(BasicBlock_PreAct, ch1,  ch2, n, first_stride=2)\n",
    "        self.stage3 = make_stage(BasicBlock_PreAct, ch2,  ch3, n, first_stride=2)\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(ch3)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(ch3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = F.relu(self.bn(x), inplace=True)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "m = TinyWideResNet(depth=28, k=2)\n",
    "demo_shapes(m, torch.randn(2,3,32,32), \"TinyWideResNet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426dfbc",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Bottleneck (Expansion) fikri\n",
    "\n",
    "Bottleneck bloklar, hesap maliyetini kontrol ederek daha “geniş” temsile izin verir.\n",
    "\n",
    "Klasik bottleneck akışı (ResNet-50 tarzı):\n",
    "- 1×1 (daralt)\n",
    "- 3×3 (işle)\n",
    "- 1×1 (genişlet / expansion)\n",
    "\n",
    "`expansion` çoğunlukla 4’tür: `out = bottleneck_ch * 4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be459efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Bottleneck s1  in: (2, 64, 32, 32) -> out: (2, 256, 32, 32) | params: 75,008\n",
      "     Bottleneck s2  in: (2, 64, 32, 32) -> out: (2, 256, 16, 16) | params: 75,008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Bottleneck_PostAct(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, cin, bottleneck_ch, stride=1):\n",
    "        super().__init__()\n",
    "        out_ch = bottleneck_ch * self.expansion\n",
    "        self.conv1 = ConvBNAct(cin, bottleneck_ch, k=1, s=1, p=0, act=\"relu\")\n",
    "        self.conv2 = ConvBNAct(bottleneck_ch, bottleneck_ch, k=3, s=stride, p=1, act=\"relu\")\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(bottleneck_ch, out_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "        self.proj = None\n",
    "        if stride != 1 or cin != out_ch:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(cin, out_ch, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "        self.out_act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = self.conv3(y)\n",
    "        y = y + skip\n",
    "        return self.out_act(y)\n",
    "\n",
    "demo_shapes(Bottleneck_PostAct(64, 64, stride=1), torch.randn(2,64,32,32), \"Bottleneck s1\")\n",
    "demo_shapes(Bottleneck_PostAct(64, 64, stride=2), torch.randn(2,64,32,32), \"Bottleneck s2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d40a9d",
   "metadata": {},
   "source": [
    "\n",
    "## 8) SE (Squeeze-Excitation) modülü\n",
    "\n",
    "SE, **channel attention** türüdür:\n",
    "\n",
    "1. Squeeze: `GlobalAvgPool` ile her kanal için tek bir özet çıkarılır.  \n",
    "2. Excite: Küçük bir MLP (genelde iki FC/1×1 conv) ile kanal önemleri öğrenilir.  \n",
    "3. Sigmoid: ağırlıklar `[0,1]` aralığına çekilir.  \n",
    "4. Ölçekleme: `x * w`\n",
    "\n",
    "> SE tek başına residual değildir.  \n",
    "> Residual bloğun içindeki `F(x)` dönüşümüne eklenen bir modüldür.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924326dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           SEBlock  in: (2, 64, 16, 16) -> out: (2, 64, 16, 16) | params: 580\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, min_hidden: int = 4):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.fc(self.pool(x))\n",
    "        return x * w\n",
    "\n",
    "# test\n",
    "demo_shapes(SEBlock(64), torch.randn(2,64,16,16), \"SEBlock\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f21815",
   "metadata": {},
   "source": [
    "\n",
    "## 9) SE-Residual Block (en yaygın yerleşim)\n",
    "\n",
    "Pratikte en sık görülen yerleşim:\n",
    "\n",
    "- `F(x)` dönüşümü tamamlanır (conv/BN/act/conv/BN)  \n",
    "- **SE, F(x) çıktısına uygulanır**  \n",
    "- sonra `+ skip` yapılır  \n",
    "- en sona aktivasyon (post-act) opsiyoneldir\n",
    "\n",
    "Aşağıdaki örnek: Post-Act tarzı bir basic blok + SE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a9d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SEResBlock s1  in: (2, 32, 32, 32) -> out: (2, 32, 32, 32) | params: 18,852\n",
      "     SEResBlock s2  in: (2, 32, 32, 32) -> out: (2, 64, 16, 16) | params: 58,308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SEResBlock(nn.Module):\n",
    "    def __init__(self, cin, cout, stride=1, reduction=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNAct(cin, cout, k=3, s=stride, p=1, act=\"silu\")\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(cout, cout, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "        )\n",
    "        self.se = SEBlock(cout, reduction=reduction)\n",
    "\n",
    "        self.proj = None\n",
    "        if stride != 1 or cin != cout:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(cin, cout, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(cout),\n",
    "            )\n",
    "        self.out_act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x if self.proj is None else self.proj(x)\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = self.se(y)\n",
    "        y = y + skip\n",
    "        return self.out_act(y)\n",
    "\n",
    "demo_shapes(SEResBlock(32,32), torch.randn(2,32,32,32), \"SEResBlock s1\")\n",
    "demo_shapes(SEResBlock(32,64,stride=2), torch.randn(2,32,32,32), \"SEResBlock s2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d917b",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Bir modeli basitten ileriye doğru kurma (progressive build)\n",
    "\n",
    "Aşağıdaki sıra, pratikte en anlaşılır “büyütme” yoludur:\n",
    "\n",
    "1. **Baseline CNN**: ConvBNAct + pooling + classifier  \n",
    "2. **Residual CNN**: BasicBlock (post veya pre) ile stage’ler  \n",
    "3. **Wide**: kanal planını `k` ile ölçekle (WideResNet mantığı)  \n",
    "4. **Bottleneck**: büyük modellerde hesap/parametre verimliliği  \n",
    "5. **SE (veya başka attention)**: seçicilik, kanal önemlendirme  \n",
    "6. **Ek düzenlemeler**: DropPath, GN/LN, FPN/PAN (task’e göre)\n",
    "\n",
    "Aşağıda 1→2 geçişini “mini” bir örnekle gösteriyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f857f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BaselineCNN  in: (2, 3, 32, 32) -> out: (2, 10) | params: 104,042\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_ch=3, base=32):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBNAct(in_ch, base, 3, 1, 1, act=\"relu\"),\n",
    "            ConvBNAct(base, base, 3, 1, 1, act=\"relu\"),\n",
    "            nn.MaxPool2d(2),  # 32->16\n",
    "            ConvBNAct(base, base*2, 3, 1, 1, act=\"relu\"),\n",
    "            nn.MaxPool2d(2),  # 16->8\n",
    "            ConvBNAct(base*2, base*4, 3, 1, 1, act=\"relu\"),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(base*4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "demo_shapes(BaselineCNN(), torch.randn(2,3,32,32), \"BaselineCNN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553d5ae",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Entegrasyon mantığı: “blok aynı, yerleştirme değişir”\n",
    "\n",
    "Bir residual/attention bloğu genelde **şu nedenle taşınabilir**:\n",
    "\n",
    "- Girdi/çıktı formatı: `(B, C, H, W)` → `(B, C', H', W')`  \n",
    "- Blok “yerel” bir dönüşüm yapar  \n",
    "- Toplama yapılacaksa `skip(x)` boyut uyumunu garanti eder\n",
    "\n",
    "**Sınıflandırma**: backbone + pooling + fc  \n",
    "**Detection (YOLO vb.)**: backbone + neck (FPN/PAN) + head  \n",
    "**Segmentation**: encoder + decoder (+ skip-merge) + mask head\n",
    "\n",
    "Entegrasyonda değişen şey çoğu zaman blok değil, blokların **hangi stage’e** yerleştirildiğidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6764176",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Mini kontrol: “Bu blok entegre olur mu?”\n",
    "\n",
    "Bir bloğu bir modele eklemeden önce şu 3 test yeterlidir:\n",
    "\n",
    "1. **Shape testi**: Rastgele `x` ile `forward` çalışıyor mu?  \n",
    "2. **Downsample testi**: stride=2 senaryosu çalışıyor mu?  \n",
    "3. **Grad testi**: `loss = y.mean()` ile `backward()` patlıyor mu?\n",
    "\n",
    "Aşağıdaki hücre bu üç kontrolü hızlıca yapar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be45fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok: (2, 32, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "def quick_checks(block: nn.Module, x: torch.Tensor):\n",
    "    y = block(x)          # shape\n",
    "    y.mean().backward()   # grad\n",
    "    return y\n",
    "\n",
    "blk = SEResBlock(16, 32, stride=2)\n",
    "x = torch.randn(2, 16, 32, 32, requires_grad=True)\n",
    "y = quick_checks(blk, x)\n",
    "print(\"ok:\", tuple(y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a6bb7",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Kapanış\n",
    "\n",
    "Bu notlardan sonra şu fark netleşmiş olmalı:\n",
    "\n",
    "- **Residual**: “bağlantı / öğrenme çerçevesi”  \n",
    "- **Attention (SE vb.)**: `F(x)` içinde “seçicilik”  \n",
    "- **Wide / Grouped / Bottleneck**: `F(x)` içindeki **kanal/hesap tasarımı**  \n",
    "- **Pre/Post-Act**: blok içindeki **sıralama şablonu**\n",
    "\n",
    "İleride bir blok gördüğünde yapılacak en pratik şey:\n",
    "1) `skip` yolu ne? (identity mi proj mi)  \n",
    "2) `F(x)` hangi sırayla? (pre mi post mu)  \n",
    "3) toplama nerede? (önce/sonra activation var mı)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
