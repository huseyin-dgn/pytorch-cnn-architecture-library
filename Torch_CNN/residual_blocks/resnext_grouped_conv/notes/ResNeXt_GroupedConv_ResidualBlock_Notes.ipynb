{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8517b1a",
   "metadata": {},
   "source": [
    "\n",
    "# ResNeXt Grouped Convolution Residual Block (ResNeXt Bloğu) — Temelden Detaya\n",
    "\n",
    "> Bu notlar; **Basic Residual**, **Pre-activation Residual**, **Bottleneck** ve **ResNeXt (Grouped Conv) Residual Block** yapılarını aynı çerçevede ele alır.  \n",
    "> Odak: ResNeXt’in ne anlama geldiği, neden ortaya çıktığı ve diğer üç bloktan farkları.\n",
    "\n",
    "---\n",
    "\n",
    "## İçindekiler\n",
    "\n",
    "1. Residual öğrenme fikri (neden residual?)\n",
    "2. Basic Residual Block (ResNet-18/34)\n",
    "3. Pre-activation Residual Block (ResNet v2)\n",
    "4. Bottleneck Residual Block (ResNet-50/101/152)\n",
    "5. ResNeXt: Grouped Conv Residual Block (Cardinality)\n",
    "6. Karşılaştırma: Basic vs Pre-Act vs Bottleneck vs ResNeXt\n",
    "7. Parametre / FLOPs sezgisi (kabaca)\n",
    "8. PyTorch: Referans implementasyonları\n",
    "9. Pratik notlar: nerede hangisi seçilir?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e36bb4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Residual Öğrenme: Temel Mantık\n",
    "\n",
    "Klasik bir konvolüsyonel blokta bir katmanlar dizisi, girdiden doğrudan bir çıktı üretir:\n",
    "\n",
    "\\[\n",
    "y = H(x)\n",
    "\\]\n",
    "\n",
    "ResNet fikri şunu söyler: **ağ, doğrudan \\(H(x)\\)'i öğrenmek yerine** bir \"artık\" (residual) fonksiyonu öğrensin:\n",
    "\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "- \\(x\\): skip/identity hattından geçen giriş\n",
    "- \\(F(x)\\): main path üzerindeki konvolüsyon + normalizasyon + aktivasyonların ürettiği dönüşüm\n",
    "\n",
    "### Neden işe yarar?\n",
    "- Derin ağlarda **gradyan akışını** iyileştirir (skip bağlantı üzerinden).\n",
    "- Ağ, \"gerekirse hiçbir şey yapmama\" (identity) davranışına kolayca yakınsar.\n",
    "- Optimizasyon daha stabil olur: \\(F(x)\\) küçük düzeltmeler öğrenebilir.\n",
    "\n",
    "### Boyut uyuşmazlığı\n",
    "Eğer kanal sayısı / uzamsal boyut (H×W) değişiyorsa, identity hattı da uyarlanır:\n",
    "\n",
    "\\[\n",
    "y = W_s x + F(x)\n",
    "\\]\n",
    "\n",
    "Pratikte \\(W_s\\) çoğu zaman **1×1 Conv (projection)** + BN'dir.\n",
    "\n",
    "---\n",
    "\n",
    "> Bu temel denklem, Basic / Pre-Act / Bottleneck / ResNeXt dahil tüm residual blokların iskeletidir. Fark, \\(F(x)\\)'in nasıl tasarlandığıdır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b58c2",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Basic Residual Block (ResNet-18/34)\n",
    "\n",
    "### Yapı\n",
    "Main path genellikle şu şekildedir:\n",
    "\n",
    "- 3×3 Conv → BN → ReLU  \n",
    "- 3×3 Conv → BN  \n",
    "- (toplama) + ReLU\n",
    "\n",
    "\\[\n",
    "F(x) = \\text{BN}(\\text{Conv}_{3\\times3}(\\sigma(\\text{BN}(\\text{Conv}_{3\\times3}(x)))))\n",
    "\\]\n",
    "\\[\n",
    "y = x + F(x) \\quad \\text{(sonra ReLU)}\n",
    "\\]\n",
    "\n",
    "### Özellikler\n",
    "- Basit ve etkilidir.\n",
    "- Çok derinleşince (50+ katman gibi) hesap maliyeti artar; bu yüzden Bottleneck yaygınlaşır.\n",
    "- Kanal genişliği (width) arttıkça maliyet hızlı yükselir (3×3 pahalıdır).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc61fd",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Pre-activation Residual Block (ResNet v2)\n",
    "\n",
    "Pre-activation yaklaşımı, aktivasyon ve normalizasyonu **toplamadan önce** konumlandırır.\n",
    "\n",
    "### Klasik (v1) vs Pre-Act (v2) farkı\n",
    "- v1: Conv → BN → ReLU → Conv → BN → (+) → ReLU\n",
    "- v2: BN → ReLU → Conv → BN → ReLU → Conv → (+)  (sonrasında ekstra ReLU yok ya da mimariye göre değişir)\n",
    "\n",
    "### Neden ortaya çıktı?\n",
    "- Skip hattı boyunca \"daha temiz\" identity akışı sağlar.\n",
    "- Normalizasyon/aktivasyonlar main path'i şekillendirirken, skip hattı mümkün olduğunca bozulmadan akar.\n",
    "- Çok derin ağlarda optimizasyonu daha da stabilize eder.\n",
    "\n",
    "Matematiksel sezgi:\n",
    "\\[\n",
    "y = x + F(\\text{BN}(x))\n",
    "\\]\n",
    "gibi düşünülebilir (tam form mimariye göre).\n",
    "\n",
    "### Özet\n",
    "- Aynı parametre sayısıyla, genellikle daha iyi eğitim dinamiği.\n",
    "- Modern pratikte bazı ResNet türevleri bu fikri içerir.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a37c67",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Bottleneck Residual Block (ResNet-50/101/152)\n",
    "\n",
    "Basic blok iki tane 3×3 kullanırken, Bottleneck pahalı 3×3'ü \"dar bir kanal alanında\" yaptırır.\n",
    "\n",
    "### Yapı (tipik)\n",
    "- 1×1 Conv (reduce) → BN → ReLU\n",
    "- 3×3 Conv (process) → BN → ReLU\n",
    "- 1×1 Conv (expand) → BN\n",
    "- (+) → ReLU\n",
    "\n",
    "Kanallar:\n",
    "- Giriş: \\(C\\)\n",
    "- İç (bottleneck): \\(C_b\\) (çoğu zaman \\(C_b = C/4\\))\n",
    "- Çıkış: \\(C_{\\text{out}} = \\text{expansion} \\cdot C_b\\) (expansion genellikle 4)\n",
    "\n",
    "\\[\n",
    "F(x) = \\text{Conv}_{1\\times1}^{\\uparrow}\\Big(\\text{Conv}_{3\\times3}(\\text{Conv}_{1\\times1}^{\\downarrow}(x))\\Big)\n",
    "\\]\n",
    "\n",
    "### Neden avantajlı?\n",
    "- 3×3 işlemi düşük kanal sayısında yapılır → FLOPs düşer.\n",
    "- Derinlik artırmak (50/101/152) daha uygulanabilir olur.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d33c28",
   "metadata": {},
   "source": [
    "\n",
    "## 5) ResNeXt: Grouped Convolution Residual Block (Cardinality)\n",
    "\n",
    "ResNeXt, Bottleneck bloğunun içindeki 3×3 konvolüsyonu **grouped convolution** olarak tasarlar.\n",
    "\n",
    "### 5.1) Grouped Convolution nedir?\n",
    "Klasik bir konvolüsyonda \\(C_{in}\\) kanallı giriş, tüm kanallar birlikte işlenir.  \n",
    "Grouped conv'ta kanallar **g** gruba ayrılır ve her grup kendi içinde konvolüsyonlanır.\n",
    "\n",
    "- Group sayısı: \\(g\\)\n",
    "- Her gruptaki kanal sayısı: yaklaşık \\(C_{in}/g\\)\n",
    "\n",
    "Bu yaklaşım iki şey sağlar:\n",
    "1. Aynı hesap bütçesinde daha fazla \"paralel dönüşüm yolu\"\n",
    "2. Model kapasitesini artırmanın yeni ekseni: **cardinality**\n",
    "\n",
    "### 5.2) ResNeXt’in fikri: \"Split-Transform-Merge\"\n",
    "ResNeXt bloğunu sezgisel olarak şu şekilde düşünebilirsin:\n",
    "\n",
    "1) **Split:** Özellik kanallarını g gruba böl  \n",
    "2) **Transform:** Her grup içinde 3×3 conv uygula  \n",
    "3) **Merge:** Çıktıları birleştir (concatenate/implicit merge)  \n",
    "4) Skip ile topla: \\(y = x + F(x)\\)\n",
    "\n",
    "Aslında grouped conv bunu tek bir op içinde yapar.\n",
    "\n",
    "### 5.3) ResNeXt bloğunun tipik yapısı (Bottleneck + Grouped Conv)\n",
    "\n",
    "- 1×1 Conv (reduce)  \n",
    "- 3×3 **Grouped Conv** (groups = cardinality)  \n",
    "- 1×1 Conv (expand)  \n",
    "- (+) → ReLU\n",
    "\n",
    "\\[\n",
    "F(x)=\\text{Conv}_{1\\times1}^{\\uparrow}\\big(\\text{GConv}_{3\\times3}^{(g)}(\\text{Conv}_{1\\times1}^{\\downarrow}(x))\\big)\n",
    "\\]\n",
    "\n",
    "Buradaki kritik fark: Bottleneck'teki 3×3 yerine **GConv**.\n",
    "\n",
    "### 5.4) Neden ortaya çıktı?\n",
    "ResNet’te performansı artırmanın tipik yolları:\n",
    "- derinliği artır (depth)\n",
    "- genişliği artır (width)\n",
    "\n",
    "ResNeXt üçüncü bir yol önerir:\n",
    "- **cardinality** artır (daha fazla grup → daha fazla paralel dönüşüm yolu)\n",
    "\n",
    "Bu pratikte genelde şu anlama gelir:\n",
    "- Aynı FLOPs seviyesinde, group sayısını artırarak daha iyi doğruluk elde edebilme.\n",
    "- Modeli \"çok genişletmeden\" kapasiteyi artırma.\n",
    "\n",
    "### 5.5) Cardinality ne demek?\n",
    "Cardinality = grouped conv içindeki **group sayısı** (g).  \n",
    "Örneğin ResNeXt-50 (32x4d) ifadesinde:\n",
    "- 32: cardinality (group sayısı)\n",
    "- 4d: her grubun taban genişliği (base width) gibi okunur\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf16dff",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Karşılaştırma (Net Farklar)\n",
    "\n",
    "| Blok | Main path omurgası | Temel amaç | Kritik fark |\n",
    "|---|---|---|---|\n",
    "| **Basic** | 3×3 → 3×3 | Basit residual öğrenme | 3×3'ler doğrudan geniş kanalda |\n",
    "| **Pre-Act** | (BN+ReLU) → Conv ... | Daha stabil eğitim | Aktivasyon/BN toplama öncesine taşınır |\n",
    "| **Bottleneck** | 1×1 → 3×3 → 1×1 | Hesabı azaltıp derinleşmek | 3×3 dar kanalda |\n",
    "| **ResNeXt** | 1×1 → **G-3×3** → 1×1 | Aynı bütçede daha iyi kapasite | 3×3 yerine grouped conv + cardinality |\n",
    "\n",
    "### Kısa sezgi\n",
    "- **Basic**: “kolay ve sağlam”  \n",
    "- **Pre-Act**: “aynı iskelet, daha iyi optimizasyon”  \n",
    "- **Bottleneck**: “3×3’ü ucuzlat”  \n",
    "- **ResNeXt**: “3×3’ü grup grup paralelleştir, kapasiteyi cardinality ile büyüt”\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9ef22",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Parametre / FLOPs Sezgisi (Kabaca)\n",
    "\n",
    "### 7.1) Standart 3×3 Conv parametre sayısı\n",
    "\\[\n",
    "\\#\\text{param} = 3\\cdot 3 \\cdot C_{in}\\cdot C_{out}\n",
    "\\]\n",
    "\n",
    "### 7.2) Grouped 3×3 Conv parametre sayısı\n",
    "Group sayısı \\(g\\) olursa:\n",
    "- Her grup \\(C_{in}/g\\) giriş kanalını ve \\(C_{out}/g\\) çıkış kanalını işler.\n",
    "\n",
    "\\[\n",
    "\\#\\text{param} = 3\\cdot 3 \\cdot \\frac{C_{in}}{g}\\cdot \\frac{C_{out}}{g}\\cdot g\n",
    "= 3\\cdot 3 \\cdot \\frac{C_{in} C_{out}}{g}\n",
    "\\]\n",
    "\n",
    "Yani parametre/FLOPs kabaca **\\(1/g\\)** oranında düşer.\n",
    "\n",
    "### 7.3) Bu ne sağlıyor?\n",
    "Aynı hesap/parametre bütçesinde:\n",
    "- ya width (kanal genişliği) bir miktar artırılabilir,\n",
    "- ya da daha fazla \"farklı dönüşüm\" elde edilir (cardinality artışı).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1bc75",
   "metadata": {},
   "source": [
    "\n",
    "## 8) PyTorch Referans Implementasyonları\n",
    "\n",
    "Aşağıdaki kodlar:\n",
    "- Basic Residual Block (v1)\n",
    "- Pre-Activation Basic Block (v2)\n",
    "- Bottleneck Block\n",
    "- ResNeXt Bottleneck (Grouped Conv)\n",
    "\n",
    "Notlar:\n",
    "- `downsample` otomatik olarak 1×1 projection + BN kullanır.\n",
    "- ResNeXt bloğunda `groups` parametresi cardinality’dir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv3x3(in_ch, out_ch, stride=1, groups=1, dilation=1):\n",
    "    return nn.Conv2d(\n",
    "        in_ch, out_ch, kernel_size=3, stride=stride,\n",
    "        padding=dilation, groups=groups, bias=False, dilation=dilation\n",
    "    )\n",
    "\n",
    "def conv1x1(in_ch, out_ch, stride=1):\n",
    "    return nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    r'''\n",
    "    ResNet-18/34 tarzı Basic Residual Block (v1):\n",
    "      Conv3x3 -> BN -> ReLU -> Conv3x3 -> BN -> (+skip) -> ReLU\n",
    "    '''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_ch, out_ch, stride=stride)\n",
    "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = conv3x3(out_ch, out_ch, stride=1)\n",
    "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(\n",
    "                conv1x1(in_ch, out_ch, stride=stride),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = out + identity\n",
    "        out = F.relu(out, inplace=True)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBasicBlock(nn.Module):\n",
    "    r'''\n",
    "    ResNet v2 (pre-activation) tarzı Basic Block:\n",
    "      BN -> ReLU -> Conv3x3 -> BN -> ReLU -> Conv3x3 -> (+skip)\n",
    "    Not: Toplamadan sonra ekstra ReLU kullanılmayabilir (v2 yaklaşımı).\n",
    "    '''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_ch)\n",
    "        self.conv1 = conv3x3(in_ch, out_ch, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = conv3x3(out_ch, out_ch, stride=1)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(\n",
    "                conv1x1(in_ch, out_ch, stride=stride),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = F.relu(out, inplace=True)\n",
    "\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(out)  # pre-activated\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = out + identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    r'''\n",
    "    ResNet-50/101/152 Bottleneck:\n",
    "      1x1 reduce -> BN -> ReLU\n",
    "      3x3       -> BN -> ReLU\n",
    "      1x1 expand -> BN\n",
    "      (+skip) -> ReLU\n",
    "    '''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_ch, bottleneck_ch, stride=1):\n",
    "        super().__init__()\n",
    "        out_ch = bottleneck_ch * self.expansion\n",
    "\n",
    "        self.conv1 = conv1x1(in_ch, bottleneck_ch)\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_ch)\n",
    "\n",
    "        self.conv2 = conv3x3(bottleneck_ch, bottleneck_ch, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm2d(bottleneck_ch)\n",
    "\n",
    "        self.conv3 = conv1x1(bottleneck_ch, out_ch)\n",
    "        self.bn3 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(\n",
    "                conv1x1(in_ch, out_ch, stride=stride),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = out + identity\n",
    "        out = F.relu(out, inplace=True)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNeXtBottleneck(nn.Module):\n",
    "    r'''\n",
    "    ResNeXt Bottleneck (Grouped Conv):\n",
    "      1x1 reduce -> BN -> ReLU\n",
    "      3x3 GROUPED -> BN -> ReLU\n",
    "      1x1 expand -> BN\n",
    "      (+skip) -> ReLU\n",
    "\n",
    "    Parametreler:\n",
    "      - groups: cardinality\n",
    "      - base_width: ResNeXt-50 32x4d gibi isimlerdeki '4' kısmı ile ilişkilidir.\n",
    "    '''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_ch, bottleneck_ch, stride=1, groups=32, base_width=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Bottleneck_ch = Bu blok asıl kaç kanallık veriyle çalışacak\n",
    "        # ResNeXt standardında iç genişlik ölçekleme (yaygın form):\n",
    "        # bottleneck_ch = 64 → “Bu bloğun beyni 64 kanallık”\n",
    "\n",
    "        width = int(bottleneck_ch * (base_width / 64.0)) * groups\n",
    "\n",
    "        # ResNeXt diyor ki:\n",
    "                ### “Ben bu 64 kanalı tek parça işlemeyeceğim.Parçalara böleceğim.”.İşte groups burada giriyor.\n",
    "\n",
    "        # Groups ::: Elimizdew 64 kanal varsa biz kanal sayısını gropus a bölerek hepsine ayrı ayrı 3x3 uyguluyoruz\n",
    "                ### Grup 1 → 16 kanal\n",
    "                ### Grup 2 → 16 kanal\n",
    "                ### Grup 3 → 16 kanal\n",
    "                ### rup 4 → 16 kanal\n",
    "                \n",
    "        # base_width ne? ::: “Her grubun KAÇ KANALLIK olmasını istiyorum?”\n",
    "                ### base_width = 4 ::: groups = 32  ========= 32 tane grup olacak.Her grup 4 kanallık olsun.İşte o formülün GERÇEK anlamı\n",
    "                    ###  width = bottleneck_ch * (base_width / 64) * groups\n",
    "\n",
    "\n",
    "        out_ch = bottleneck_ch * self.expansion\n",
    "\n",
    "        self.conv1 = conv1x1(in_ch, width)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.conv2 = conv3x3(width, width, stride=stride, groups=groups)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.conv3 = conv1x1(width, out_ch)\n",
    "        self.bn3 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(\n",
    "                conv1x1(in_ch, out_ch, stride=stride),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = out + identity\n",
    "        out = F.relu(out, inplace=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e24a2",
   "metadata": {},
   "source": [
    "\n",
    "### Hızlı Şekil Kontrolü\n",
    "\n",
    "Aşağıdaki test, blokların çıktı boyutlarını kontrol eder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f25c1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 56, 56]),\n",
       " torch.Size([2, 64, 56, 56]),\n",
       " torch.Size([2, 256, 56, 56]),\n",
       " torch.Size([2, 256, 56, 56]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "b1 = BasicBlock(64, 64, stride=1)\n",
    "p1 = PreActBasicBlock(64, 64, stride=1)\n",
    "bn = Bottleneck(64, 64, stride=1)            # out: 256\n",
    "rx = ResNeXtBottleneck(64, 64, stride=1, groups=32, base_width=4)  # out: 256\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_b1 = b1(x)\n",
    "    y_p1 = p1(x)\n",
    "    y_bn = bn(x)\n",
    "    y_rx = rx(x)\n",
    "\n",
    "y_b1.shape, y_p1.shape, y_bn.shape, y_rx.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3be2a",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Pratik Seçim Rehberi\n",
    "\n",
    "- **BasicBlock**: Küçük/orta derinlikte (18/34) güçlü baseline. Eğitim kolay.\n",
    "- **Pre-Act**: Çok derin mimarilerde eğitim stabilitesi için tercih edilebilir.\n",
    "- **Bottleneck**: 50+ katman hedefleniyorsa klasik seçim. Hesap verimliliği iyi.\n",
    "- **ResNeXt (Grouped Conv)**: Aynı bütçede daha yüksek kapasite istendiğinde iyi aday.  \n",
    "  - Cardinality (groups) artırımı, bazen width artırmaktan daha verimli olur.\n",
    "  - Inference tarafında grouped conv kernel verimliliği ve hedef donanım desteği göz önünde bulundurulur.\n",
    "\n",
    "---\n",
    "\n",
    "### Tek cümlelik özet\n",
    "ResNeXt bloğu, residual iskeleti koruyup Bottleneck’in merkezindeki 3×3 işlemi **grouped conv** yaparak kapasiteyi **cardinality** ekseninde ölçekler; bu, onu Basic/Pre-Act/Bottleneck’ten ayıran ana fikirdir.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
