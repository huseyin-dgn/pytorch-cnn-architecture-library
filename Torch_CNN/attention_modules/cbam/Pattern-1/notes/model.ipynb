{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9fc2082",
   "metadata": {},
   "source": [
    "## Adım 1-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76855d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, min_hidden: int = 4):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=True),\n",
    "        )\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.gate(self.mlp(self.avg(x)) + self.mlp(self.mx(x)))\n",
    "        return x * w\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "        p = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=p, bias=False)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        s = torch.cat([avg_map, max_map], dim=1)\n",
    "        w = self.gate(self.conv(s))\n",
    "        return x * w\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, sa_kernel: int = 7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
    "        self.sa = SpatialAttention(kernel_size=sa_kernel)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ca(x)\n",
    "        x = self.sa(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd77573",
   "metadata": {},
   "source": [
    "## Adım 2-) ChannelAttention’i sağlamlaştıralım: gate + act seçimi\n",
    "\n",
    "* Bazı yerlerde ReLU yerine SiLU daha stabil olabiliyor; ayrıca sigmoid/hardsigmoid seçimi mobil vs için işe yarar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttentionV1(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        act: str = \"relu\",          # \"relu\" | \"silu\"\n",
    "        gate: str = \"sigmoid\",      # \"sigmoid\" | \"hardsigmoid\"\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=bias)\n",
    "        self.act = nn.ReLU(inplace=True) if act == \"relu\" else nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=bias)\n",
    "\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        a = self.fc2(self.act(self.fc1(self.avg(x))))\n",
    "        m = self.fc2(self.act(self.fc1(self.mx(x))))\n",
    "        w = self.gate_fn(a + m)\n",
    "        return x * w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee5116",
   "metadata": {},
   "source": [
    "## Adım 3-) Avg/Max birleşimini “öğrenilebilir” yapalım (fusion)\n",
    "\n",
    "* Şu an avg+max “sabit” bir kural. Bunu öğrenilebilir yaparsak bazı modellerde daha iyi olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c50448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttentionV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        act: str = \"relu\",\n",
    "        gate: str = \"sigmoid\",\n",
    "        fusion: str = \"softmax\",    # \"sum\" | \"softmax\"\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=bias)\n",
    "        self.act = nn.ReLU(inplace=True) if act == \"relu\" else nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=bias)\n",
    "\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "        self.fusion = fusion\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalı.\")\n",
    "\n",
    "        self.fusion_logits = nn.Parameter(torch.zeros(2))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        a = self.fc2(self.act(self.fc1(self.avg(x))))\n",
    "        m = self.fc2(self.act(self.fc1(self.mx(x))))\n",
    "\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            w2 = torch.softmax(self.fusion_logits, dim=0)\n",
    "            z = w2[0] * a + w2[1] * m\n",
    "\n",
    "        w = self.gate_fn(z)\n",
    "        return x * w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c08b07",
   "metadata": {},
   "source": [
    "## Adım 4-) SpatialAttention’i genişlet: kernel + farklı “pool” kombinasyonları"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05023fbc",
   "metadata": {},
   "source": [
    "* Bazı senaryolarda avg+max yerine farklı birleştirme (örn. sum veya concat) denenebilir. Biz standart concatı koruyup kontrol ekliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttentionV1(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: int = 7,\n",
    "        gate: str = \"sigmoid\",      # \"sigmoid\" | \"hardsigmoid\"\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "        p = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=p, bias=bias)\n",
    "\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        s = torch.cat([avg_map, max_map], dim=1)\n",
    "        w = self.gate_fn(self.conv(s))\n",
    "        return x * w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47b52b",
   "metadata": {},
   "source": [
    "## Adım 4 — CBAM’e “residual scaling” ekleyelim\n",
    "Bu, attention’ı “tam çarpma” yerine daha stabil hale getirir:\n",
    "\n",
    "* Normal: x * w\n",
    "\n",
    "* Residual: x * (1 + alpha * w)\n",
    "\n",
    "alpha küçükse, model bozulmadan attention öğrenmeye başlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAMV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        sa_kernel: int = 7,\n",
    "        ca_act: str = \"relu\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        sa_gate: str = \"sigmoid\",\n",
    "        ca_fusion: str = \"softmax\",     # \"sum\" | \"softmax\"\n",
    "        residual: bool = True,\n",
    "        alpha_init: float = 1.0,\n",
    "        learnable_alpha: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttentionV2(\n",
    "            channels, reduction=reduction, act=ca_act, gate=ca_gate, fusion=ca_fusion\n",
    "        )\n",
    "        self.sa = SpatialAttentionV1(kernel_size=sa_kernel, gate=sa_gate)\n",
    "\n",
    "        self.residual = residual\n",
    "        if residual:\n",
    "            if learnable_alpha:\n",
    "                self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
    "            else:\n",
    "                self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.ca(x)\n",
    "        y = self.sa(y)\n",
    "        if self.residual:\n",
    "            return x + self.alpha * (y - x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bab36",
   "metadata": {},
   "source": [
    "## Adım 6-) “Çıktıyla beraber maskeleri de döndürelim” (debug/analiz için)\n",
    "\n",
    "Training’de kapalı tutarız, ama görselleştirme/analiz için çok iyi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAMV3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        sa_kernel: int = 7,\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.return_maps = return_maps\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "        hidden = max(4, channels // reduction)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "\n",
    "        if sa_kernel % 2 == 0:\n",
    "            sa_kernel += 1\n",
    "        p = sa_kernel // 2\n",
    "        self.sa_conv = nn.Conv2d(2, 1, kernel_size=sa_kernel, padding=p, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        ca_logits = self.fc2(self.act(self.fc1(self.avg(x)))) + self.fc2(self.act(self.fc1(self.mx(x))))\n",
    "        ca = torch.sigmoid(ca_logits)\n",
    "        y = x * ca\n",
    "\n",
    "        avg_map = torch.mean(y, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(y, dim=1, keepdim=True)\n",
    "        sa = torch.sigmoid(self.sa_conv(torch.cat([avg_map, max_map], dim=1)))\n",
    "        z = y * sa\n",
    "\n",
    "        if self.return_maps:\n",
    "            return z, ca, sa\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e5a1c",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "----\n",
    "\n",
    "# Şimdi ise yukarıda 6 adımı tek bir modele entegre edelim...\n",
    "\n",
    "----\n",
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a6f9c",
   "metadata": {},
   "source": [
    "## CBAM - MODEL - ATTENTİON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73cb0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 64, 56, 56]) y: torch.Size([2, 64, 56, 56]) ca: torch.Size([2, 64, 1, 1]) sa: torch.Size([2, 1, 56, 56]) fusion_w: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        act: str = \"relu\",              # \"relu\" | \"silu\"\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        fusion: str = \"softmax\",        # \"sum\" | \"softmax\"\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=bias)\n",
    "        self.act = nn.ReLU(inplace=True) if act == \"relu\" else nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=bias)\n",
    "\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "        self.fusion = fusion\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalı.\")\n",
    "\n",
    "        self.fusion_logits = nn.Parameter(torch.zeros(2))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        a = self.fc2(self.act(self.fc1(self.avg(x))))\n",
    "        m = self.fc2(self.act(self.fc1(self.mx(x))))\n",
    "\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "            w2 = None\n",
    "        else:\n",
    "            w2 = torch.softmax(self.fusion_logits, dim=0)\n",
    "            z = w2[0] * a + w2[1] * m\n",
    "\n",
    "        ca = self.gate_fn(z)\n",
    "        y = x * ca\n",
    "        return y, ca, w2\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: int = 7,\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "        p = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=p, bias=bias)\n",
    "\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        s = torch.cat([avg_map, max_map], dim=1)\n",
    "        sa = self.gate_fn(self.conv(s))\n",
    "        y = x * sa\n",
    "        return y, sa\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        ca_act: str = \"relu\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        ca_fusion: str = \"softmax\",     # \"sum\" | \"softmax\"\n",
    "        sa_kernel: int = 7,\n",
    "        sa_gate: str = \"sigmoid\",\n",
    "        residual: bool = True,\n",
    "        alpha_init: float = 1.0,\n",
    "        learnable_alpha: bool = False,\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.return_maps = return_maps\n",
    "        self.residual = residual\n",
    "\n",
    "        self.ca = ChannelAttention(\n",
    "            channels=channels,\n",
    "            reduction=reduction,\n",
    "            min_hidden=min_hidden,\n",
    "            act=ca_act,\n",
    "            gate=ca_gate,\n",
    "            fusion=ca_fusion,\n",
    "        )\n",
    "        self.sa = SpatialAttention(kernel_size=sa_kernel, gate=sa_gate)\n",
    "\n",
    "        if residual:\n",
    "            if learnable_alpha:\n",
    "                self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
    "            else:\n",
    "                self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y, ca, fusion_w = self.ca(x)\n",
    "        y, sa = self.sa(y)\n",
    "\n",
    "        if self.residual:\n",
    "            out = x + self.alpha * (y - x)\n",
    "        else:\n",
    "            out = y\n",
    "\n",
    "        if self.return_maps:\n",
    "            return out, ca, sa, fusion_w\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(2, 64, 56, 56)\n",
    "    m = CBAM(\n",
    "        channels=64,\n",
    "        reduction=16,\n",
    "        min_hidden=4,\n",
    "        ca_act=\"silu\",\n",
    "        ca_gate=\"sigmoid\",\n",
    "        ca_fusion=\"softmax\",\n",
    "        sa_kernel=7,\n",
    "        sa_gate=\"sigmoid\",\n",
    "        residual=True,\n",
    "        alpha_init=1.0,\n",
    "        learnable_alpha=True,\n",
    "        return_maps=True,\n",
    "    )\n",
    "    y, ca, sa, fw = m(x)\n",
    "    print(\"x:\", x.shape, \"y:\", y.shape, \"ca:\", ca.shape, \"sa:\", sa.shape, \"fusion_w:\", None if fw is None else fw.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd76e1",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852414ce",
   "metadata": {},
   "source": [
    "# Adım 1 — Residual CBAM (Alpha ile)\n",
    "### Ne problemi çözüyor?\n",
    "\n",
    "Klasik CBAM:\n",
    "\n",
    "* out = x * mask yaptığı için,\n",
    "\n",
    "- attention daha baştan agresif davranırsa bazı kanalları/konumları gereğinden fazla baskılayıp eğitimi bozabilir.\n",
    "\n",
    "- Özellikle detection/segmentation gibi büyük sistemlerde “train instability” ve “performans dalgalanması” yaratabilir.\n",
    "\n",
    "Residual form:\n",
    "\n",
    "- CBAM etkisini “yumuşatır” ve modeli başta identity’e yakın tutar.\n",
    "\n",
    "* Attention öğrenirken backbone’u kırmaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4d07434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, min_hidden: int = 4):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=True),\n",
    "        )\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.gate(self.mlp(self.avg(x)) + self.mlp(self.mx(x)))\n",
    "        return x * w\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "        p = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=p, bias=False)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        s = torch.cat([avg_map, max_map], dim=1)\n",
    "        w = self.gate(self.conv(s))\n",
    "        return x * w\n",
    "\n",
    "class CBAMResidual(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        sa_kernel: int = 7,\n",
    "        alpha_init: float = 1.0,\n",
    "        learnable_alpha: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
    "        self.sa = SpatialAttention(kernel_size=sa_kernel)\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
    "        else:\n",
    "            self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.ca(x)\n",
    "        y = self.sa(y)\n",
    "        return x + self.alpha * (y - x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52eaca",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500c316",
   "metadata": {},
   "source": [
    "## Adım 2 — Learnable Fusion + Residual CBAM\n",
    "\n",
    "Bu adımın çözmeye çalıştığı problem şu:\n",
    "\n",
    "* Klasik CBAM’de avg ve max daima eşit ağırlıkla toplanıyor.\n",
    "\n",
    "* Ama bazı katmanlarda avg daha faydalı (genel bağlam), bazı katmanlarda max daha faydalı (pik/nesne sinyali).\n",
    "\n",
    "* Sabit toplama bazen maskeyi “yanlış yönde” sertleştirebiliyor.\n",
    "\n",
    "Bu yüzden fusion’ı iki modlu yapıyoruz:\n",
    "\n",
    "* fusion=\"sum\" → klasik davranış\n",
    "\n",
    "* fusion=\"softmax\" → öğrenilebilir ağırlık ile avg/max karışımı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce759fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttentionFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",   # \"sum\" | \"softmax\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert fusion in (\"sum\", \"softmax\")\n",
    "        self.fusion = fusion\n",
    "\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "        if fusion == \"softmax\":\n",
    "            self.fusion_logits = nn.Parameter(torch.zeros(2))\n",
    "        else:\n",
    "            self.fusion_logits = None\n",
    "\n",
    "    def _mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        a = self._mlp(self.avg(x))\n",
    "        m = self._mlp(self.mx(x))\n",
    "\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            w2 = torch.softmax(self.fusion_logits, dim=0)  # (2,)\n",
    "            z = w2[0] * a + w2[1] * m\n",
    "\n",
    "        w = self.gate(z)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "        p = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=p, bias=False)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        s = torch.cat([avg_map, max_map], dim=1)\n",
    "        w = self.gate(self.conv(s))\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class CBAMResidualV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        sa_kernel: int = 7,\n",
    "        ca_fusion: str = \"softmax\",   # \"sum\" | \"softmax\"\n",
    "        alpha_init: float = 1.0,\n",
    "        learnable_alpha: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttentionFusion(\n",
    "            channels=channels,\n",
    "            reduction=reduction,\n",
    "            min_hidden=min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "        )\n",
    "        self.sa = SpatialAttention(kernel_size=sa_kernel)\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
    "        else:\n",
    "            self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.ca(x)\n",
    "        y = self.sa(y)\n",
    "        return x + self.alpha * (y - x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f009f6",
   "metadata": {},
   "source": [
    "#### Ne değişti? (kodda net fark)\n",
    "\n",
    "Klasik CA:\n",
    "```python\n",
    "z = mlp(avg) + mlp(max)\n",
    "```\n",
    "\n",
    "\n",
    "Yeni CA:\n",
    "```python\n",
    "w2 = softmax([p1, p2])\n",
    "z = w2[0]*mlp(avg) + w2[1]*mlp(max)\n",
    "```\n",
    "\n",
    "\n",
    "Bu w2 iki parametreyle öğreniliyor:\n",
    "\n",
    "* Bir katmanda avg daha iyi ise, ağırlık avg tarafına kayıyor.\n",
    "\n",
    "* Başka bir katmanda max daha iyi ise, max tarafına kayıyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dba1c",
   "metadata": {},
   "source": [
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186cf51",
   "metadata": {},
   "source": [
    "\n",
    "## Adım 3 — Temperature + Gate + Residual + Fusion\n",
    "Bu adımın çözdüğü problemler:\n",
    "\n",
    "* Mask çok keskinleşip (0’a yakın / 1’e yakın) feature’ları boğabiliyor → eğitim dalgalanır.\n",
    "\n",
    "* Bazı modellerde sigmoid “yumuşak” kalıyor, bazılarında tam tersi “aşırı agresif” oluyor.\n",
    "\n",
    "* Özellikle detection/segmentation’da bu “stability” meselesi sık görülür.\n",
    "\n",
    "Çözüm:\n",
    "\n",
    "* Gate’i seçilebilir yap (sigmoid / hardsigmoid)\n",
    "\n",
    "Maskeyi üretmeden önce logit’i temperature ile ölçekle:\n",
    "\n",
    "* T > 1 → daha yumuşak maske\n",
    "\n",
    "* T < 1 → daha keskin maske\n",
    "\n",
    "* İstersek T learnable da olabilir.\n",
    "\n",
    "Aşağıda Adım 2’nin üstüne eklenmiş hali mecvut.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b319c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",        # \"sum\" | \"softmax\"\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert fusion in (\"sum\", \"softmax\")\n",
    "        assert temperature > 0\n",
    "        self.fusion = fusion\n",
    "        self.eps = eps\n",
    "\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "        if fusion == \"softmax\":\n",
    "            self.fusion_logits = nn.Parameter(torch.zeros(2))\n",
    "        else:\n",
    "            self.fusion_logits = None\n",
    "\n",
    "        self.learnable_temperature = learnable_temperature\n",
    "        if learnable_temperature:\n",
    "            t_raw = torch.tensor(float(temperature))\n",
    "            t_inv = torch.log(torch.exp(t_raw) - 1.0 + eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def _get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def _mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        a = self._mlp(self.avg(x))\n",
    "        m = self._mlp(self.mx(x))\n",
    "\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            w2 = torch.softmax(self.fusion_logits, dim=0)\n",
    "            z = w2[0] * a + w2[1] * m\n",
    "\n",
    "        T = self._get_T()\n",
    "        w = self.gate_fn(z / T)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class SpatialAttentionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: int = 7,\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert temperature > 0\n",
    "        self.eps = eps\n",
    "\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "        p = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=p, bias=False)\n",
    "\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "        self.learnable_temperature = learnable_temperature\n",
    "        if learnable_temperature:\n",
    "            t_raw = torch.tensor(float(temperature))\n",
    "            t_inv = torch.log(torch.exp(t_raw) - 1.0 + eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def _get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        s = torch.cat([avg_map, max_map], dim=1)\n",
    "\n",
    "        z = self.conv(s)\n",
    "        T = self._get_T()\n",
    "        w = self.gate_fn(z / T)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class CBAMResidualV3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        sa_kernel: int = 7,\n",
    "        ca_fusion: str = \"softmax\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        sa_gate: str = \"sigmoid\",\n",
    "        ca_temperature: float = 1.0,\n",
    "        sa_temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        alpha_init: float = 1.0,\n",
    "        learnable_alpha: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=reduction,\n",
    "            min_hidden=min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "        )\n",
    "        self.sa = SpatialAttentionT(\n",
    "            kernel_size=sa_kernel,\n",
    "            gate=sa_gate,\n",
    "            temperature=sa_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "        )\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
    "        else:\n",
    "            self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.ca(x)\n",
    "        y = self.sa(y)\n",
    "        return x + self.alpha * (y - x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05416f45",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b1f87",
   "metadata": {},
   "source": [
    "## Adım 4 -) Dynamic Spatial CBAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc42ba0",
   "metadata": {},
   "source": [
    "#### 1) Bu sürümde CBAM’in temel akışı ne?\n",
    "\n",
    "Hâlâ aynı sıralama var:\n",
    "\n",
    "* Channel Attention (CA)\n",
    "\n",
    "* Spatial Attention (SA)\n",
    "\n",
    "* Residual karışım"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6345d3",
   "metadata": {},
   "source": [
    "#### 2) Channel Attention tarafında neler var?\n",
    "###### 2.1 Avg + Max Squeeze\n",
    "\n",
    "Kanal önemini çıkarırken iki tür global özet alıyoruz:\n",
    "\n",
    "* avgpool → genel enerji\n",
    "\n",
    "* maxpool → pik enerji\n",
    "\n",
    "##### 2.2 Fusion (sum / softmax)\n",
    "\n",
    "* Klasikte avg+max sabit toplama.\n",
    "Biz şunu ekledik:\n",
    "\n",
    "* sum: klasik\n",
    "\n",
    "* softmax: model iki ağırlık öğreniyor → avg mi max mı daha önemli?\n",
    "\n",
    "* Bu sayede her blok kendi ihtiyacına göre “avg ağırlıklı” veya “max ağırlıklı” davranabiliyor.\n",
    "\n",
    "##### 2.3 Temperature + Gate\n",
    "\n",
    "Maskeyi üretmeden önce logit’i T ile ölçekliyoruz:\n",
    "\n",
    "* T > 1: daha yumuşak (stabil)\n",
    "\n",
    "* T < 1: daha keskin\n",
    "\n",
    "Gate olarak da:\n",
    "\n",
    "* sigmoid veya\n",
    "\n",
    "* hardsigmoid (daha hızlı)\n",
    "\n",
    "seçebiliyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b0077",
   "metadata": {},
   "source": [
    "#### 3) Dynamic Spatial Attention ne demek?\n",
    "\n",
    "Klasik CBAM’de spatial attention şu:\n",
    "\n",
    "* avg_map + max_map çıkar\n",
    "\n",
    "* tek bir Conv2d(2→1, kernel=7) ile maske üret\n",
    "\n",
    "* Bu “tek ölçek” demek.\n",
    "\n",
    "Dynamic SA’de yaptığımız şey:\n",
    "\n",
    "* Birden fazla spatial branch tanımladık:\n",
    "\n",
    ">Branch 1: 3×3 (lokal detay)\n",
    "\n",
    ">Branch 2: 7×7 (orta ölçek bağlam)\n",
    "\n",
    ">Branch 3 (opsiyonel): 7×7 dilated (d=2) (geniş bağlam / pseudo-global)\n",
    "\n",
    "* Her branch aynı inputtan mask logiti üretir:\n",
    "\n",
    ">z1, z2, z3 gibi.\n",
    "\n",
    "* Sonra bir “router” ile bu branch’leri ağırlıklandırıyoruz:\n",
    "\n",
    ">Router inputu: SA inputunun kendisi (avg_map + max_map, yani (B,2,H,W))\n",
    "\n",
    ">Router outputu: her branch için ağırlıklar (B, num_branches)\n",
    "\n",
    "* Bu yüzden “dinamik”:\n",
    "\n",
    ">Bazı görüntülerde küçük detay önemli → 3×3 ağırlığı artar\n",
    "\n",
    ">Bazı görüntülerde büyük bağlam önemli → dilated veya 7×7 ağırlığı artar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a681533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 56, 56]) torch.Size([2, 64, 1, 1]) torch.Size([2, 1, 56, 56]) torch.Size([2]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",        # \"sum\" | \"softmax\"\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        act: str = \"relu\",              # \"relu\" | \"silu\"\n",
    "        bias: bool = True,\n",
    "        return_fusion_weights: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalı.\")\n",
    "        if gate.lower() not in (\"sigmoid\", \"hardsigmoid\"):\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if act not in (\"relu\", \"silu\"):\n",
    "            raise ValueError(\"act 'relu' veya 'silu' olmalı.\")\n",
    "\n",
    "        self.fusion = fusion\n",
    "        self.return_fusion_weights = return_fusion_weights\n",
    "        self.eps = eps\n",
    "\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=bias)\n",
    "        self.act = nn.ReLU(inplace=True) if act == \"relu\" else nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=bias)\n",
    "\n",
    "        if gate.lower() == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        else:\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "\n",
    "        if fusion == \"softmax\":\n",
    "            self.fusion_logits = nn.Parameter(torch.zeros(2))\n",
    "        else:\n",
    "            self.fusion_logits = None\n",
    "\n",
    "        self.learnable_temperature = learnable_temperature\n",
    "        if learnable_temperature:\n",
    "            t_raw = torch.tensor(float(temperature))\n",
    "            t_inv = torch.log(torch.exp(t_raw) - 1.0 + eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def _get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def _mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        a = self._mlp(self.avg(x))\n",
    "        m = self._mlp(self.mx(x))\n",
    "\n",
    "        fusion_w = None\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            fusion_w = torch.softmax(self.fusion_logits, dim=0)  # (2,)\n",
    "            z = fusion_w[0] * a + fusion_w[1] * m\n",
    "\n",
    "        T = self._get_T()\n",
    "        ca = self.gate_fn(z / T)  # (B,C,1,1)\n",
    "        y = x * ca\n",
    "\n",
    "        if self.return_fusion_weights and (fusion_w is not None):\n",
    "            return y, ca, fusion_w\n",
    "        return y, ca\n",
    "\n",
    "\n",
    "class DynamicSpatialAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernels=(3, 7),\n",
    "        use_dilated: bool = True,\n",
    "        dilated_kernel: int = 7,\n",
    "        dilated_d: int = 2,\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        router_hidden: int = 8,\n",
    "        bias: bool = True,\n",
    "        return_router_weights: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if gate.lower() not in (\"sigmoid\", \"hardsigmoid\"):\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "        if router_hidden < 1:\n",
    "            raise ValueError(\"router_hidden >= 1 olmalı.\")\n",
    "\n",
    "        self.eps = eps\n",
    "        self.return_router_weights = return_router_weights\n",
    "\n",
    "        ks = []\n",
    "        for k in kernels:\n",
    "            k = int(k)\n",
    "            if k % 2 == 0:\n",
    "                k += 1\n",
    "            if k < 1:\n",
    "                raise ValueError(\"kernel_size >= 1 olmalı.\")\n",
    "            ks.append(k)\n",
    "\n",
    "        self.branches = nn.ModuleList()\n",
    "        for k in ks:\n",
    "            p = k // 2\n",
    "            self.branches.append(nn.Conv2d(2, 1, kernel_size=k, padding=p, bias=False))\n",
    "\n",
    "        if use_dilated:\n",
    "            k = int(dilated_kernel)\n",
    "            if k % 2 == 0:\n",
    "                k += 1\n",
    "            if dilated_d < 1:\n",
    "                raise ValueError(\"dilated_d >= 1 olmalı.\")\n",
    "            p = dilated_d * (k - 1) // 2\n",
    "            self.branches.append(\n",
    "                nn.Conv2d(2, 1, kernel_size=k, padding=p, dilation=dilated_d, bias=False)\n",
    "            )\n",
    "\n",
    "        self.num_branches = len(self.branches)\n",
    "\n",
    "        if gate.lower() == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        else:\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "\n",
    "        self.learnable_temperature = learnable_temperature\n",
    "        if learnable_temperature:\n",
    "            t_raw = torch.tensor(float(temperature))\n",
    "            t_inv = torch.log(torch.exp(t_raw) - 1.0 + eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "        self.router = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(2, router_hidden, 1, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(router_hidden, self.num_branches, 1, bias=bias),\n",
    "        )\n",
    "\n",
    "    def _get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        s = torch.cat([avg_map, max_map], dim=1)  # (B,2,H,W)\n",
    "\n",
    "        logits = self.router(s).flatten(1)             # (B,K)\n",
    "        rw = torch.softmax(logits, dim=1)              # (B,K)\n",
    "\n",
    "        z = torch.stack([br(s) for br in self.branches], dim=1)  # (B,K,1,H,W)\n",
    "        wlogit = (rw[:, :, None, None, None] * z).sum(dim=1)     # (B,1,H,W)\n",
    "\n",
    "        T = self._get_T()\n",
    "        sa = self.gate_fn(wlogit / T)  # (B,1,H,W)\n",
    "        y = x * sa\n",
    "\n",
    "        if self.return_router_weights:\n",
    "            return y, sa, rw\n",
    "        return y, sa\n",
    "\n",
    "\n",
    "class CBAMResidualDynamicSA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        ca_fusion: str = \"softmax\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        ca_temperature: float = 1.0,\n",
    "        ca_act: str = \"relu\",\n",
    "        sa_gate: str = \"sigmoid\",\n",
    "        sa_temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        sa_kernels=(3, 7),\n",
    "        sa_use_dilated: bool = True,\n",
    "        sa_dilated_kernel: int = 7,\n",
    "        sa_dilated_d: int = 2,\n",
    "        sa_router_hidden: int = 8,\n",
    "        residual: bool = True,\n",
    "        alpha_init: float = 1.0,\n",
    "        learnable_alpha: bool = False,\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.return_maps = return_maps\n",
    "        self.residual = residual\n",
    "\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=reduction,\n",
    "            min_hidden=min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            act=ca_act,\n",
    "            return_fusion_weights=return_maps,\n",
    "        )\n",
    "\n",
    "        self.sa = DynamicSpatialAttention(\n",
    "            kernels=sa_kernels,\n",
    "            use_dilated=sa_use_dilated,\n",
    "            dilated_kernel=sa_dilated_kernel,\n",
    "            dilated_d=sa_dilated_d,\n",
    "            gate=sa_gate,\n",
    "            temperature=sa_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            router_hidden=sa_router_hidden,\n",
    "            return_router_weights=return_maps,\n",
    "        )\n",
    "\n",
    "        if residual:\n",
    "            if learnable_alpha:\n",
    "                self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
    "            else:\n",
    "                self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.return_maps:\n",
    "            y, ca, fusion_w = self.ca(x)\n",
    "            y, sa, router_w = self.sa(y)\n",
    "            out = x + self.alpha * (y - x) if self.residual else y\n",
    "            return out, ca, sa, fusion_w, router_w\n",
    "\n",
    "        y, _ = self.ca(x)\n",
    "        y, _ = self.sa(y)\n",
    "        out = x + self.alpha * (y - x) if self.residual else y\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(2, 64, 56, 56)\n",
    "    m = CBAMResidualDynamicSA(\n",
    "        channels=64,\n",
    "        reduction=16,\n",
    "        ca_fusion=\"softmax\",\n",
    "        ca_gate=\"sigmoid\",\n",
    "        ca_temperature=1.0,\n",
    "        ca_act=\"silu\",\n",
    "        sa_gate=\"sigmoid\",\n",
    "        sa_temperature=1.0,\n",
    "        sa_kernels=(3, 7),\n",
    "        sa_use_dilated=True,\n",
    "        sa_dilated_kernel=7,\n",
    "        sa_dilated_d=2,\n",
    "        sa_router_hidden=8,\n",
    "        residual=True,\n",
    "        learnable_alpha=True,\n",
    "        alpha_init=1.0,\n",
    "        learnable_temperature=False,\n",
    "        return_maps=True,\n",
    "    )\n",
    "    out, ca, sa, fusion_w, router_w = m(x)\n",
    "    print(out.shape, ca.shape, sa.shape, fusion_w.shape, router_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3e280",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Her class ın açıklamaları klasör içerisindeki .ipynb dosyalarında mevcuttu.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4325c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "466474af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1caf6863",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
