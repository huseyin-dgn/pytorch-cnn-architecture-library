{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9662f8",
   "metadata": {},
   "source": [
    "\n",
    "# DynamicSpatialAttention — Satır Satır, Parametre Parametre İnceleme\n",
    "\n",
    "Bu notebook, aşağıdaki `DynamicSpatialAttention` modülünün **en ince detayına kadar** açıklamasını içerir.\n",
    "\n",
    "Modülün temel amacı:\n",
    "- Girdi feature map üzerinde **nerelere bakılacağını** belirleyen bir **spatial attention maskesi** üretmek.\n",
    "- Klasik CBAM-Spatial gibi `avg_map` + `max_map` oluşturur.\n",
    "- Tek bir 7×7 conv yerine **çoklu branch (farklı kernel/dilation)** kullanır.\n",
    "- Hangi branch’in daha uygun olduğuna **router** karar verir (input’a bağlı softmax ağırlıkları).\n",
    "- Sonunda `sigmoid/hardsigmoid` ile 0–1 arası maske üretip `x` ile çarpar.\n",
    "\n",
    "---\n",
    "\n",
    "## İncelenen Kod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5433cf",
   "metadata": {},
   "source": [
    "* Spatial Attention’ı kafada oturtan 5 cümle (çekirdek)\n",
    "\n",
    "* Kanalı ez, konumu çıkar: avg_map ve max_map ile (B,1,H,W) iki harita üret.\n",
    "\n",
    "* Bunları üst üste koy: s = cat([avg_map, max_map]) → (B,2,H,W)\n",
    "\n",
    "* K farklı gözle bak: branches ile s’den K tane mask-logit üret → her biri (B,1,H,W)\n",
    "\n",
    "* Router karar versin: router s’ye bakıp her örnek için rw ağırlıklarını üretir (B,K)\n",
    "\n",
    "* Karıştır ve uygula: K logiti rw ile ağırlıklı topla, sonra sigmoid(wlogit/T) ile maskeyi üret, x*sa yap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc5244",
   "metadata": {},
   "source": [
    "**Channel attention’da MLP şarttır çünkü kanallar arası ilişkiyi başka türlü öğrenemezsin; spatial attention’da ise uzamsal ilişkiyi zaten convolution öğrendiği için ekstra bir MLP’ye ihtiyaç yoktur.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623491f7",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## Kod özeti : Genel Tanım \n",
    "\n",
    "**Önce parametreleri alıyoruz; kernels normal branch sayısını ve kernel boyutlarını belirliyor, router_hidden ise router içindeki ara kanal genişliği. Init içinde sıcaklık, gate türü ve router_hidden gibi değerler için kontrolleri yapıyoruz. Ardından kernels içindeki değerleri integer’a çevirip çiftse tek yaparak güvenli bir ks listesi oluşturuyoruz. Bu ks üzerinden Conv2d(2→1, k×k) şeklinde normal branch’leri ModuleList içine ekliyoruz; use_dilated=True ise ek olarak bir tane dilated branch (ör. kernel 7, dilation 2) daha branches listesine ekleniyor. Sonra gate fonksiyonunu (sigmoid/hardsigmoid) seçiyoruz. Temperature learnable olacaksa, T=softplus(t_raw)+eps pozitif kalsın diye t_raw’ı nn.Parameter yapıyoruz ve başlangıç değeri temperature olsun diye softplus’ın tersini log(exp(T)-1+eps) formülüyle ayarlıyoruz; learnable değilse sabit temperature’ı register_buffer(\"T\", ...) ile modele bağlıyoruz. Ardından router’ı kuruyoruz: s haritasını önce global average pooling ile (B,2,1,1) özetleyip 1×1 conv + ReLU + 1×1 conv ile (B,K,1,1) logits üretiyoruz; burada K=num_branches. Forward’da önce x üzerinden kanal boyunca ortalama ve maksimum haritaları çıkarıp s=(B,2,H,W) elde ediyoruz. Router s’den logits üretip softmax ile rw=(B,K) branch ağırlıklarını çıkarıyor. Aynı s’yi tüm branch conv’lardan geçirip her biri (B,1,H,W) olacak şekilde K adet maske-logit üretip stack ile z=(B,K,1,H,W) halinde topluyoruz. Sonra rw’yi broadcast edebilmek için (B,K) → (B,K,1,1,1) yapıp rw*z hesaplıyor ve sum(dim=1) ile K branch’i tek haritaya indirerek wlogit=(B,1,H,W) elde ediyoruz. Son olarak wlogit/T ile ölçekleyip gate fonksiyonundan geçirerek sa=(B,1,H,W) spatial attention maskesi üretiyoruz ve y = x * sa ile maskeyi tüm kanallara uyguluyoruz; istenirse debug için rw de döndürülebiliyor.**\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DynamicSpatialAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernels=(3, 7),\n",
    "        use_dilated: bool = True,\n",
    "        dilated_kernel: int = 7,\n",
    "        dilated_d: int = 2,\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        router_hidden: int = 8,\n",
    "        bias: bool = True,\n",
    "        return_router_weights: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if gate.lower() not in (\"sigmoid\", \"hardsigmoid\"):\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "        if router_hidden < 1:\n",
    "            raise ValueError(\"router_hidden >= 1 olmalı.\")\n",
    "\n",
    "        self.eps = eps\n",
    "        self.return_router_weights = return_router_weights\n",
    "\n",
    "        # aslında burdaki amaç kernelların daha rahat işlemesini ve daha güvenli\n",
    "        # işlemesini sağlamak.\n",
    "        # Meseka k tek olmak zorunda ve 1 den büyük olmak zorunda.\n",
    "        # Eğer bunları sağlıyorsa kernel diye bunları buraya koyabiliyoruz\n",
    "\n",
    "        ks = []\n",
    "        for k in kernels:\n",
    "            k = int(k)\n",
    "            if k % 2 == 0:\n",
    "                k += 1\n",
    "            if k < 1:\n",
    "                raise ValueError(\"kernel_size >= 1 olmalı.\")\n",
    "            ks.append(k)\n",
    "\n",
    "        self.branches = nn.ModuleList()\n",
    "         ## branches’in yaptığı iş: “K tane farklı yöntemle ‘buraya bak’ haritası üretmek.”\n",
    "         ## ModuleList = “Bu listedeki katmanlar modelin parçasıdır, unutma.”\n",
    "        for k in ks:\n",
    "            p = k // 2\n",
    "            self.branches.append(nn.Conv2d(2, 1, kernel_size=k, padding=p, bias=False))\n",
    "\n",
    "        if use_dilated:\n",
    "            k = int(dilated_kernel)\n",
    "            if k % 2 == 0:\n",
    "                k += 1\n",
    "            if dilated_d < 1:\n",
    "                raise ValueError(\"dilated_d >= 1 olmalı.\")\n",
    "            p = dilated_d * (k - 1) // 2\n",
    "            self.branches.append(\n",
    "                nn.Conv2d(2, 1, kernel_size=k, padding=p, dilation=dilated_d, bias=False)\n",
    "            )\n",
    "\n",
    "        self.num_branches = len(self.branches)\n",
    "\n",
    "        if gate.lower() == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        else:\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "\n",
    "        self.learnable_temperature = learnable_temperature\n",
    "        if learnable_temperature:\n",
    "            t_raw = torch.tensor(float(temperature))\n",
    "            t_inv = torch.log(torch.exp(t_raw) - 1.0 + eps)\n",
    "            self.t_raw = nn.Parameter(t_inv) ## “t_raw değerini modelin öğrenebileceği bir ağırlık (parametre) haline getir.”\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    # REGİSTER BUFFER İÇERİSİNDEKİ T DEĞERİ ÖYLESİNE BİR DEĞER.YANİ ORAYA A DA YAZABİLİRSİN.AMA O ZAMAN AŞAĞIDAKİ GET_T FONKSİYONUNDAKİ RETURN Ü SELF.A OLARAK DEĞİİTİRMEN GEREKECEK\n",
    "\n",
    "    # nn.Parameter → öğrenilecek\n",
    "\n",
    "    # register_buffer → öğrenilmeyecek ama modelle birlikte taşınacak/kaydedilecek\n",
    "\n",
    "        self.router = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(2, router_hidden, 1, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(router_hidden, self.num_branches, 1, bias=bias),\n",
    "        )\n",
    "    # router_hidden, router’ın karar vermeden önce 2 kanallı bilgiyi biraz işleyip \n",
    "    # daha anlamlı hale getirmesini sağlayan ara temsil boyutudur; \n",
    "    # bu sayede branch seçimi basit lineer değil, daha esnek ve güçlü olur.\n",
    "\n",
    "    # “Hangi convolution çıktısı ne kadar dinlenecek?” ağırlığı.\n",
    "\n",
    "    def _get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "# NEDEN CHANNELS ATTENTİON DA 2 ADET KATMAN VARKEN BURDA HERHANGİ BİR KATMAN YOK ? \n",
    "# Channel attention’da kanallar arasında ilişki öğreniyorsun,\n",
    "# Spatial attention’da ise uzamsal ilişkiyi doğrudan convolution zaten öğreniyor\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True) # Her (H,W) konumu için kanalları ortalayıp tek harita çıkarıyor\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True) # “Bu konumda en güçlü sinyal hangi kanaldan gelirse gelsin, ne kadar güçlü?\n",
    "        s = torch.cat([avg_map, max_map], dim=1)  # (B,2,H,W)\n",
    "\n",
    "        logits = self.router(s).flatten(1)             # (B,K)\n",
    "        rw = torch.softmax(logits, dim=1)              # (B,K) # Örn: rw[b] = [0.6, 0.3, 0.1]\n",
    "\n",
    "# logits aslında router’dan çıkan ve softmax’a girecek ham skorlar; \n",
    "# router, s=(B,2,H,W) haritasını önce global havuzlama ile (B,2,1,1) özetine indiriyor,\n",
    "# ardından 1×1 katmanlarla bunu (B,K,1,1) olacak şekilde num_branches=K tane skora \n",
    "# çeviriyor;\n",
    "# bu skorlar flatten ile (B,K) yapılıyor ve softmax ile rw elde ediliyor, \n",
    "# böylece model her örnek için \n",
    "# “hangi kernel/branch daha iyi?” sorusuna yüzde olarak cevap veriyor.\n",
    "\n",
    "        # “Her branch conv’u al, aynı s input’una uygula.” Elde ettiğin şey:K tane ayrı ayrı (B,1,H,W) harita\n",
    "        z = torch.stack([br(s) for br in self.branches], dim=1)  # (B,K,1,H,W) # Örn: z[b,0] = 3×3 conv’un maskesi, z[b,1] = 7×7 maskesi, ...\n",
    "        # z, branches içindeki farklı kernel/dilation’a sahip conv katmanlarının\n",
    "        # her birinin s üzerinden ürettiği spatial logit haritalarının, \n",
    "        # branch boyutunda bir araya getirilmiş halidir.\n",
    "\n",
    "\n",
    "        wlogit = (rw[:, :, None, None, None] * z).sum(dim=1)     # (B,1,H,W)\n",
    "        #    rw şu an (B,K).\n",
    "        # Ama z ile çarpabilmek için rw’yi şu şekle getirmeliyiz\n",
    "        # (B,K) → (B,K,1,1,1) , Bu “None” eklemek şu demek:\n",
    "        # “Bu boyutlarda tek değer var, otomatik yay.”\n",
    "        #         rw_expanded * z\n",
    "        # rw_expanded: (B,K,1,1,1)\n",
    "        # z: (B,K,1,H,W)\n",
    "        # Sonuç:\n",
    "        # (B,K,1,H,W)\n",
    "        # .sum(dim=1)\n",
    "        # şunu yapar:\n",
    "        # “Branch boyutunu topla, K haritayı tek haritaya indir.”\n",
    "        # ===== rw * z her branch maskesini kendi ağırlığıyla ölçekler; \n",
    "        # .sum(dim=1) ise K branch’i \n",
    "        # tek bir (B,1,H,W) haritaya indirerek nihai spatial logit haritasını üretir.\n",
    "\n",
    "        T = self._get_T()\n",
    "        sa = self.gate_fn(wlogit / T)  # (B,1,H,W)\n",
    "\n",
    "\n",
    "        y = x * sa\n",
    "\n",
    "#         “Neden çarptık?” (y = x * sa)\n",
    "# Çünkü sa dediğin şey dikkat maskesi.\n",
    "\n",
    "# sa shape: (B, 1, H, W)\n",
    "# x shape: (B, C, H, W)\n",
    "\n",
    "# Çarpınca ne oluyor?\n",
    "# (B,1,H,W) maskesi tüm kanallara broadcast edilir.\n",
    "# Her (h,w) konumu için:\n",
    "# sa büyükse → o konumdaki tüm kanallar güçlenir / korunur\n",
    "# sa küçükse → o konumdaki tüm kanallar kısılır / bastırılır\n",
    "\n",
    "# Yani bu çarpma şunu yapıyor:\n",
    "# “Önemli yerleri geçir, önemsiz yerleri sustur.”\n",
    "\n",
    "# Bu bir gating (kapı) mekanizması.\n",
    "# Add (toplama) yapsaydın “bastırma” olmazdı; sadece bir şey eklemiş olurduk.\n",
    "# Attention’ın klasik uygulaması bu yüzden çarpmadır.\n",
    "\n",
    "        if self.return_router_weights:\n",
    "            return y, sa, rw\n",
    "        return y, sa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983aa9f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 1) Modül Ne Üretiyor? (Çıktılar ve Şekiller)\n",
    "\n",
    "Girdi:\n",
    "- `x`: `(B, C, H, W)`\n",
    "\n",
    "Çıktı:\n",
    "- `y`: `(B, C, H, W)`  → attention uygulanmış çıktı\n",
    "- `sa`: `(B, 1, H, W)` → spatial attention maskesi\n",
    "- opsiyonel `rw`: `(B, K)` → router branch ağırlıkları (her örnek için)\n",
    "\n",
    "Burada `K = num_branches`:\n",
    "- `kernels` listesindeki branch sayısı\n",
    "- `use_dilated=True` ise +1 dilated branch\n",
    "\n",
    "Örnek:\n",
    "- `kernels=(3,7)` ve `use_dilated=True` → K=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc547418",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 2) Parametreler: Ne İşe Yarar?\n",
    "\n",
    "## `kernels=(3,7)`\n",
    "- Normal branch’lerde kullanılacak kernel boyutları.\n",
    "- Her kernel için ayrı bir `Conv2d(2→1)` branch kurulur.\n",
    "- İçeride **tek sayıya zorlanır** (çiftse +1 yapılır).\n",
    "\n",
    "## `use_dilated=True`\n",
    "- Ek bir **dilated** branch ekler.\n",
    "\n",
    "## `dilated_kernel=7`, `dilated_d=2`\n",
    "- Dilated conv’un kernel boyutu ve dilation oranı.\n",
    "- Dilation, receptive field’ı büyütür ama parametre sayısını artırmaz.\n",
    "\n",
    "## `gate=\"sigmoid\"` / `\"hardsigmoid\"`\n",
    "- En sonda maskeyi 0–1 aralığına sıkıştırır.\n",
    "\n",
    "## `temperature=1.0`, `learnable_temperature=False`, `eps=1e-6`\n",
    "- `wlogit / T` ile maskenin keskinliği ayarlanır.\n",
    "- Learnable ise `T=softplus(t_raw)+eps` ile pozitif kalması garanti edilir.\n",
    "\n",
    "## `router_hidden=8`\n",
    "- Router’ın iç hidden kanal sayısı.\n",
    "\n",
    "## `bias=True`\n",
    "- Router içindeki 1×1 conv’larda bias kullanımı.\n",
    "\n",
    "## `return_router_weights=False`\n",
    "- Debug için `rw` döndürür.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca4834",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 3) Kernel Düzeltmeleri: Çift Kernel Neden Tek Yapılıyor?\n",
    "\n",
    "Kod:\n",
    "```python\n",
    "if k % 2 == 0:\n",
    "    k += 1\n",
    "p = k // 2\n",
    "```\n",
    "\n",
    "- Tek kernel ile padding simetrik olur.\n",
    "- `padding = k//2` seçilince `H,W` korunur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047baf0a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 4) Branch’ler: `Conv2d(2→1)` Neden 2 Giriş Kanalı?\n",
    "\n",
    "Forward’da:\n",
    "- `avg_map`: `(B,1,H,W)`\n",
    "- `max_map`: `(B,1,H,W)`\n",
    "- `s = cat([avg_map, max_map], dim=1)` → `(B,2,H,W)`\n",
    "\n",
    "Branch conv’lar `s`’yi alır ve `(B,1,H,W)` logit haritası üretir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aee5e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 5) Dilated Branch ve Padding Formülü\n",
    "\n",
    "Dilated conv’da efektif alan büyür.\n",
    "Boyutu korumak için:\n",
    "\n",
    "```python\n",
    "p = d * (k - 1) // 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35669c1e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 6) Router: Input’a Göre Branch Ağırlığı Üreten Küçük Ağ\n",
    "\n",
    "Router:\n",
    "- `AdaptiveAvgPool2d(1)` ile `(B,2,H,W)` → `(B,2,1,1)`\n",
    "- 1×1 conv + ReLU ile küçük bir MLP gibi çalışır\n",
    "- Son 1×1 conv: `(B, router_hidden,1,1)` → `(B,K,1,1)`\n",
    "\n",
    "Sonra:\n",
    "- `flatten(1)` → `(B,K)`\n",
    "- `softmax(dim=1)` → `(B,K)` (her örnek için olasılık dağılımı)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84211897",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 7) Forward: En Önemli Satırlar\n",
    "\n",
    "## 7.1) `s` üretimi\n",
    "```python\n",
    "avg_map = mean(x, dim=1, keepdim=True)   # (B,1,H,W)\n",
    "max_map, _ = max(x, dim=1, keepdim=True) # (B,1,H,W)\n",
    "s = cat([avg_map, max_map], dim=1)       # (B,2,H,W)\n",
    "```\n",
    "\n",
    "## 7.2) Router ağırlıkları\n",
    "```python\n",
    "logits = router(s).flatten(1)  # (B,K)\n",
    "rw = softmax(logits, dim=1)    # (B,K)\n",
    "```\n",
    "\n",
    "## 7.3) Branch çıktıları + ağırlıklı toplam\n",
    "```python\n",
    "z = stack([br(s) for br in branches], dim=1)  # (B,K,1,H,W)\n",
    "wlogit = (rw[:, :, None, None, None] * z).sum(dim=1)  # (B,1,H,W)\n",
    "```\n",
    "\n",
    "Buradaki `None` eklemeleri sadece broadcast içindir:\n",
    "- `(B,K)` → `(B,K,1,1,1)`\n",
    "- `(B,K,1,H,W)` ile çarpılabilir hale gelir.\n",
    "\n",
    "## 7.4) Temperature + gate + uygulama\n",
    "```python\n",
    "sa = gate_fn(wlogit / T)  # (B,1,H,W)\n",
    "y = x * sa                # (B,C,H,W)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c448056",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 8) Mini Deneyler (Şekil ve Router Ağırlıkları)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2219cf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: torch.Size([2, 64, 56, 56])\n",
      "sa: torch.Size([2, 1, 56, 56])\n",
      "rw: torch.Size([2, 3]) num_branches: 3\n",
      "rw[0]: tensor([0.4861, 0.2412, 0.2727], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Şekil kontrolü ve router weights örneği\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "m = DynamicSpatialAttention(kernels=(3,7), use_dilated=True, return_router_weights=True)\n",
    "y, sa, rw = m(x)\n",
    "\n",
    "print(\"y:\", y.shape)\n",
    "print(\"sa:\", sa.shape)\n",
    "print(\"rw:\", rw.shape, \"num_branches:\", m.num_branches)\n",
    "print(\"rw[0]:\", rw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3601ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T, mean, std, min, max = (0.5, 0.5809781551361084, 0.06028931215405464, 0.28509071469306946, 0.7919651865959167)\n",
      "T, mean, std, min, max = (1.0, 0.4193452000617981, 0.05250205472111702, 0.2961040139198303, 0.6760010123252869)\n",
      "T, mean, std, min, max = (2.0, 0.46876415610313416, 0.017804302275180817, 0.4033283293247223, 0.5379263162612915)\n",
      "T, mean, std, min, max = (5.0, 0.5082592368125916, 0.011276109144091606, 0.46128103137016296, 0.5573007464408875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdgn5\\AppData\\Local\\Temp\\ipykernel_22988\\3387644643.py:5: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:837.)\n",
      "  return (T, float(sa.mean()), float(sa.std()), float(sa.min()), float(sa.max()))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Temperature etkisi: maskenin istatistiği\n",
    "def stats(T):\n",
    "    m = DynamicSpatialAttention(temperature=T, learnable_temperature=False)\n",
    "    y, sa = m(x)\n",
    "    return (T, float(sa.mean()), float(sa.std()), float(sa.min()), float(sa.max()))\n",
    "\n",
    "for T in [0.5, 1.0, 2.0, 5.0]:\n",
    "    print(\"T, mean, std, min, max =\", stats(T))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
