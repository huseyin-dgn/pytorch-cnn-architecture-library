{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CBAMResidualDynamicSA — Detaylı Notlar\n",
        "\n",
        "Bu defter yalnızca aşağıdaki modülü anlatır:\n",
        "\n",
        "- `CBAMResidualDynamicSA`\n",
        "\n",
        "Bu sınıfın görevi:\n",
        "1. **Channel Attention** (CA) uygula  \n",
        "2. Çıkan sonuç üzerine **Dynamic Spatial Attention** (SA) uygula  \n",
        "3. İsteğe bağlı olarak **residual karışım** ile girişe geri bağla  \n",
        "4. İsteğe bağlı olarak debug için **haritaları/ağırlıkları** geri döndür\n",
        "\n",
        "> Not: CA ve SA’nın iç yapısı ayrı notlarda detaylandırıldı. Burada bu ikisini **nasıl birleştirdiğimiz** ve residual/alpha mantığı anlatılıyor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Sınıfın Büyük Resmi\n",
        "\n",
        "Giriş: `x`  → shape: **(B, C, H, W)**\n",
        "\n",
        "Aşamalar:\n",
        "\n",
        "1) `y = CA(x)`  \n",
        "- Kanal bazlı “hangi kanal önemli?” ölçekleme\n",
        "\n",
        "2) `y = SA(y)`  \n",
        "- Uzamsal “hangi konum önemli?” maskeleme\n",
        "\n",
        "3) (Opsiyonel) residual karışım:  \n",
        "- `out = x + alpha * (y - x)`  \n",
        "- Bu ifade pratikte şuna eşittir:  \n",
        "  - `out = (1 - alpha) * x + alpha * y`\n",
        "\n",
        "Bu sayede `alpha`:\n",
        "- `0` ise: **çıktı = x** (attention etkisi yok)\n",
        "- `1` ise: **çıktı = y** (tam attention)\n",
        "- aradaysa: **yumuşak karışım** (stabil/kolay öğrenme)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Parametreler — Ne İşe Yarıyor?\n",
        "\n",
        "### Temel\n",
        "- **channels**: Girişin kanal sayısı (C). CA bunu bilmek zorunda; SA zaten (avg/max) ile 2 kanala indirger.\n",
        "- **reduction, min_hidden**: Channel Attention içindeki MLP’nin “daraltma” ayarları.\n",
        "- **return_maps**: Debug modu. True ise forward daha fazla çıktı döndürür:\n",
        "  - CA map (`ca`)\n",
        "  - SA map (`sa`)\n",
        "  - CA fusion ağırlıkları (`fusion_w`)\n",
        "  - SA router ağırlıkları (`router_w`)\n",
        "\n",
        "### CA (ChannelAttentionFusionT) ile ilgili\n",
        "- **ca_fusion**: avg ve max squeeze çıktıları nasıl birleştirilecek? `\"sum\"` veya `\"softmax\"`.\n",
        "- **ca_gate**: Channel maskesi üretirken `sigmoid` mi `hardsigmoid` mi?\n",
        "- **ca_temperature**: Channel maskesinin keskinlik/yumuşaklık kontrolü.\n",
        "- **ca_act**: CA MLP içindeki aktivasyon (`relu` / `silu`).\n",
        "\n",
        "### SA (DynamicSpatialAttention) ile ilgili\n",
        "- **sa_kernels**: Normal branch kernel listesi (örn `(3,7)`).\n",
        "- **sa_use_dilated**: Ek dilated branch eklensin mi?\n",
        "- **sa_dilated_kernel, sa_dilated_d**: Dilated branch ayarları.\n",
        "- **sa_router_hidden**: Router’ın ara kanal sayısı (karar kapasitesi).\n",
        "- **sa_gate, sa_temperature**: Spatial mask için gate ve temperature.\n",
        "\n",
        "### Ortak Temperature Öğrenilebilirliği\n",
        "- **learnable_temperature**: True ise hem CA hem SA içindeki `T` öğrenilebilir olur.\n",
        "\n",
        "### Residual ve Alpha\n",
        "- **residual**: True ise `out = x + alpha*(y-x)` yapılır. False ise direkt `out=y`.\n",
        "- **alpha_init**: alpha başlangıç değeri.\n",
        "- **learnable_alpha**:\n",
        "  - True → alpha öğrenilir (`nn.Parameter`)\n",
        "  - False → alpha sabit kalır (`register_buffer`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) `alpha` Neden Var?\n",
        "\n",
        "Attention bloklarında bazen problem şudur:\n",
        "- CA+SA güçlü bir maske üretir\n",
        "- Modelin ilk dönemlerinde bu maske fazla agresif olabilir\n",
        "- Gradyan/optimizasyon daha zor hale gelebilir\n",
        "\n",
        "`alpha` ile amaç:\n",
        "- attention’ı **kontrollü** devreye almak\n",
        "- “tam bas” yerine “kademeli karıştır” mantığı\n",
        "\n",
        "Matematik:\n",
        "- `out = x + alpha*(y-x)`  \n",
        "- `out = (1-alpha)*x + alpha*y`\n",
        "\n",
        "Bu, residual öğrenmeyi genelde daha stabil yapar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) `nn.Parameter` vs `register_buffer` — Neden İkisi Var?\n",
        "\n",
        "Alpha iki şekilde tutulabilir:\n",
        "\n",
        "### a) Öğrenilebilir alpha\n",
        "```python\n",
        "self.alpha = nn.Parameter(torch.tensor(alpha_init))\n",
        "```\n",
        "- optimizer alpha’yı günceller\n",
        "- model “ne kadar residual karışım yapacağını” öğrenir\n",
        "\n",
        "### b) Sabit alpha\n",
        "```python\n",
        "self.register_buffer(\"alpha\", torch.tensor(alpha_init))\n",
        "```\n",
        "- alpha öğrenilmez ama:\n",
        "  - model ile birlikte GPU’ya taşınır\n",
        "  - checkpoint içine girer\n",
        "- eğitim boyunca sabit kalır\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Not: ChannelAttentionFusionT ve DynamicSpatialAttention\n",
        "# sınıflarının aynı dosyada/ortamda tanımlı olduğu varsayılır.\n",
        "# Bu notebook, sadece CBAMResidualDynamicSA'nın mantığına odaklanır.\n",
        "\n",
        "\n",
        "class CBAMResidualDynamicSA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        reduction: int = 16,\n",
        "        min_hidden: int = 4,\n",
        "        ca_fusion: str = \"softmax\",\n",
        "        ca_gate: str = \"sigmoid\",\n",
        "        ca_temperature: float = 1.0,\n",
        "        ca_act: str = \"relu\",\n",
        "        sa_gate: str = \"sigmoid\",\n",
        "        sa_temperature: float = 1.0,\n",
        "        learnable_temperature: bool = False,\n",
        "        sa_kernels=(3, 7),\n",
        "        sa_use_dilated: bool = True,\n",
        "        sa_dilated_kernel: int = 7,\n",
        "        sa_dilated_d: int = 2,\n",
        "        sa_router_hidden: int = 8,\n",
        "        residual: bool = True,\n",
        "        alpha_init: float = 1.0,\n",
        "        learnable_alpha: bool = False,\n",
        "        return_maps: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.return_maps = return_maps\n",
        "        self.residual = residual\n",
        "\n",
        "        self.ca = ChannelAttentionFusionT(\n",
        "            channels=channels,\n",
        "            reduction=reduction,\n",
        "            min_hidden=min_hidden,\n",
        "            fusion=ca_fusion,\n",
        "            gate=ca_gate,\n",
        "            temperature=ca_temperature,\n",
        "            learnable_temperature=learnable_temperature,\n",
        "            act=ca_act,\n",
        "            return_fusion_weights=return_maps,\n",
        "        )\n",
        "\n",
        "        self.sa = DynamicSpatialAttention(\n",
        "            kernels=sa_kernels,\n",
        "            use_dilated=sa_use_dilated,\n",
        "            dilated_kernel=sa_dilated_kernel,\n",
        "            dilated_d=sa_dilated_d,\n",
        "            gate=sa_gate,\n",
        "            temperature=sa_temperature,\n",
        "            learnable_temperature=learnable_temperature,\n",
        "            router_hidden=sa_router_hidden,\n",
        "            return_router_weights=return_maps,\n",
        "        )\n",
        "\n",
        "    # alpha_init, residual karışımın BAŞLANGIÇTA ne kadar güçlü olacağını belirleyen ilk değerdir.\n",
        "    # alpha_init, alpha’nın ilk değeri:: Model daha eğitime başlamadan önce, attention’ın etkisi yüzde kaç olsun?\n",
        "        if residual:\n",
        "            if learnable_alpha:\n",
        "                self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
        "            else:\n",
        "                self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # return_maps şu demek: “Sadece sonucu mu döndüreyim, yoksa attention maskelerini/ağırlıklarını da yanında vereyim mi?”\n",
        "\n",
        "        # return_maps=True → CA 3 şey döndürür :: :: return_maps=False → CA 2 şey döndürür\n",
        "        if self.return_maps:\n",
        "            # ca :: Maske \n",
        "            # y :: Attention uygulanmış çıktı\n",
        "            # fusion_w :: avg mi öenmli max mı ? \n",
        "            # router_w :: Spatial de olan işlem aslında.Yani oluşan kernellara güvenden çıkan değerler.Düz tabirle ise :: SA’nın “hangi kernel branch daha işe yarıyor?” kararının ağırlıklarıdır.\n",
        "            # out :: Nihai çıktı\n",
        "            y, ca, fusion_w = self.ca(x)\n",
        "            y, sa, router_w = self.sa(y)\n",
        "            out = x + self.alpha * (y - x) if self.residual else y\n",
        "            # y - x = attention’ın x’i ne kadar değiştirdiği\n",
        "            # alpha = bu değişikliğin ne kadarını ekleyelim? :: alpha = “attention’a ne kadar katayım?” katsayısı\n",
        "            return out, ca, sa, fusion_w, router_w\n",
        "        \n",
        "# return_maps bu blokta “debug modu” gibi çalışır. Amaç, sadece nihai çıktıyı vermek mi, yoksa attention’ın ürettiği maskeleri ve ağırlıkları da yanında göstermek mi kararını vermektir.\n",
        "\n",
        "# Eğer self.return_maps True ise, model önce self.ca(x) çağrısı ile channel attention uygular ve bu çağrıdan üç şey alır: attention uygulanmış ara çıktı y, kanal maskesi ca \n",
        "# ve (avg mi max mı daha baskın?) bilgisini veren fusion_w. Ardından bu y çıktısını self.sa(y) içine sokar; \n",
        "# burada da spatial attention uygulanır ve yine üç şey alınır: yeni y, uzamsal maske sa ve branch seçim ağırlıkları router_w.\n",
        "    \n",
        "# Daha sonra residual açıksa x ile y arasında alpha katsayısı ile karışım yapılır (out = x + alpha*(y-x)), residual kapalıysa doğrudan y kullanılır.\n",
        "# Bu debug modunda fonksiyon beş değer döndürür: out, ca, sa, fusion_w, router_w. \n",
        "# Yani hem nihai sonucu hem de “model nereye bakıyor/neyi seçiyor?” diye inceleyebileceğin tüm haritaları ve ağırlıkları verir.\n",
        "\n",
        "# Eğer self.return_maps False ise, aynı attention işlemleri yine yapılır; fakat bu sefer self.ca(x) ve self.sa(y) çağrılarından dönen maskeler _ ile alınmayıp bilerek atılır.\n",
        "# Yani model channel ve spatial maskeleri hesaplayıp uygular, ama senin dışarıda görmene gerek olmadığı için sadece attention uygulanmış y üzerinden devam eder. \n",
        "# Ardından yine residual karışımı hesaplanır ve fonksiyon yalnızca tek çıktı döndürür: out. Bu mod “normal kullanım” modudur; maskeler içeride uygulanır, dışarıya yalnızca nihai çıktı verilir.\n",
        "\n",
        "        y, _ = self.ca(x)\n",
        "        y, _ = self.sa(y)\n",
        "        out = x + self.alpha * (y - x) if self.residual else y\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Forward Akışı — Satır Satır (Önemli Kısım)\n",
        "\n",
        "Aşağıdaki anlatım, `forward()` içindeki iki akışı da kapsar.\n",
        "\n",
        "---\n",
        "\n",
        "### A) `return_maps = True` iken (debug modu)\n",
        "\n",
        "```python\n",
        "y, ca, fusion_w = self.ca(x)\n",
        "y, sa, router_w = self.sa(y)\n",
        "out = x + alpha * (y - x) if residual else y\n",
        "return out, ca, sa, fusion_w, router_w\n",
        "```\n",
        "\n",
        "- **CA** üç şey döndürür:\n",
        "  1) `y`: attention uygulanmış feature\n",
        "  2) `ca`: channel mask (B,C,1,1)\n",
        "  3) `fusion_w`: avg/max fuse ağırlıkları (2,) (sadece fusion=\"softmax\" iken anlamlı)\n",
        "\n",
        "- **SA** üç şey döndürür:\n",
        "  1) `y`: spatial attention uygulanmış feature\n",
        "  2) `sa`: spatial mask (B,1,H,W)\n",
        "  3) `router_w`: branch seçim ağırlıkları (B,K)\n",
        "\n",
        "- Sonra **residual** açıksa:\n",
        "  - `out = x + alpha*(y - x)`  \n",
        "  - yani `x` ile `y` arasında kontrollü karışım\n",
        "\n",
        "Debug çıktıları ne işe yarar?\n",
        "- `ca` ve `sa` görselleştirilebilir\n",
        "- `fusion_w` ile CA’da avg mı max mı baskın takip edilir\n",
        "- `router_w` ile SA’da hangi branch’in baskın olduğu takip edilir\n",
        "\n",
        "---\n",
        "\n",
        "### B) `return_maps = False` iken (normal mod)\n",
        "\n",
        "```python\n",
        "y, _ = self.ca(x)\n",
        "y, _ = self.sa(y)\n",
        "out = x + alpha*(y-x) if residual else y\n",
        "return out\n",
        "```\n",
        "\n",
        "Bu modda:\n",
        "- Ek harita/weight dönmez\n",
        "- Daha hafif ve üretime daha uygun\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Residual Kapalıysa Ne Olur?\n",
        "\n",
        "```python\n",
        "out = y\n",
        "```\n",
        "\n",
        "- Bu durumda CA+SA çıktısı direkt alınır.\n",
        "- Alpha zaten oluşturulmuş olsa bile (kodda residual False ise kullanılmaz).\n",
        "\n",
        "---\n",
        "\n",
        "## 7) `alpha` ile “Karışım” Mantığı\n",
        "\n",
        "`out = x + alpha*(y-x)` ifadesi şunu sağlar:\n",
        "\n",
        "- `alpha = 0` → `out = x`\n",
        "- `alpha = 1` → `out = y`\n",
        "- `alpha = 0.5` → `out = 0.5*x + 0.5*y`\n",
        "\n",
        "Bu sayede:\n",
        "- attention etkisini kontrollü şekilde açıp kapatabilirsin\n",
        "- stabilite artabilir (özellikle çok güçlü maskelerde)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Mini Notlar (Sık Takılınan Yerler)\n",
        "\n",
        "- **Neden CA önce, SA sonra?**  \n",
        "  CBAM geleneği: önce kanal seçimi, sonra uzamsal odak.  \n",
        "  Kanal seçiminden sonra uzamsal maske daha anlamlı çalışır.\n",
        "\n",
        "- **CA ve SA ikisi de mask üretiyor, neden ikisi birden?**  \n",
        "  CA: “hangi feature kanalları önemli?”  \n",
        "  SA: “hangi konum önemli?”  \n",
        "  İkisi farklı eksenlerde çalışır, birbirini tamamlar.\n",
        "\n",
        "- **return_maps neden hem CA’ya hem SA’ya gidiyor?**  \n",
        "  Çünkü debug çıktıları ayrı ayrı izlenmek istenir:\n",
        "  - CA: `fusion_w`\n",
        "  - SA: `router_w`\n",
        "\n",
        "- **learnable_temperature tek parametreyle ikisini de etkiliyor**  \n",
        "  Bu tasarım seçimi: tek flag ile CA+SA sıcaklıklarının öğrenilebilirliği açılır.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
