{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb9d484",
   "metadata": {},
   "source": [
    "# CBAMResidualDynamicSA (Wrapper) — Açıklama + Tam Birleşik Kod\n",
    "\n",
    "Bu not defteri:\n",
    "1. CA ve SA çıktılarının wrapper içinde nasıl birleştirildiğini açıklar.\n",
    "2. Residual karışımın (alpha) ne yaptığını gösterir.\n",
    "3. En sonda tüm kodun tek dosyada birleştirilmiş halini verir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6297279",
   "metadata": {},
   "source": [
    "## 1) Wrapper’ın görevi\n",
    "\n",
    "Wrapper sınıfı bir attention tasarlamaz; iki attention bloğunu **sırayla çalıştırır**:\n",
    "\n",
    "- **CA (Channel Attention)**: kanalların önemini belirler.\n",
    "- **SA (Spatial Attention)**: konumların önemini belirler.\n",
    "\n",
    "Bu iki çıktıyı tek bir akışta birleştirip, opsiyonel residual karışım uygular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a02df",
   "metadata": {},
   "source": [
    "## 2) Akış: CA → SA\n",
    "\n",
    "Forward akışı şu şekildedir:\n",
    "\n",
    "1. `y = CA(x)`  (kanalları kıs/aç)\n",
    "2. `y = SA(y)`  (konumları kıs/aç)\n",
    "\n",
    "Bu, CBAM’in genel tasarım felsefesiyle uyumludur: önce kanal filtresi, sonra uzamsal filtresi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdc34e",
   "metadata": {},
   "source": [
    "## 3) Residual karışım (`residual=True`)\n",
    "\n",
    "Residual açıkken:\n",
    "\n",
    "\\\\[\\nout = x + \\\\alpha (y - x)\\n\\\\]\\n\\n- `y - x`: attention’ın getirdiği değişim.\n",
    "- `alpha`: bu değişimi ne kadar uygulayacağını kontrol eder.\n",
    "\n",
    "Bu mekanizma, attention maskeleri agresifleştiğinde eğitimi stabil tutmaya yardım eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221108c",
   "metadata": {},
   "source": [
    "## 4) `alpha` sabit mi learnable mı?\n",
    "\n",
    "- `learnable_alpha=True`: `alpha` öğrenilir (`nn.Parameter`).\n",
    "- `learnable_alpha=False`: `alpha` sabittir (`register_buffer`).\n",
    "\n",
    "Sabit alpha, attention etkisini kontrollü tutar.\n",
    "Learnable alpha, modelin bu etkiyi kendisinin ayarlamasına izin verir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034595cc",
   "metadata": {},
   "source": [
    "## 5) `return_maps` (debug modu)\n",
    "\n",
    "- `return_maps=False`: sadece `out` döner.\n",
    "- `return_maps=True`: `out, ca, sa, fusion_w, router_w` döner.\n",
    "\n",
    "Bu sayede:\n",
    "- CA tarafında avg/max karışım ağırlıklarını (`fusion_w`),\n",
    "- SA tarafında branch ağırlıklarını (`router_w`)\n",
    "inceleyebilirsin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b679dc",
   "metadata": {},
   "source": [
    "## 6) Tam birleşik kod (CA + SA + Wrapper + __main__ testi)\n",
    "\n",
    "Aşağıdaki hücre, istenen tüm sınıfları ve en altta kısa bir test bloğunu içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def _softplus_inverse(y: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return torch.log(torch.clamp(torch.exp(y) - 1.0, min=eps))\n",
    "\n",
    "\n",
    "def _get_gate(gate: str):\n",
    "    g = gate.lower()\n",
    "    if g == \"sigmoid\":\n",
    "        return torch.sigmoid\n",
    "    if g == \"hardsigmoid\":\n",
    "        return F.hardsigmoid\n",
    "    raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "\n",
    "def _get_act(act: str):\n",
    "    a = act.lower()\n",
    "    if a == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    if a == \"silu\":\n",
    "        return nn.SiLU(inplace=True)\n",
    "    raise ValueError(\"act 'relu' veya 'silu' olmalı.\")\n",
    "\n",
    "\n",
    "def _make_odd(k: int) -> int:\n",
    "    k = int(k)\n",
    "    if k < 1:\n",
    "        raise ValueError(\"Kernel size >= 1 olmalı.\")\n",
    "    return k if (k % 2 == 1) else (k + 1)\n",
    "\n",
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",        # \"sum\" | \"softmax\"\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        act: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "        fusion_router_hidden: int = 16,\n",
    "        return_fusion_weights: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalı.\")\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if fusion_router_hidden < 1:\n",
    "            raise ValueError(\"fusion_router_hidden >= 1 olmalı.\")\n",
    "\n",
    "        self.eps = float(eps)\n",
    "        self.fusion = fusion\n",
    "        self.return_fusion_weights = bool(return_fusion_weights)\n",
    "        self.gate_fn = _get_gate(gate)\n",
    "\n",
    "        hidden = max(int(min_hidden), int(channels) // int(reduction))\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=bias)\n",
    "        self.act = _get_act(act)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        if self.fusion == \"softmax\":\n",
    "            self.fusion_router = nn.Sequential(\n",
    "                nn.Conv2d(2 * channels, fusion_router_hidden, kernel_size=1, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(fusion_router_hidden, 2, kernel_size=1, bias=True),\n",
    "            )\n",
    "        else:\n",
    "            self.fusion_router = None\n",
    "\n",
    "        self.learnable_temperature = bool(learnable_temperature)\n",
    "        if self.learnable_temperature:\n",
    "            t0 = torch.tensor(float(temperature))\n",
    "            t_inv = _softplus_inverse(t0, eps=self.eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_s = self.avg_pool(x)  # (B,C,1,1)\n",
    "        max_s = self.max_pool(x)  # (B,C,1,1)\n",
    "\n",
    "        a = self.mlp(avg_s)       # (B,C,1,1)\n",
    "        m = self.mlp(max_s)       # (B,C,1,1)\n",
    "\n",
    "        fusion_w = None\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            s_cat = torch.cat([avg_s, max_s], dim=1)          # (B,2C,1,1)\n",
    "            logits = self.fusion_router(s_cat).flatten(1)     # (B,2)\n",
    "            fusion_w = torch.softmax(logits, dim=1)           # (B,2)\n",
    "            z = fusion_w[:, 0].view(-1, 1, 1, 1) * a + fusion_w[:, 1].view(-1, 1, 1, 1) * m\n",
    "\n",
    "        T = self.get_T()\n",
    "        ca = self.gate_fn(z / T)  # (B,C,1,1)\n",
    "        y = x * ca\n",
    "\n",
    "        if self.return_fusion_weights and (fusion_w is not None):\n",
    "            return y, ca, fusion_w\n",
    "        return y, ca\n",
    "\n",
    "\n",
    "class _DWPointwiseBranch(nn.Module):\n",
    "    def __init__(self, in_ch: int, k: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        k = _make_odd(k)\n",
    "        dilation = int(dilation)\n",
    "        if dilation < 1:\n",
    "            raise ValueError(\"dilation >= 1 olmalı.\")\n",
    "        pad = dilation * (k - 1) // 2\n",
    "\n",
    "        self.dw = nn.Conv2d(\n",
    "            in_ch, in_ch, kernel_size=k, padding=pad, dilation=dilation, groups=in_ch, bias=False\n",
    "        )\n",
    "        self.pw = nn.Conv2d(in_ch, 1, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pw(self.dw(s))\n",
    "\n",
    "\n",
    "class DynamicSpatialAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernels=(3, 7),\n",
    "        use_dilated: bool = True,\n",
    "        dilated_kernel: int = 7,\n",
    "        dilated_d: int = 2,\n",
    "        gate: str = \"sigmoid\",\n",
    "        temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        router_hidden: int = 8,\n",
    "        bias: bool = True,\n",
    "        return_router_weights: bool = False,\n",
    "        coord_norm: str = \"minus1to1\",  # \"minus1to1\" | \"0to1\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if router_hidden < 1:\n",
    "            raise ValueError(\"router_hidden >= 1 olmalı.\")\n",
    "        if coord_norm not in (\"minus1to1\", \"0to1\"):\n",
    "            raise ValueError(\"coord_norm 'minus1to1' veya '0to1' olmalı.\")\n",
    "\n",
    "        self.eps = float(eps)\n",
    "        self.return_router_weights = bool(return_router_weights)\n",
    "        self.gate_fn = _get_gate(gate)\n",
    "        self.coord_norm = coord_norm\n",
    "\n",
    "        in_ch = 4  # [avg_map, max_map, x_coord, y_coord]\n",
    "\n",
    "        ks = [_make_odd(int(k)) for k in kernels]\n",
    "        branches = [_DWPointwiseBranch(in_ch=in_ch, k=k, dilation=1) for k in ks]\n",
    "\n",
    "        if use_dilated:\n",
    "            dk = _make_odd(int(dilated_kernel))\n",
    "            dd = int(dilated_d)\n",
    "            if dd < 1:\n",
    "                raise ValueError(\"dilated_d >= 1 olmalı.\")\n",
    "            branches.append(_DWPointwiseBranch(in_ch=in_ch, k=dk, dilation=dd))\n",
    "\n",
    "        self.branches = nn.ModuleList(branches)\n",
    "        self.num_branches = len(self.branches)\n",
    "\n",
    "        self.router = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_ch, router_hidden, kernel_size=1, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(router_hidden, self.num_branches, kernel_size=1, bias=bias),\n",
    "        )\n",
    "\n",
    "        self.learnable_temperature = bool(learnable_temperature)\n",
    "        if self.learnable_temperature:\n",
    "            t0 = torch.tensor(float(temperature))\n",
    "            t_inv = _softplus_inverse(t0, eps=self.eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "        self._coord_cache = {}\n",
    "\n",
    "    def get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def _coords(self, B: int, H: int, W: int, device, dtype):\n",
    "        key = (H, W, str(device), str(dtype), self.coord_norm)\n",
    "        if key in self._coord_cache:\n",
    "            xg, yg = self._coord_cache[key]\n",
    "        else:\n",
    "            if self.coord_norm == \"minus1to1\":\n",
    "                xs = torch.linspace(-1.0, 1.0, W, device=device, dtype=dtype)\n",
    "                ys = torch.linspace(-1.0, 1.0, H, device=device, dtype=dtype)\n",
    "            else:\n",
    "                xs = torch.linspace(0.0, 1.0, W, device=device, dtype=dtype)\n",
    "                ys = torch.linspace(0.0, 1.0, H, device=device, dtype=dtype)\n",
    "\n",
    "            yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
    "            xg = xx.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "            yg = yy.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "            self._coord_cache[key] = (xg, yg)\n",
    "\n",
    "        return xg.expand(B, -1, -1, -1), yg.expand(B, -1, -1, -1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)       # (B,1,H,W)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)     # (B,1,H,W)\n",
    "\n",
    "        x_coord, y_coord = self._coords(B, H, W, x.device, x.dtype)\n",
    "        s = torch.cat([avg_map, max_map, x_coord, y_coord], dim=1)  # (B,4,H,W)\n",
    "\n",
    "        logits = self.router(s).flatten(1)    # (B,K)\n",
    "        rw = torch.softmax(logits, dim=1)     # (B,K)\n",
    "\n",
    "        z = torch.stack([br(s) for br in self.branches], dim=1)  # (B,K,1,H,W)\n",
    "        wlogit = (rw[:, :, None, None, None] * z).sum(dim=1)     # (B,1,H,W)\n",
    "\n",
    "        T = self.get_T()\n",
    "        sa = self.gate_fn(wlogit / T)         # (B,1,H,W)\n",
    "        y = x * sa\n",
    "\n",
    "        if self.return_router_weights:\n",
    "            return y, sa, rw\n",
    "        return y, sa\n",
    "\n",
    "\n",
    "class CBAMResidualDynamicSA(nn.Module):\n",
    "    \"\"\"Wrapper: CA -> SA + opsiyonel residual karışım.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        # CA\n",
    "        ca_fusion: str = \"softmax\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        ca_temperature: float = 1.0,\n",
    "        ca_act: str = \"relu\",\n",
    "        ca_fusion_router_hidden: int = 16,\n",
    "        # SA\n",
    "        sa_gate: str = \"sigmoid\",\n",
    "        sa_temperature: float = 1.0,\n",
    "        sa_kernels=(3, 7),\n",
    "        sa_use_dilated: bool = True,\n",
    "        sa_dilated_kernel: int = 7,\n",
    "        sa_dilated_d: int = 2,\n",
    "        sa_router_hidden: int = 8,\n",
    "        sa_coord_norm: str = \"minus1to1\",\n",
    "        # shared\n",
    "        learnable_temperature: bool = False,\n",
    "        # residual\n",
    "        residual: bool = True,\n",
    "        alpha_init: float = 1.0,\n",
    "        learnable_alpha: bool = False,\n",
    "        # outputs\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        \n",
    "#         CA (Channel) grubu :: :: channels, reduction, min_hidden ,  ca_fusion, ca_gate, ca_temperature, ca_act, ca_fusion_router_hidden \n",
    "#         SA (Spatial) grubu :: :: sa_kernels, sa_use_dilated, sa_dilated_kernel, sa_dilated_d , sa_gate, sa_temperature, sa_router_hidden, sa_coord_norm\n",
    "#         Shared (ortak kontrol) :: :: learnable_temperature (CA ve SA’nın temperature’ını öğrenilebilir yapar)\n",
    "#         Wrapper davranışı :: :: residual, alpha_init, learnable_alpha, return_maps\n",
    "\n",
    "        super().__init__()\n",
    "        self.return_maps = bool(return_maps)\n",
    "        self.residual = bool(residual)\n",
    "\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=reduction,\n",
    "            min_hidden=min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            eps=1e-6,\n",
    "            act=ca_act,\n",
    "            bias=True,\n",
    "            fusion_router_hidden=ca_fusion_router_hidden,\n",
    "            return_fusion_weights=self.return_maps,\n",
    "        )\n",
    "\n",
    "        self.sa = DynamicSpatialAttention(\n",
    "            kernels=sa_kernels,\n",
    "            use_dilated=sa_use_dilated,\n",
    "            dilated_kernel=sa_dilated_kernel,\n",
    "            dilated_d=sa_dilated_d,\n",
    "            gate=sa_gate,\n",
    "            temperature=sa_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            eps=1e-6,\n",
    "            router_hidden=sa_router_hidden,\n",
    "            bias=True,\n",
    "            return_router_weights=self.return_maps,\n",
    "            coord_norm=sa_coord_norm,\n",
    "        )\n",
    "\n",
    "        if self.residual:\n",
    "            if learnable_alpha:\n",
    "                self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
    "            else:\n",
    "                self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.return_maps:\n",
    "            y, ca, fusion_w = self.ca(x)\n",
    "            y, sa, router_w = self.sa(y)\n",
    "            out = x + self.alpha * (y - x) if self.residual else y\n",
    "            return out, ca, sa, fusion_w, router_w\n",
    "\n",
    "        y, _ = self.ca(x)\n",
    "        y, _ = self.sa(y)\n",
    "        out = x + self.alpha * (y - x) if self.residual else y\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(2, 64, 56, 56)\n",
    "    model = CBAMResidualDynamicSA(\n",
    "        channels=64,\n",
    "        return_maps=True,\n",
    "        residual=True,\n",
    "        learnable_alpha=False,\n",
    "        learnable_temperature=True,\n",
    "        sa_kernels=(3, 5, 7),\n",
    "        sa_use_dilated=True,\n",
    "    )\n",
    "    out, ca, sa, fusion_w, router_w = model(x)\n",
    "    print(\"out:\", out.shape)\n",
    "    print(\"ca:\", ca.shape)\n",
    "    print(\"sa:\", sa.shape)\n",
    "    print(\"fusion_w:\", fusion_w.shape)\n",
    "    print(\"router_w:\", router_w.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
