{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4626119",
   "metadata": {},
   "source": [
    "# ChannelAttentionFusionT — Hata Avı Çalışmaları (Soru–Çözüm Derlemesi)\n",
    "\n",
    "Bu not defteri, **ChannelAttentionFusionT** (CA) bloğu üzerinde yapılan “tek satır değişiklik” senaryolarını,\n",
    "bu değişikliklerin **davranışsal etkilerini**, **beklenen eğitim belirtilerini** ve **en hızlı debug kontrollerini**\n",
    "toplu biçimde sunar.\n",
    "\n",
    "> Odak: *sample-wise adaptiflik*, *stabil öğrenme*, *mask/temperature davranışı*, *router doğruluğu*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a50dfd",
   "metadata": {},
   "source": [
    "## Referans: Doğru (beklenen) akış\n",
    "\n",
    "- `avg_s = AvgPool(x)`, `max_s = MaxPool(x)` → (B,C,1,1)  \n",
    "- `a = MLP(avg_s)`, `m = MLP(max_s)` → (B,C,1,1)  \n",
    "- `fusion_router([avg_s,max_s])` → logits (B,2)  \n",
    "- `fusion_w = softmax(logits, dim=1)` → (B,2) **(her örnek kendi içinde normalize)**  \n",
    "- `z = w0*a + w1*m`  \n",
    "- `ca = gate(z / T)`  \n",
    "- `y = x * ca`  \n",
    "\n",
    "**Kritik invariant:** `fusion_w.sum(dim=1) ≈ 1` (her örnek için).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccc96bf",
   "metadata": {},
   "source": [
    "## Senaryo 1: Softmax ekseni hatası (dim=0)\n",
    "\n",
    "### Yapılan değişiklik\n",
    "```python\n",
    "fusion_w = torch.softmax(logits, dim=0)\n",
    "```\n",
    "\n",
    "### Neden sorunlu?\n",
    "- `logits` şekli (B,2). `dim=0` seçilirse normalizasyon **batch boyunca** yapılır.\n",
    "- Sonuç: ağırlıklar örnek içi (avg vs max) yarışmak yerine örnekler birbirini etkiler.\n",
    "- Bu durum **sample-wise adaptifliği bozar** ve batch kompozisyonuna bağımlılık üretir.\n",
    "\n",
    "### Eğitimde beklenen belirtiler\n",
    "- Aynı örnek farklı batch’te farklı `fusion_w` alabilir.\n",
    "- Batch size/kompozisyon değişiminde performans dalgalanması.\n",
    "- Daha gürültülü öğrenme; genelleme düşebilir.\n",
    "\n",
    "### En hızlı debug kontrolleri\n",
    "- Doğru durumda: `fusion_w.sum(dim=1)` → (B,) her eleman ≈ 1 olmalı.\n",
    "- Hatalı dim=0 imzası: `fusion_w.sum(dim=0)` → (2,) ≈ [1,1].\n",
    "- Ek kontrol: `fusion_w.std(dim=0)` davranış anomalisini gösterebilir.\n",
    "\n",
    "### Doğrusu (fix)\n",
    "```python\n",
    "fusion_w = torch.softmax(logits, dim=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e1929",
   "metadata": {},
   "source": [
    "## Senaryo 2: Temperature kullanımının ters çevrilmesi (z * T)\n",
    "\n",
    "### Yapılan değişiklik\n",
    "```python\n",
    "ca = gate_fn(z * T)\n",
    "```\n",
    "\n",
    "### Neden sorunlu?\n",
    "- Temperature, gate öncesi logitleri **bölerek** yumuşatma/sertleştirme amacı taşır.\n",
    "- Çarpma yapılınca kontrol tersine döner: `T` büyüdükçe logit büyür → maske daha çok saturate olur.\n",
    "- Özellikle `learnable_temperature=True` iken model T’yi büyütüp maskeyi kilitleyebilir.\n",
    "\n",
    "### Eğitimde beklenen belirtiler\n",
    "- `ca` hızla 0/1’e yapışabilir (saturasyon).\n",
    "- Gradyan zayıflayabilir (sigmoid/hardsigmoid uç bölgelerde türev küçülür).\n",
    "- Eğitim kararsızlaşabilir ya da erken “kilitlenme” görülebilir.\n",
    "\n",
    "### En hızlı debug kontrolleri\n",
    "- `T` arttıkça `ca` dağılımının daha keskinleşip keskinleşmediğini izleyin.\n",
    "- `ca` istatistikleri: `min/max/mean/std` ve `P(ca>0.99)`/`P(ca<0.01)`.\n",
    "- `|z/T|` yerine `|z*T|` büyümesi saturasyon işaretidir.\n",
    "\n",
    "### Doğrusu (fix)\n",
    "```python\n",
    "ca = gate_fn(z / T)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e457c",
   "metadata": {},
   "source": [
    "## Senaryo 3: z’yi ham squeeze’tan üretmek (avg_s/max_s) — ifade gücü kaybı\n",
    "\n",
    "### Yapılan değişiklik\n",
    "```python\n",
    "z = w0*avg_s + w1*max_s  # a,m yerine\n",
    "```\n",
    "\n",
    "### Neden sorunlu?\n",
    "- `a` ve `m`, MLP ile **öğrenilmiş kanal dönüşümü** (C→hidden→C) içerir.\n",
    "- `avg_s/max_s` ise yalnızca ham istatistik. `z` ham squeeze ile üretilirse CA’nın temsil gücü zayıflar.\n",
    "- Kod çalışır; ancak attention katkısı azalır.\n",
    "\n",
    "### Eğitimde beklenen belirtiler\n",
    "- Konverjans yavaşlayabilir.\n",
    "- `ca` daha az ayrıştırıcı olabilir; bazen “var ama etkisi az” hissi.\n",
    "- Tavan performans düşebilir (görev/veri setine bağlı).\n",
    "\n",
    "### En hızlı debug kontrolleri\n",
    "- `ca`’nın kanallar arası varyansını izleyin.\n",
    "- `a` ile `avg_s` arasındaki fark: `||a-avg_s||` (MLP’nin dönüşüm gücü).\n",
    "- Ablation: a/m ile üretim vs avg_s/max_s ile üretim kıyası.\n",
    "\n",
    "### Doğrusu (fix)\n",
    "```python\n",
    "z = w0*a + w1*m\n",
    "```\n",
    "\n",
    "**Not:** Bu bir “hata”dan çok tasarım zayıflatmasıdır; hedef güçlendirme ise `a/m` tercih edilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b2271",
   "metadata": {},
   "source": [
    "## Senaryo 4: MLP aktivasyonunun kaldırılması (lineerleşme)\n",
    "\n",
    "### Yapılan değişiklik\n",
    "```python\n",
    "return fc2(fc1(s))  # act yok\n",
    "```\n",
    "\n",
    "### Neden sorunlu?\n",
    "- Aktivasyon kaldırılınca iki lineer katman ardışık gelir ve tek lineer dönüşüme indirgenir.\n",
    "- Nonlinearity kaybolur; kanal ilişkilerini modelleme kapasitesi düşer.\n",
    "\n",
    "### Eğitimde beklenen belirtiler\n",
    "- CA etkisi zayıflayabilir; maske daha “düz” davranabilir.\n",
    "- Konverjans bazen yavaşlar; tavan performans düşebilir.\n",
    "- Model yine öğrenir (backbone taşır) ama attention katkısı azalır.\n",
    "\n",
    "### En hızlı debug kontrolleri\n",
    "- `ca` ayrıştırıcılığı: std/variance, uçlara yığılma.\n",
    "- `a/m` istatistikleri (std/mean) ve epoch boyunca değişim.\n",
    "- Ablation: aktivasyonlu vs aktivasyonsuz kıyas.\n",
    "\n",
    "### Doğrusu (fix)\n",
    "```python\n",
    "return fc2(act(fc1(s)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782c53d",
   "metadata": {},
   "source": [
    "## Senaryo 5: `get_T()` içinde +eps kaldırılması (T → 0 riski)\n",
    "\n",
    "### Yapılan değişiklik\n",
    "```python\n",
    "return F.softplus(self.t_raw)  # +eps yok\n",
    "```\n",
    "\n",
    "### Neden sorunlu?\n",
    "- Softplus pozitif üretse de çok küçük değerlere yaklaşabilir.\n",
    "- `z/T` ifadesinde `T` çok küçülürse logit büyür → saturasyon/taşma riski.\n",
    "- `eps` T’ye alt sınır koyarak sayısal stabilite sağlar.\n",
    "\n",
    "### Eğitimde beklenen belirtiler\n",
    "- `T` çok küçülürse maske keskinleşir; öğrenme kilitlenebilir.\n",
    "- Aşırı durumda `z/T` büyür → inf/NaN riskleri.\n",
    "- AMP/mixed precision ortamında risk artar.\n",
    "\n",
    "### En hızlı debug kontrolleri\n",
    "- `T.min()` kırmızı bayrak (örn <1e-4 gibi).\n",
    "- `torch.isfinite(z/T)` ve `torch.isfinite(loss)` kontrolleri.\n",
    "- `ca` saturasyon oranı artıyor mu?\n",
    "\n",
    "### Doğrusu (fix)\n",
    "```python\n",
    "return F.softplus(self.t_raw) + self.eps\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148376d",
   "metadata": {},
   "source": [
    "## Senaryo 6: Batch ortalaması + expand ile ‘global’ fusion_w (sample-wise ölür)\n",
    "\n",
    "### Yapılan değişiklik\n",
    "```python\n",
    "s_cat = cat([avg_s,max_s], dim=1).mean(dim=0, keepdim=True)\n",
    "logits = fusion_router(s_cat)  # (1,2)\n",
    "fusion_w = softmax(logits, dim=1).expand(B,-1)  # tüm batch aynı\n",
    "```\n",
    "\n",
    "### Neden sorunlu?\n",
    "- `mean(dim=0)` ile batch tek ‘ortalama örneğe’ indirgenir.\n",
    "- `expand` ile aynı ağırlık tüm örneklere uygulanır.\n",
    "- `fusion_w` şeklen (B,2) olsa da içerik olarak **her satır aynıdır** → sample-wise adaptiflik yok olur.\n",
    "\n",
    "### Eğitimde beklenen belirtiler\n",
    "- Batch kompozisyonu değiştikçe fusion_w değişir.\n",
    "- Aynı örnek farklı batch’te farklı ağırlık alabilir.\n",
    "- Genelleme ve stabilite olumsuz etkilenebilir.\n",
    "\n",
    "### En hızlı debug kontrolleri\n",
    "- Batch içi satır eşitliği:\n",
    "  - `(fusion_w - fusion_w[0:1]).abs().max()` ≈ 0 ise şüpheli.\n",
    "  - `fusion_w.std(dim=0)` ≈ 0 ise tüm örnekler aynı ağırlığı alıyordur.\n",
    "- Aynı sample farklı batch’lerde: fusion_w değişiyor mu?\n",
    "\n",
    "### Doğrusu (fix)\n",
    "```python\n",
    "s_cat = torch.cat([avg_s, max_s], dim=1)\n",
    "logits = fusion_router(s_cat).flatten(1)\n",
    "fusion_w = softmax(logits, dim=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c2309",
   "metadata": {},
   "source": [
    "## Senaryo 7: Detach ile CA’yı ‘öğrenemez’ hale getirmek (grad akışı kesilir)\n",
    "\n",
    "### Yapılan değişiklik\n",
    "```python\n",
    "ca = gate_fn((z / T).detach())\n",
    "```\n",
    "\n",
    "### Neden sorunlu?\n",
    "- `detach()` hesaplama grafiğinden koparır → `ca` üzerinden `z` ve `T` tarafına gradyan akmaz.\n",
    "- Sonuç: fusion_router, MLP (fc1/fc2) ve learnable T (t_raw) attention üzerinden öğrenmeyi kaybeder.\n",
    "- Model yine öğrenebilir (backbone), fakat CA bloğu ‘var ama öğrenmeyen’ hale gelir.\n",
    "\n",
    "### Eğitimde beklenen belirtiler\n",
    "- CA parametrelerinin grad’leri yok/çok küçük olabilir.\n",
    "- CA maskesi epoch boyunca anlamlı değişmeyebilir.\n",
    "- Performans artışı beklenen seviyeye ulaşmayabilir.\n",
    "\n",
    "### En hızlı debug kontrolleri\n",
    "- Gradient kontrolü:\n",
    "  - `fc1.weight.grad`, `fc2.weight.grad`, `fusion_router[*].weight.grad`, `t_raw.grad`.\n",
    "- Ablation: detach’li vs detach’siz performans.\n",
    "- `ca` dağılımının zamanla değişimi.\n",
    "\n",
    "### Doğrusu (fix)\n",
    "```python\n",
    "ca = gate_fn(z / T)\n",
    "```\n",
    "\n",
    "**Not:** Detach, teacher-student/target network gibi yapılarda bilinçli kullanılır; attention maskesi öğrenmesi isteniyorsa genelde kullanılmaz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6abbd",
   "metadata": {},
   "source": [
    "## Mini Kontrol Listesi (CA)\n",
    "\n",
    "1) **Softmax ekseni**  \n",
    "- `fusion_w` (B,2) → `sum(dim=1) ≈ 1`\n",
    "\n",
    "2) **Temperature mantığı**  \n",
    "- Doğru: `z / T` (T↑ → yumuşar)  \n",
    "- Yanlış: `z * T` (T↑ → sertleşir)\n",
    "\n",
    "3) **Learnable T stabilitesi**  \n",
    "- `T.min()` çok küçükse (örn. `< 1e-4`) risk.\n",
    "\n",
    "4) **MLP nonlinearity**  \n",
    "- Aktivasyon yoksa lineerleşir; ayrıştırıcılık düşebilir.\n",
    "\n",
    "5) **Sample-wise adaptiflik**  \n",
    "- `fusion_w` satırları aynıysa (`std≈0`) sample-wise ölür.\n",
    "\n",
    "6) **Gradient akışı**  \n",
    "- `detach` varsa CA parametreleri grad alamaz.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
