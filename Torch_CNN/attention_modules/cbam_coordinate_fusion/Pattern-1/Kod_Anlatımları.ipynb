{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e4499c",
   "metadata": {},
   "source": [
    "# CBAM (Channel) + Coordinate Attention (Yontem 1) — Cok Detayli Calisma Defteri\n",
    "\n",
    "Bu defter, **tek tek kod bloklarini** ve bu bloklarin **birbiriyle nasil baglandigini** adim adim aciklar.\n",
    "\n",
    "Hedef: Bir feature map uzerinde\n",
    "- once **kanal secimi** (CBAM kanal),\n",
    "- sonra **eksensel uzamsal secicilik** (Coordinate attention),\n",
    "- en sonda da **kontrollu residual karisim**\n",
    "uygulamak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d44871",
   "metadata": {},
   "source": [
    "## 0) Okuma rehberi\n",
    "\n",
    "Bu defterde her parca su formatta ilerler:\n",
    "1) Kod blogu\n",
    "2) O blogun amaci\n",
    "3) O blogun girdi/cikti sekilleri (shape)\n",
    "4) Bir sonraki bloga nasil baglandigi\n",
    "\n",
    "Not: Kod hucreleri calistirilabilir olacak sekilde duzenlenmistir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb97ef5",
   "metadata": {},
   "source": [
    "## 1) Kurulum\n",
    "\n",
    "Bu bolumde sadece PyTorch import edilir. Moduller burada tanimlanacagi icin ekstra bagimlilik yok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c422440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2223226",
   "metadata": {},
   "source": [
    "## 2) Yardimci fonksiyonlar — neden gerekli?\n",
    "\n",
    "Bu projede iki tip ayar surekli karsina cikar:\n",
    "\n",
    "- **Gate secimi**: attention ciktisini 0–1 araligina sikistirma\n",
    "- **Sicaklik (temperature)**: gate'in ne kadar keskin/sert davranacagini ayarlama\n",
    "\n",
    "Ayrica kucuk MLP benzeri kisimlar icin aktivasyon secimi yapilir.\n",
    "\n",
    "Asagidaki yardimcilar bunlari tek yerden yonetmek icin var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _softplus_inverse(y: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"Softplus ile pozitif kalacak bir parametreyi, hedef baslangic degerine oturtmak icin ters donusum.\"\"\"\n",
    "    return torch.log(torch.clamp(torch.exp(y) - 1.0, min=eps))\n",
    "\n",
    "\n",
    "def _get_gate(gate: str):\n",
    "    \"\"\"Attention maskesini 0–1 araligina tasiyan fonksiyon secimi.\"\"\"\n",
    "    g = gate.lower()\n",
    "    if g == \"sigmoid\":\n",
    "        return torch.sigmoid\n",
    "    if g == \"hardsigmoid\":\n",
    "        return F.hardsigmoid\n",
    "    raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmali.\")\n",
    "\n",
    "\n",
    "def _get_act(act: str):\n",
    "    \"\"\"Kucuk MLP bloklarinda aktivasyon secimi.\"\"\"\n",
    "    a = act.lower()\n",
    "    if a == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    if a == \"silu\":\n",
    "        return nn.SiLU(inplace=True)\n",
    "    raise ValueError(\"act 'relu' veya 'silu' olmali.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f4676",
   "metadata": {},
   "source": [
    "### 2.1 `_softplus_inverse` nasil dusunulmeli?\n",
    "\n",
    "Ogrenilebilir bir parametrenin **pozitif kalmasi** isteniyorsa (or. temperature), tipik yaklasim su:\n",
    "\n",
    "- Iceride \"ham\" bir parametre tutulur\n",
    "- Forward'da `softplus(ham)` uygulanir ve pozitif deger uretilir\n",
    "\n",
    "Ama baslangicta temperature'i belirli bir degere oturtmak istersen,\n",
    "ham parametreyi oyle ayarlamak gerekir ki `softplus(ham)` yaklasik hedefi versin.\n",
    "\n",
    "`_softplus_inverse` bu baslangic ayarini pratik hale getirir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8e4a3",
   "metadata": {},
   "source": [
    "### 2.2 Gate fonksiyonu neden iki secenek?\n",
    "\n",
    "- `sigmoid`: daha yumusak; kucuk degisimlere daha duyarli\n",
    "- `hardsigmoid`: daha ucuz ve parcali dogrusal; bazi senaryolarda daha stabil\n",
    "\n",
    "Bu secim maskenin davranisini etkiler (cok agresif mi, yumusak mi)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa145a",
   "metadata": {},
   "source": [
    "## 3) Channel Attention (CBAM kanal) — butun akis\n",
    "\n",
    "Bu modul sunu yapar:\n",
    "\n",
    "1) `x` (B,C,H,W) -> iki ozet: avg ve max (B,C,1,1)\n",
    "2) Her ozet kucuk bir MLP'den gecer -> iki aday kanal skoru uretir\n",
    "3) Bu iki aday skor ya toplanir ya da **softmax ile agirliklandirilarak** birlestirilir\n",
    "4) Gate uygulanir -> kanal maskesi `ca` (B,C,1,1)\n",
    "5) `y = x * ca` ile kanal bazinda yeniden olceklenir\n",
    "\n",
    "Onemli: Bu modul uzamsal (H,W) yapiyi bozmaz; sadece kanallari olcekler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525aa780",
   "metadata": {},
   "source": [
    "### 3.1 Kod — ChannelAttentionFusionT\n",
    "\n",
    "Bu surumde `fusion=softmax` opsiyonu var.\n",
    "Bu ne demek? Model \"avg mi daha faydali, max mi?\" sorusunu ornek bazinda cevaplayabilir.\n",
    "\n",
    "Ayrica `temperature` var: maskeyi daha yumusak ya da daha keskin yapmaya yarar.\n",
    "\n",
    "`last = self.fusion_router[-1]` kismi:\n",
    "- `nn.Sequential` icindeki **son katmani** alir\n",
    "- Bu son katmanin agirlik/bias'ini 0 yaparak **baslangicta tarafsiz** bir router elde edilir\n",
    "  (softmax(logits) yaklasik [0.5, 0.5])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c37017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",\n",
    "        gate: str = \"sigmoid\",\n",
    "        temperature: float = 0.9,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        act: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "        fusion_router_hidden: int = 16,\n",
    "        return_fusion_weights: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmali.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmali.\")\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmali.\")\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmali.\")\n",
    "        if fusion_router_hidden < 1:\n",
    "            raise ValueError(\"fusion_router_hidden >= 1 olmali.\")\n",
    "\n",
    "        self.eps = float(eps)\n",
    "        self.fusion = fusion\n",
    "        self.return_fusion_weights = bool(return_fusion_weights)\n",
    "        self.gate_fn = _get_gate(gate)\n",
    "\n",
    "        hidden = max(int(min_hidden), int(channels) // int(reduction))\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=bias)\n",
    "        self.act = _get_act(act)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        if self.fusion == \"softmax\":\n",
    "            self.fusion_router = nn.Sequential(\n",
    "                nn.Conv2d(2 * channels, fusion_router_hidden, kernel_size=1, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(fusion_router_hidden, 2, kernel_size=1, bias=True),\n",
    "            )\n",
    "            last = self.fusion_router[-1]\n",
    "            nn.init.zeros_(last.weight)\n",
    "            nn.init.zeros_(last.bias)\n",
    "        else:\n",
    "            self.fusion_router = None\n",
    "\n",
    "        self.learnable_temperature = bool(learnable_temperature)\n",
    "        if self.learnable_temperature:\n",
    "            t0 = torch.tensor(float(temperature))\n",
    "            t_inv = _softplus_inverse(t0, eps=self.eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_s = self.avg_pool(x)  # (B,C,1,1)\n",
    "        max_s = self.max_pool(x)  # (B,C,1,1)\n",
    "\n",
    "        a = self.mlp(avg_s)       # (B,C,1,1)\n",
    "        m = self.mlp(max_s)       # (B,C,1,1)\n",
    "\n",
    "        fusion_w = None\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            s_cat = torch.cat([avg_s, max_s], dim=1)     # (B,2C,1,1)\n",
    "            logits = self.fusion_router(s_cat).flatten(1)  # (B,2)\n",
    "            fusion_w = torch.softmax(logits, dim=1)        # (B,2)\n",
    "\n",
    "            # (B,2) -> (B,1,1,1) yapinca a/m ile broadcast carpilabilir\n",
    "            w0 = fusion_w[:, 0].view(-1, 1, 1, 1)\n",
    "            w1 = fusion_w[:, 1].view(-1, 1, 1, 1)\n",
    "            z = w0 * a + w1 * m\n",
    "\n",
    "        T = self.get_T().to(device=x.device, dtype=x.dtype)\n",
    "        ca = self.gate_fn(z / T)  # (B,C,1,1)\n",
    "        y = x * ca                # (B,C,H,W)\n",
    "\n",
    "        if self.return_fusion_weights and (fusion_w is not None):\n",
    "            return y, ca, fusion_w\n",
    "        return y, ca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21879f0",
   "metadata": {},
   "source": [
    "### 3.2 `fusion_w[:,0].view(-1,1,1,1)` niye var?\n",
    "\n",
    "`fusion_w` sekli `(B,2)`.\n",
    "- `fusion_w[:,0]` -> `(B,)` (her ornek icin tek sayi)\n",
    "- `view(-1,1,1,1)` -> `(B,1,1,1)`\n",
    "\n",
    "Bu sekle sokunca su olur:\n",
    "- `a` ve `m` zaten `(B,C,1,1)`\n",
    "- `(B,1,1,1)` ile carpinca PyTorch bunu kanal boyutuna otomatik yayar (broadcast)\n",
    "\n",
    "Sonuc: her ornek icin tek bir agirlikla o ornegin tum kanallari olceklenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182868e1",
   "metadata": {},
   "source": [
    "### 3.3 `register_buffer` ile `nn.Parameter` farki\n",
    "\n",
    "- `nn.Parameter`: egitimde optimizer gorur, guncellenir\n",
    "- `register_buffer`: guncellenmez ama modelle birlikte tasinir (`state_dict` icinde, CPU/GPU tasimasi vs.)\n",
    "\n",
    "Bu projede:\n",
    "- temperature sabitse buffer\n",
    "- temperature ogrenilecekse Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2b629",
   "metadata": {},
   "source": [
    "## 4) Coordinate Attention (Plus) — butun akis\n",
    "\n",
    "Coordinate attention kismi, uzamsal dikkati tek bir 2D maske yerine **iki eksende** uretir:\n",
    "- H yonu icin ayri bir maske\n",
    "- W yonu icin ayri bir maske\n",
    "\n",
    "Akis (ozet):\n",
    "1) `h_profile`: W boyunca ozet -> `(B,C,H,1)`\n",
    "2) `w_profile`: H boyunca ozet -> `(B,C,1,W)`\n",
    "3) Local + dilated depthwise ile cok olcekli filtreleme\n",
    "4) H ve W profillerini birlestirip ortak bottleneck'ten gecirme\n",
    "5) Tekrar H ve W'ye ayirip ayri head'lerle maske uretme\n",
    "6) Alpha ve beta ile \"ne kadar uygulanacagini\" yumusatma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd8c9f",
   "metadata": {},
   "source": [
    "### 4.1 Aktivasyon ve norm yardimcilari\n",
    "\n",
    "Coordinate kisminda kucuk bottleneck ve refine bloklari var.\n",
    "- Aktivasyon: HSwish (mobil tarzi, stabil ve ucuz)\n",
    "- Normalizasyon: GN genelde detection / small batch icin daha stabil olabiliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e277e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HSwish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0\n",
    "\n",
    "\n",
    "def make_norm(norm: str, ch: int):\n",
    "    norm = norm.lower()\n",
    "    if norm == \"bn\":\n",
    "        return nn.BatchNorm2d(ch)\n",
    "    if norm == \"gn\":\n",
    "        g = min(32, ch)\n",
    "        while ch % g != 0 and g > 2:\n",
    "            g //= 2\n",
    "        if ch % g != 0:\n",
    "            g = 2 if (ch % 2 == 0) else 1\n",
    "        return nn.GroupNorm(g, ch)\n",
    "    if norm == \"none\":\n",
    "        return nn.Identity()\n",
    "    raise ValueError(\"norm 'none', 'bn', 'gn' disinda olamaz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9225c67",
   "metadata": {},
   "source": [
    "### 4.2 `mid_floor` ve `mid` secimi neden katmanli?\n",
    "\n",
    "Coordinate attention'da `mid` (bottleneck kanal sayisi) cok kucuk olursa:\n",
    "- H/W bilgisi yeterince tasinamaz\n",
    "- maske kalitesi dusus gorebilir\n",
    "\n",
    "Cok buyuk olursa:\n",
    "- hesap pahali\n",
    "- maske gereksiz agresiflesebilir\n",
    "\n",
    "Bu yuzden uc kontrol birlikte kullaniliyor:\n",
    "- `in_channels // reduction` (dogal bottleneck)\n",
    "- `min_mid_channels` (alt limit)\n",
    "- `mid_floor` (pratik taban, \"fazla kuculmesin\")\n",
    "\n",
    "Bu ucunun max'ini almak: her kanal sayisinda \"makul bir bottleneck\" hedefler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51194ece",
   "metadata": {},
   "source": [
    "### 4.3 Kod — CoordinateAttPlus (detayli)\n",
    "\n",
    "Onemli parcalar:\n",
    "- `h_local_dw` / `w_local_dw`: eksen yonlu lokal depthwise\n",
    "- `h_dilated_dw` / `w_dilated_dw`: eksen yonlu dilated depthwise (genis baglam)\n",
    "- `*_channel_mixer`: 1x1 ile kanallari karistirip profili guclendirme\n",
    "- `shared_bottleneck_*`: H ve W bilgisini ortak ara uzayda birlestirme\n",
    "- `h_attention_head` / `w_attention_head`: iki eksen icin ayri maske uretme\n",
    "- `alpha_*`: eksen bazinda \"attention ne kadar devreye girsin\" kontrolu\n",
    "- `beta`: genel kuvvet ayari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoordinateAttPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        reduction: int = 32,\n",
    "        min_mid_channels: int = 8,\n",
    "        act: str = \"hswish\",\n",
    "        init_alpha: float = 0.7,\n",
    "        learnable_alpha: bool = True,\n",
    "        beta: float = 0.35,\n",
    "        dilation: int = 2,\n",
    "        norm: str = \"gn\",\n",
    "        use_spatial_gate: bool = False,\n",
    "        spatial_gate_beta: float = 0.35,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if in_channels < 1:\n",
    "            raise ValueError(\"in_channels >= 1 olmali.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmali.\")\n",
    "        if dilation < 1:\n",
    "            raise ValueError(\"dilation >= 1 olmali.\")\n",
    "\n",
    "        # Bottleneck kanal sayisi: cok dusmesin diye taban koyuluyor\n",
    "        mid_floor = max(8, min(32, int(in_channels) // 4))\n",
    "        mid = max(int(min_mid_channels), int(in_channels) // int(reduction))\n",
    "        mid = max(mid, int(mid_floor))\n",
    "\n",
    "        # Aktivasyon secimi\n",
    "        act_l = act.lower()\n",
    "        if act_l == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        elif act_l == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act_l == \"silu\":\n",
    "            self.act = nn.SiLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act 'hswish', 'relu', 'silu' olmali.\")\n",
    "\n",
    "        # Ortak bottleneck (H+W birlikte)\n",
    "        self.shared_bottleneck_proj = nn.Conv2d(in_channels, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_norm = make_norm(norm, mid)\n",
    "        self.shared_bottleneck_refine = nn.Conv2d(mid, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_refine_norm = make_norm(norm, mid)\n",
    "\n",
    "        # Eksen yonlu lokal depthwise\n",
    "        self.h_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(3, 1), padding=(1, 0), groups=in_channels, bias=False\n",
    "        )\n",
    "        self.w_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(1, 3), padding=(0, 1), groups=in_channels, bias=False\n",
    "        )\n",
    "\n",
    "        # Eksen yonlu dilated depthwise (genis baglam)\n",
    "        d = int(dilation)\n",
    "        self.h_dilated_dw = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=(3, 1),\n",
    "            padding=(d, 0),\n",
    "            dilation=(d, 1),\n",
    "            groups=in_channels,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.w_dilated_dw = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=(1, 3),\n",
    "            padding=(0, d),\n",
    "            dilation=(1, d),\n",
    "            groups=in_channels,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # 1x1 karistirma: kanallari guclendirip profili daha esnek yapar\n",
    "        self.h_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        self.w_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        # Iki eksen icin maske head'leri (mid -> C)\n",
    "        self.h_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        self.w_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "\n",
    "        # Genel kuvvet ayari\n",
    "        self.beta = float(beta)\n",
    "\n",
    "        # Alpha parametreleri (0-1 araliginda kalacak)\n",
    "        eps = 1e-6\n",
    "        a0 = float(init_alpha)\n",
    "        a0 = min(max(a0, eps), 1.0 - eps)\n",
    "        raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha_h_raw = nn.Parameter(raw0.clone())\n",
    "            self.alpha_w_raw = nn.Parameter(raw0.clone())\n",
    "        else:\n",
    "            self.register_buffer(\"alpha_h_raw\", raw0.clone())\n",
    "            self.register_buffer(\"alpha_w_raw\", raw0.clone())\n",
    "\n",
    "        # Opsiyonel ek spatial gate (daha agresif olabilir)\n",
    "        self.use_spatial_gate = bool(use_spatial_gate)\n",
    "        self.spatial_gate_beta = float(spatial_gate_beta)\n",
    "        if self.use_spatial_gate:\n",
    "            self.spatial_gate_dw = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False)\n",
    "            self.spatial_gate_pw = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        # Debug icin son maskeleri sakla\n",
    "        self._last_ah = None\n",
    "        self._last_aw = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        _, _, H, W = x.shape\n",
    "\n",
    "        # Eksen ozetleri: mean + max karisimi\n",
    "        h_profile = 0.5 * (x.mean(dim=3, keepdim=True) + x.amax(dim=3, keepdim=True))  # (B,C,H,1)\n",
    "        w_profile = 0.5 * (x.mean(dim=2, keepdim=True) + x.amax(dim=2, keepdim=True))  # (B,C,1,W)\n",
    "\n",
    "        # Cok olcekli (lokal + dilated) filtreleme + 1x1 karistirma\n",
    "        h_ms = self.h_channel_mixer(self.h_local_dw(h_profile) + self.h_dilated_dw(h_profile))  # (B,C,H,1)\n",
    "        w_ms = self.w_channel_mixer(self.w_local_dw(w_profile) + self.w_dilated_dw(w_profile))  # (B,C,1,W)\n",
    "\n",
    "        # Cat icin w'yi (B,C,W,1) yap\n",
    "        w_ms = w_ms.permute(0, 1, 3, 2)\n",
    "\n",
    "        # (B,C,H,1) + (B,C,W,1) -> (B,C,H+W,1)\n",
    "        hw = torch.cat([h_ms, w_ms], dim=2)\n",
    "\n",
    "        # Ortak bottleneck: H ve W birlikte ogrenir\n",
    "        mid = self.act(self.shared_bottleneck_norm(self.shared_bottleneck_proj(hw)))\n",
    "        mid = self.act(self.shared_bottleneck_refine_norm(self.shared_bottleneck_refine(mid)))\n",
    "\n",
    "        # Tekrar ayir\n",
    "        mid_h, mid_w = torch.split(mid, [H, W], dim=2)\n",
    "        mid_w = mid_w.permute(0, 1, 3, 2)  # (B,mid,W,1) -> (B,mid,1,W)\n",
    "\n",
    "        # Maskeler\n",
    "        attn_h = F.hardsigmoid(self.h_attention_head(mid_h), inplace=False)  # (B,C,H,1)\n",
    "        attn_w = F.hardsigmoid(self.w_attention_head(mid_w), inplace=False)  # (B,C,1,W)\n",
    "\n",
    "        self._last_ah = attn_h.detach()\n",
    "        self._last_aw = attn_w.detach()\n",
    "\n",
    "        # Alpha: ham -> sigmoid ile 0-1\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw).to(device=x.device, dtype=x.dtype)\n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # Yumusak karisim: alpha=0 -> etki yok, alpha=1 -> tam mask\n",
    "        scale_h = (1.0 - alpha_h) + alpha_h * attn_h\n",
    "        scale_w = (1.0 - alpha_w) + alpha_w * attn_w\n",
    "\n",
    "        # Iki eksen birlesir\n",
    "        scale = scale_h * scale_w\n",
    "\n",
    "        # Beta ile genel kuvvet kontrolu\n",
    "        scale = 1.0 + self.beta * (scale - 1.0)\n",
    "\n",
    "        out = x * scale\n",
    "\n",
    "        if self.use_spatial_gate:\n",
    "            sg = self.spatial_gate_pw(self.spatial_gate_dw(x))\n",
    "            sg = F.hardsigmoid(sg, inplace=False)\n",
    "            sg = 1.0 + self.spatial_gate_beta * (sg - 1.0)\n",
    "            out = out * sg\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def last_mask_stats(self):\n",
    "        if (self._last_ah is None) or (self._last_aw is None):\n",
    "            return None\n",
    "        ah = self._last_ah\n",
    "        aw = self._last_aw\n",
    "        return {\n",
    "            \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "            \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511ec64",
   "metadata": {},
   "source": [
    "### 4.4 `init_alpha -> raw0 (logit uzayi)` mantigi\n",
    "\n",
    "Buradaki fikir:\n",
    "\n",
    "- Kullanici `init_alpha=0.7` diyor (0–1 araliginda kontrol degeri)\n",
    "- Iceride dogrudan 0.7 tutmak yerine \"ham\" deger tutuluyor\n",
    "- Forward'da `sigmoid(ham)` ile tekrar 0–1'e donuluyor\n",
    "\n",
    "Bu yaklasim:\n",
    "- ogrenmeyi stabil yapar\n",
    "- parametreyi dogal olarak 0–1 araliginda tutar\n",
    "\n",
    "`eps` eklenme sebebi:\n",
    "- 0 ve 1 civarinda sayisal sorunlar olmasin diye guvenlik payi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda3cb0",
   "metadata": {},
   "source": [
    "### 4.5 `0.5*(mean + amax)` profilleri niye var?\n",
    "\n",
    "Tek basina mean:\n",
    "- daha stabil bir ozet verir\n",
    "- ama sivri (peak) sinyalleri zayif gosterebilir\n",
    "\n",
    "Tek basina max:\n",
    "- sivri sinyali yakalar\n",
    "- ama gurultuye daha acik olabilir\n",
    "\n",
    "Ikisini yarim yarim birlestirmek:\n",
    "- mean'in stabilitesini\n",
    "- max'in seciciligini\n",
    "aynanda tasir.\n",
    "\n",
    "Burada ozet, her kanal icin H veya W boyunca cikartilir. Bu yuzden \"konumla baglantili\" sinyal yakalanir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead0f35",
   "metadata": {},
   "source": [
    "## 5) Birlesim blogu — Yontem 1\n",
    "\n",
    "Bu blok:\n",
    "1) `x` -> Channel attention -> `y`\n",
    "2) `y` -> Coordinate attention -> `y2`\n",
    "3) Residual aciksa `x` ile `y2` kontrollu karisir\n",
    "\n",
    "Residual neden onemli?\n",
    "- Iki attention ust uste gelince bazen fazla bastirma olur\n",
    "- Residual, etkisini ayarlanabilir ve daha guvenli hale getirir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfab30e",
   "metadata": {},
   "source": [
    "### 5.1 Residual alpha (blok duzeyi) neden ayri?\n",
    "\n",
    "Coordinate'in icinde zaten `alpha_h` ve `alpha_w` var. Onlar eksen bazli yumusatma.\n",
    "\n",
    "Buradaki `alpha_raw` ise butun blogun etkisini ayarlar:\n",
    "- kanal + coord kombinasyonunun tamamini\n",
    "\n",
    "Iki seviyeli kontrol, pratikte daha stabil bir davranis verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384115ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CBAMChannelPlusCoord(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        ca_reduction: int = 16,\n",
    "        ca_min_hidden: int = 4,\n",
    "        ca_fusion: str = \"softmax\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        ca_temperature: float = 0.9,\n",
    "        ca_act: str = \"relu\",\n",
    "        ca_fusion_router_hidden: int = 16,\n",
    "        learnable_temperature: bool = False,\n",
    "        coord_reduction: int = 32,\n",
    "        coord_min_mid: int = 8,\n",
    "        coord_act: str = \"hswish\",\n",
    "        coord_init_alpha: float = 0.7,\n",
    "        coord_learnable_alpha: bool = True,\n",
    "        coord_beta: float = 0.35,\n",
    "        coord_dilation: int = 2,\n",
    "        coord_norm: str = \"gn\",\n",
    "        coord_use_spatial_gate: bool = False,\n",
    "        coord_spatial_gate_beta: float = 0.35,\n",
    "        residual: bool = True,\n",
    "        alpha_init: float = 0.75,\n",
    "        learnable_alpha: bool = False,\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmali.\")\n",
    "\n",
    "        self.return_maps = bool(return_maps)\n",
    "        self.residual = bool(residual)\n",
    "\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=ca_reduction,\n",
    "            min_hidden=ca_min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            eps=1e-6,\n",
    "            act=ca_act,\n",
    "            bias=True,\n",
    "            fusion_router_hidden=ca_fusion_router_hidden,\n",
    "            return_fusion_weights=self.return_maps,\n",
    "        )\n",
    "\n",
    "        self.coord = CoordinateAttPlus(\n",
    "            in_channels=channels,\n",
    "            reduction=coord_reduction,\n",
    "            min_mid_channels=coord_min_mid,\n",
    "            act=coord_act,\n",
    "            init_alpha=coord_init_alpha,\n",
    "            learnable_alpha=coord_learnable_alpha,\n",
    "            beta=coord_beta,\n",
    "            dilation=coord_dilation,\n",
    "            norm=coord_norm,\n",
    "            use_spatial_gate=coord_use_spatial_gate,\n",
    "            spatial_gate_beta=coord_spatial_gate_beta,\n",
    "        )\n",
    "\n",
    "        # Blok seviyesinde residual karisim icin alpha\n",
    "        if self.residual:\n",
    "            eps = 1e-6\n",
    "            a0 = float(alpha_init)\n",
    "            a0 = min(max(a0, eps), 1.0 - eps)\n",
    "            raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "            if learnable_alpha:\n",
    "                self.alpha_raw = nn.Parameter(raw0)\n",
    "            else:\n",
    "                self.register_buffer(\"alpha_raw\", raw0)\n",
    "\n",
    "    def _alpha(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # residual=True iken alpha_raw yoksa (normalde olmamali) alpha=1.0 ile fallback\n",
    "        if not hasattr(self, \"alpha_raw\"):\n",
    "            return x.new_tensor(1.0)\n",
    "        return torch.sigmoid(self.alpha_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.return_maps:\n",
    "            y, ca_map, fusion_w = self.ca(x)  # channel\n",
    "            y = self.coord(y)                 # coordinate\n",
    "\n",
    "            if self.residual:\n",
    "                out = x + self._alpha(x) * (y - x)\n",
    "            else:\n",
    "                out = y\n",
    "\n",
    "            coord_stats = self.coord.last_mask_stats()\n",
    "            return out, ca_map, fusion_w, coord_stats\n",
    "\n",
    "        y, _ = self.ca(x)\n",
    "        y = self.coord(y)\n",
    "\n",
    "        if self.residual:\n",
    "            out = x + self._alpha(x) * (y - x)\n",
    "        else:\n",
    "            out = y\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a19d63",
   "metadata": {},
   "source": [
    "### 5.2 `x.new_tensor(1.0)` ne demek?\n",
    "\n",
    "`new_tensor` su garantiyi verir:\n",
    "- 1.0 degeri **x ile ayni cihazda** olusur (CPU/GPU)\n",
    "- 1.0 degeri **x ile uyumlu dtype** ile olusur (float16/float32)\n",
    "\n",
    "Bu, \"GPU tensoru ile CPU scalari carpma\" gibi hatalari engeller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e239295a",
   "metadata": {},
   "source": [
    "## 6) Test: shape ve istatistikleri okuma\n",
    "\n",
    "Bu hucre bes seyi ayni anda kontrol eder:\n",
    "- giris/cikis shape ayni mi?\n",
    "- `ca_map` kanal maskesi mi?\n",
    "- `fusion_w` router agirliklari mi?\n",
    "- coordinate maskeleri makul aralikta mi?\n",
    "- dikkat bloklari fazla bastiriyor mu? (kaba kontrol)\n",
    "\n",
    "Not: `float(tensor)` uyarisi gormemek icin `.detach()` kullanilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tensor_stats(t: torch.Tensor):\n",
    "    t = t.detach()\n",
    "    return {\n",
    "        \"min\": float(t.min()),\n",
    "        \"mean\": float(t.mean()),\n",
    "        \"max\": float(t.max()),\n",
    "        \"std\": float(t.std()),\n",
    "    }\n",
    "\n",
    "x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "m = CBAMChannelPlusCoord(\n",
    "    channels=64,\n",
    "    return_maps=True,\n",
    "    residual=True,\n",
    "    alpha_init=0.75,\n",
    "    learnable_alpha=False,\n",
    "    learnable_temperature=True,\n",
    "    ca_temperature=0.9,\n",
    "    coord_beta=0.35,\n",
    "    coord_dilation=2,\n",
    "    coord_norm=\"gn\",\n",
    ")\n",
    "\n",
    "out, ca_map, fusion_w, coord_stats = m(x)\n",
    "\n",
    "print(\"x:\", x.shape)\n",
    "print(\"out:\", out.shape)\n",
    "print(\"ca_map:\", ca_map.shape)\n",
    "print(\"fusion_w:\", fusion_w.shape)\n",
    "print(\"coord_stats:\", coord_stats)\n",
    "\n",
    "print(\"CA stats:\", tensor_stats(ca_map))\n",
    "print(\"fusion_w mean:\", fusion_w.detach().mean(dim=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fdf2b1",
   "metadata": {},
   "source": [
    "## 7) Debug checklist — kotu gorunurse neye bakilir?\n",
    "\n",
    "Bu bolum pratik: egitimde veya forward testinde istatistikler kotu gorunurse hizli teshis.\n",
    "\n",
    "### 7.1 `ca_map` cok kucukse\n",
    "- `ca_map.mean` dusuk (or. 0.2–0.3) ve `out` dagilimi cok daraliyorsa:\n",
    "  - `ca_temperature` arttirmak maskeyi yumusatir\n",
    "  - `alpha_init` dusurmek (residual etkisi) blogun genel gucunu azaltir\n",
    "\n",
    "### 7.2 `ca_map` hep 1'e yakin ise\n",
    "- attention etkisizlesmis olabilir:\n",
    "  - temperature biraz dusur\n",
    "  - gate degistir (sigmoid/hardsigmoid)\n",
    "\n",
    "### 7.3 Coordinate maskeleri cok oynaksa\n",
    "- `coord_stats['a_h']['std']` ve `a_w['std']` cok yuksekse:\n",
    "  - `coord_beta` dusur\n",
    "  - `coord_dilation` azalt\n",
    "  - `coord_use_spatial_gate=False` tut\n",
    "\n",
    "### 7.4 `fusion_w` hemen kilitleniyorsa\n",
    "- `fusion_w` hizla tek tarafa gidiyorsa (or. [0.95, 0.05]):\n",
    "  - router kapasitesi (fusion_router_hidden)\n",
    "  - learning rate\n",
    "  - temperature\n",
    "  ayarlari etkiler.\n",
    "\n",
    "Bu checklist, ciktilari \"iyi mi kotu mu\" hizli yorumlamak icin."
   ]
  }
 ],
 "metadata": {
  "created": "2025-12-28T00:44:16.772346Z",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
