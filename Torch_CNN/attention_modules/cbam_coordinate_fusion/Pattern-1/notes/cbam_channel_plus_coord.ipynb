{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e9e4a1",
   "metadata": {},
   "source": [
    "# YÃ¶ntem 1: **CBAM Channel + Coordinate Attention** (Tek Spatial Mekanizma)\n",
    "\n",
    "Bu defter, **CBAMâ€™den sadece kanal dikkatini (Channel Attention)** alÄ±p, **uzamsal/konumsal tarafÄ± Coordinate Attention ile** Ã§Ã¶zmeyi anlatÄ±r.  \n",
    "AmaÃ§: **iki farklÄ± spatial gateâ€™i Ã¼st Ã¼ste bindirmeden** (overâ€‘suppression riskini artÄ±rmadan) hem kanal seÃ§iciliÄŸi hem de eksenâ€‘tabanlÄ± konum duyarlÄ±lÄ±ÄŸÄ± elde etmek.\n",
    "\n",
    "---\n",
    "\n",
    "## Neden â€œYÃ¶ntem 1â€?\n",
    "CBAMâ€™in klasik yapÄ±sÄ±: **CA (channel) + SA (spatial)**.  \n",
    "Coordinate Attention ise **(H ve W eksenlerinden) konum bilgisi taÅŸÄ±yan** bir tÃ¼r spatial gating Ã¼retir.\n",
    "\n",
    "Bu nedenle:\n",
    "- CBAMâ€™in **SA** kÄ±smÄ±nÄ± da aÃ§Ä±p Ã¼stÃ¼ne Coordinate eklemek â†’ Ã§oÄŸu senaryoda *maskelerin Ã§arpÄ±lmasÄ±* gibi davranÄ±r ve **aÅŸÄ±rÄ± bastÄ±rma** yapabilir.\n",
    "- Daha stabil tasarÄ±m: **CBAMâ€™den CA al â†’ Coordinate uygula â†’ residual ile karÄ±ÅŸtÄ±r**.\n",
    "\n",
    "Bu defterde tam olarak bu tasarÄ±m uygulanÄ±r:\n",
    "\n",
    "\\[ x \\rightarrow \\text{ChannelAttentionFusionT} \\rightarrow \\text{CoordinateAttPlus} \\rightarrow \\text{Residual Mix} \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92d482",
   "metadata": {},
   "source": [
    "## BloklarÄ±n kÄ±sa Ã¶zeti\n",
    "\n",
    "### 1) ChannelAttentionFusionT (CBAMâ€‘CAâ€™nÄ±n geliÅŸtirilmiÅŸ hali)\n",
    "- GiriÅŸten **global ortalama** ve **global maksimum** Ã¶zet Ã§Ä±karÄ±r.\n",
    "- AynÄ± MLP (1Ã—1 conv) Ã¼zerinden iki Ã¶zet iÅŸlenir.\n",
    "- **Fusion (sum veya softmax router)** ile `avg` ve `max` katkÄ±larÄ± birleÅŸtirilir.\n",
    "- SonuÃ§ **temperature (T)** ile Ã¶lÃ§eklenir ve **sigmoid / hardsigmoid** ile kanal maskesi Ã¼retilir.\n",
    "\n",
    "Ã–nemli detaylar:\n",
    "- `fusion=\"softmax\"` seÃ§ilirse router son katmanÄ± **sÄ±fÄ±r init** yapÄ±lÄ±r â†’ eÄŸitim baÅŸÄ±nda **avg/max dengeli** baÅŸlar.\n",
    "- `temperature=0.9` gibi deÄŸerler maskeyi biraz daha keskinleÅŸtirir (Ã§ok yÃ¼ksek T â†’ fazla yumuÅŸak, Ã§ok dÃ¼ÅŸÃ¼k T â†’ Ã§ok sert).\n",
    "\n",
    "### 2) CoordinateAttPlus (H/W eksen profili + multiâ€‘scale DW conv)\n",
    "- `H_profile`: W boyunca Ã¶zet (mean + max karÄ±ÅŸÄ±mÄ±) â†’ ÅŸekil `(B,C,H,1)`\n",
    "- `W_profile`: H boyunca Ã¶zet (mean + max karÄ±ÅŸÄ±mÄ±) â†’ ÅŸekil `(B,C,1,W)`\n",
    "- Her eksen iÃ§in:\n",
    "  - **local depthwise** (k=3)\n",
    "  - **dilated depthwise** (k=3, dilation=d)\n",
    "  - sonra **1Ã—1 channel mixer**\n",
    "- H ve W Ã¶zellikleri birleÅŸtirilir, ortak bottleneckâ€™ten geÃ§er, tekrar split edilir.\n",
    "- `attn_h` ve `attn_w` ile eksenâ€‘maskeleri Ã¼retilir ve Ã§arpÄ±lÄ±p **beta** ile yumuÅŸatÄ±lÄ±r.\n",
    "\n",
    "Ã–nemli detaylar:\n",
    "- `mid` (bottleneck geniÅŸliÄŸi) kÃ¼Ã§Ã¼k kalÄ±rsa temsil gÃ¼cÃ¼ dÃ¼ÅŸer.  \n",
    "  Bu yÃ¼zden `mid_floor = max(8, min(32, C//4))` gibi adaptif taban kullanÄ±lÄ±r.\n",
    "- `beta` spatial Ã¶lÃ§eklemeyi yumuÅŸatÄ±r: `scale = 1 + beta*(scale-1)`\n",
    "- `alpha_h/alpha_w` eksen maskesinin gÃ¼cÃ¼nÃ¼ Ã¶ÄŸrenilebilir ÅŸekilde ayarlar (0â†’noâ€‘effect, 1â†’tam mask).\n",
    "\n",
    "### 3) Residual Mix\n",
    "Son Ã§Ä±ktÄ±:\n",
    "- residual aÃ§Ä±kken: `out = x + alpha*(y - x)`\n",
    "- residual kapalÄ±ysa: `out = y`\n",
    "\n",
    "Buradaki `alpha` da logit uzayÄ±nda tutulur, `sigmoid(alpha_raw)` ile [0,1] aralÄ±ÄŸÄ±na alÄ±nÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e589e48",
   "metadata": {},
   "source": [
    "## Pratik yerleÅŸtirme rehberi (CNN / YOLO gibi yapÄ±larda)\n",
    "\n",
    "Bu blok genelde ÅŸu noktalarda iyi Ã§alÄ±ÅŸÄ±r:\n",
    "- **Backbone orta katmanlarÄ±**: Ã–zellikle C=64/128/256 gibi kanallarÄ±n arttÄ±ÄŸÄ± bÃ¶lgeler.\n",
    "- **Neck (FPN/PAN)**: BirleÅŸtirme sonrasÄ± gÃ¼rÃ¼ltÃ¼yÃ¼ azaltÄ±p kanal seÃ§iciliÄŸi kazandÄ±rabilir.\n",
    "- **Headâ€™e Ã§ok yakÄ±n** yerlerde: daha agresif olabilir, dikkatli ayarlanmalÄ±.\n",
    "\n",
    "Ã–neri:\n",
    "- Ä°lk denemede **tek bir seviyede** ekle (Ã¶r. backboneâ€™un bir stageâ€™i).\n",
    "- Sonra katman sayÄ±sÄ±nÄ± artÄ±r veya neckâ€™e taÅŸÄ±.\n",
    "- Her eklemede: **std_ratio**, **mask stats**, **latency** kontrol et.\n",
    "\n",
    "Ä°lk ayar seti (stabil baÅŸlangÄ±Ã§):\n",
    "- CA: `fusion=\"softmax\"`, `temperature=0.9`, `learnable_temperature=True` (opsiyonel)\n",
    "- Coord: `beta=0.35`, `dilation=2`, `norm=\"gn\"`\n",
    "- Residual: `alpha_init=0.75`, `learnable_alpha=False` (baÅŸlangÄ±Ã§ta sabit tutmak stabilite saÄŸlar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465d634",
   "metadata": {},
   "source": [
    "## Kod\n",
    "\n",
    "AÅŸaÄŸÄ±daki kod; **ChannelAttentionFusionT**, **CoordinateAttPlus** ve wrapper **CBAMChannelPlusCoord** sÄ±nÄ±flarÄ±nÄ± iÃ§erir.  \n",
    "Kod iÃ§inde sadece kritik yerlerde kÄ±sa aÃ§Ä±klama vardÄ±r; asÄ±l mantÄ±k aÃ§Ä±klamasÄ± markdown hÃ¼crelerinde verilmiÅŸtir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd1842",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "## Kodun iÅŸleyiÅŸ kÄ±smÄ±na aÅŸina olanlar iÃ§indir.EÄŸer tam aÃ§Ä±klamalarÄ± incelemek isterseniz bu dosyayÄ± ziyaret edin :: \"Attention MekanizmalarÄ±\\CBAM + Cordinat Attention Block (5)\\YÃ¶ntem -1\\Kod_AnlatÄ±mlarÄ±.ipynb\"\n",
    "\n",
    "**Bu yapÄ±da amaÃ§, feature map Ã¼zerinde bilgiyi iki aÅŸamalÄ± ve kontrollÃ¼ ÅŸekilde yeniden aÄŸÄ±rlÄ±klandÄ±rmak. Ä°lk aÅŸamada kanal bazÄ±nda, ikinci aÅŸamada ise uzamsal (konumsal) bazda dikkat uygulanÄ±yor.**\n",
    "\n",
    "**Ä°lk olarak CBAMâ€™in kanal attention kÄ±smÄ± Ã§alÄ±ÅŸÄ±yor. Burada her kanal iÃ§in global ortalama ve global maksimum deÄŸerler Ã§Ä±karÄ±lÄ±yor. Ortalama deÄŸer, kanalÄ±n tÃ¼m uzay boyunca genel aktivasyon seviyesini temsil ederken; maksimum deÄŸer o kanaldaki en baskÄ±n, en ayÄ±rt edici sinyali temsil ediyor. Bu iki farklÄ± bakÄ±ÅŸ aÃ§Ä±sÄ±, kanalÄ±n gerÃ§ekten anlamlÄ± olup olmadÄ±ÄŸÄ±nÄ± deÄŸerlendirmek iÃ§in birlikte kullanÄ±lÄ±yor.**\n",
    "\n",
    "*Bu kanal Ã¶zetleri kÃ¼Ã§Ã¼k bir Ã¶ÄŸrenilebilir yapÄ±dan geÃ§irilerek her kanal iÃ§in bir attention deÄŸeri Ã¼retiliyor. Ortalama ve maksimum bilgilerin katkÄ±sÄ± sabit tutulmuyor; model, bu iki bilginin hangisinin daha Ã¶nemli olduÄŸunu Ã¶ÄŸrenebiliyor. Temperature parametresi burada devreye girerek attention maskesinin daha sert mi yoksa daha yumuÅŸak mÄ± uygulanacaÄŸÄ±nÄ± ayarlÄ±yor. BÃ¶ylece kanal attention, veri setine ve Ã¶ÄŸrenme sÃ¼recine gÃ¶re kendini adapte edebiliyor*.*\n",
    "\n",
    "**Kanal attention tamamlandÄ±ktan sonra Coordinate Attention aÅŸamasÄ±na geÃ§iliyor. Bu aÅŸamada artÄ±k soru â€œhangi kanal Ã¶nemliâ€ deÄŸil, â€œbu Ã¶nemli kanal gÃ¶rÃ¼ntÃ¼nÃ¼n hangi bÃ¶lgesinde etkiliâ€ sorusu oluyor. Bunun iÃ§in yÃ¼kseklik (H) ve geniÅŸlik (W) yÃ¶nleri ayrÄ± ayrÄ± ele alÄ±nÄ±yor. Feature map, H ve W eksenlerinde Ã¶zetlenerek iki farklÄ± uzamsal profil oluÅŸturuluyor. Bu Ã¶zetleme iÅŸlemi hem ortalama hem de maksimum deÄŸerler kullanÄ±larak yapÄ±lÄ±yor; bÃ¶ylece hem genel uzamsal daÄŸÄ±lÄ±m hem de lokal olarak baskÄ±n bÃ¶lgeler korunmuÅŸ oluyor.**\n",
    "\n",
    "*Bu uzamsal profiller, hem lokal hem de daha geniÅŸ baÄŸlamÄ± yakalayabilmek iÃ§in farklÄ± uzamsal iÅŸlemlerden geÃ§iriliyor. SonrasÄ±nda bu bilgiler birleÅŸtirilerek her kanal iÃ§in H ve W yÃ¶nlÃ¼ uzamsal attention maskeleri Ã¼retiliyor. Bu maskeler, kanal attentionâ€™dan geÃ§en feature mapâ€™e uygulanarak uzamsal olarak da seÃ§ici bir yapÄ± elde ediliyor.*\n",
    "\n",
    "**Coordinate Attentionâ€™da attentionâ€™Ä±n gÃ¼cÃ¼ doÄŸrudan sabitlenmiyor. Ã–ÄŸrenilebilir katsayÄ±lar sayesinde, uzamsal attentionâ€™Ä±n ne kadar baskÄ±n olacaÄŸÄ± ayarlanabiliyor. BÃ¶ylece model, bazÄ± durumlarda uzamsal dikkat mekanizmasÄ±nÄ± gÃ¼Ã§lÃ¼ ÅŸekilde kullanÄ±rken, bazÄ± durumlarda daha geri planda tutabiliyor**\n",
    "\n",
    "**Son aÅŸamada ise bu iki attention aÅŸamasÄ±ndan geÃ§en Ã§Ä±ktÄ±, residual bir yapÄ± ile orijinal feature map ile birleÅŸtiriliyor. Bu birleÅŸim de kontrollÃ¼ bir ÅŸekilde yapÄ±lÄ±yor. Yani model isterse attention etkisini azaltabiliyor, isterse tamamen kullanabiliyor. Bu yaklaÅŸÄ±m, Ã¶zellikle YOLO gibi hassas mimarilerde attentionâ€™Ä±n eÄŸitimi bozmasÄ±nÄ± veya gereÄŸinden fazla bastÄ±rma yapmasÄ±nÄ± engelliyor.**\n",
    "\n",
    "* Ã–zetle bu blok, kanal bazlÄ± Ã¶nemlendirme ile uzamsal konum bilgisini birlikte kullanarak, bilgiyi daha seÃ§ici, daha dengeli ve daha stabil bir ÅŸekilde Ã¶ne Ã§Ä±karan bir attention mekanizmasÄ± sunuyor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def _softplus_inverse(y: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return torch.log(torch.clamp(torch.exp(y) - 1.0, min=eps))\n",
    "\n",
    "\n",
    "def _get_gate(gate: str):\n",
    "    g = gate.lower()\n",
    "    if g == \"sigmoid\":\n",
    "        return torch.sigmoid\n",
    "    if g == \"hardsigmoid\":\n",
    "        return F.hardsigmoid\n",
    "    raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalÄ±.\")\n",
    "\n",
    "\n",
    "def _get_act(act: str):\n",
    "    a = act.lower()\n",
    "    if a == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    if a == \"silu\":\n",
    "        return nn.SiLU(inplace=True)\n",
    "    raise ValueError(\"act 'relu' veya 'silu' olmalÄ±.\")\n",
    "\n",
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",\n",
    "        gate: str = \"sigmoid\",\n",
    "        temperature: float = 0.9,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        act: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "        fusion_router_hidden: int = 16,\n",
    "        return_fusion_weights: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalÄ±.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalÄ±.\")\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalÄ±.\")\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalÄ±.\")\n",
    "        if fusion_router_hidden < 1:\n",
    "            raise ValueError(\"fusion_router_hidden >= 1 olmalÄ±.\")\n",
    "\n",
    "        self.eps = float(eps)\n",
    "        self.fusion = fusion\n",
    "        self.return_fusion_weights = bool(return_fusion_weights)\n",
    "        self.gate_fn = _get_gate(gate)\n",
    "\n",
    "        hidden = max(int(min_hidden), int(channels) // int(reduction))\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=bias)\n",
    "        self.act = _get_act(act)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        if self.fusion == \"softmax\":\n",
    "            self.fusion_router = nn.Sequential(\n",
    "                nn.Conv2d(2 * channels, fusion_router_hidden, kernel_size=1, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(fusion_router_hidden, 2, kernel_size=1, bias=True),\n",
    "            )\n",
    "\n",
    "            last = self.fusion_router[-1] # Sequentialâ€™Ä±n son katmanÄ±nÄ± yakalama\n",
    "            nn.init.zeros_(last.weight)\n",
    "            nn.init.zeros_(last.bias)\n",
    "            # self.fusion_router bir nn.Sequential. [-1] demek â€œiÃ§indeki en son layerâ€. \n",
    "            # Burada en son layer Conv2d(..., out_channels=2) olan katman.\n",
    "            # Bunu alÄ±p last.weight ve last.biasâ€™Ä± sÄ±fÄ±rlÄ±yoruz â†’ baÅŸlangÄ±Ã§ta router tarafsÄ±z baÅŸlasÄ±n diye.\n",
    "            # Ä°lk adÄ±mda logits = 0 olur â†’ softmax([0,0]) = [0.5, 0.5] â†’ avg ve max eÅŸit aÄŸÄ±rlÄ±klÄ± baÅŸlar (stabil baÅŸlangÄ±Ã§).\n",
    "\n",
    "        else:\n",
    "            self.fusion_router = None\n",
    "\n",
    "        self.learnable_temperature = bool(learnable_temperature)\n",
    "        if self.learnable_temperature:\n",
    "            t0 = torch.tensor(float(temperature))\n",
    "            t_inv = _softplus_inverse(t0, eps=self.eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_s = self.avg_pool(x)\n",
    "        max_s = self.max_pool(x)\n",
    "\n",
    "        a = self.mlp(avg_s)\n",
    "        m = self.mlp(max_s)\n",
    "\n",
    "        fusion_w = None\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            s_cat = torch.cat([avg_s, max_s], dim=1)\n",
    "            logits = self.fusion_router(s_cat).flatten(1)\n",
    "            fusion_w = torch.softmax(logits, dim=1) \n",
    "            # fusion_w shape: (B, 2)\n",
    "            # Her Ã¶rnek iÃ§in iki sayÄ±: [w_avg, w_max].\n",
    "            # a ve m shape: (B, C, 1, 1)\n",
    "            # fusion_w[:, 0] shape (B,) olur. Bunu (B,1,1,1) yapÄ±yoruz \n",
    "            # ki PyTorch broadcasting ile (B,C,1,1) ile Ã§arpÄ±labilsin.\n",
    "\n",
    "            w0 = fusion_w[:, 0].view(-1, 1, 1, 1)\n",
    "            w1 = fusion_w[:, 1].view(-1, 1, 1, 1)\n",
    "            z = w0 * a + w1 * m # burada w0 her batch sample iÃ§in â€œavg tarafÄ±nÄ±n genel katsayÄ±sÄ±â€, w1 max tarafÄ±nÄ±n katsayÄ±sÄ±.\n",
    "\n",
    "        T = self.get_T().to(device=x.device, dtype=x.dtype)\n",
    "        ca = self.gate_fn(z / T)\n",
    "        y = x * ca\n",
    "\n",
    "        if self.return_fusion_weights and (fusion_w is not None):\n",
    "            return y, ca, fusion_w\n",
    "        return y, ca\n",
    "\n",
    "\n",
    "class HSwish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0\n",
    "\n",
    "\n",
    "def make_norm(norm: str, ch: int):\n",
    "    norm = norm.lower()\n",
    "    if norm == \"bn\":\n",
    "        return nn.BatchNorm2d(ch)\n",
    "    if norm == \"gn\":\n",
    "        g = min(32, ch)\n",
    "        while ch % g != 0 and g > 2:\n",
    "            g //= 2\n",
    "        if ch % g != 0:\n",
    "            g = 2 if (ch % 2 == 0) else 1\n",
    "        return nn.GroupNorm(g, ch)\n",
    "    if norm == \"none\":\n",
    "        return nn.Identity()\n",
    "    raise ValueError(\"norm 'none', 'bn', 'gn' dÄ±ÅŸÄ±nda olamaz.\")\n",
    "\n",
    "\n",
    "class CoordinateAttPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        reduction: int = 32,\n",
    "        min_mid_channels: int = 8,\n",
    "        act: str = \"hswish\",\n",
    "        init_alpha: float = 0.7,\n",
    "        learnable_alpha: bool = True,\n",
    "        beta: float = 0.35,\n",
    "        dilation: int = 2,\n",
    "        norm: str = \"gn\",\n",
    "        use_spatial_gate: bool = False,\n",
    "        spatial_gate_beta: float = 0.35,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if in_channels < 1:\n",
    "            raise ValueError(\"in_channels >= 1 olmalÄ±.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalÄ±.\")\n",
    "        if dilation < 1:\n",
    "            raise ValueError(\"dilation >= 1 olmalÄ±.\")\n",
    "\n",
    "        # SEBEBÄ° :: AÅŸÄ±rÄ± dar bottleneckâ€™i engelleme (capacity tabanÄ±)\n",
    "        mid_floor = max(8, min(32, int(in_channels) // 4))\n",
    "        # in_channels//reduction bazÄ± kanallarda Ã§ok kÃ¼Ã§Ã¼k olabilir (Ã¶rn 64//32=2). \n",
    "        # Bu, koordinat bilgisini taÅŸÄ±yan bottleneckâ€™i â€œnefessizâ€ bÄ±rakÄ±r.\n",
    "        # mid_floor ile minimum kapasiteyi garanti ediyoruz :en az 8 ama Ã§ok da ÅŸiÅŸmesin diye \n",
    "        # 32â€™ye kadar sÄ±nÄ±rayrÄ±ca kanal bÃ¼yÃ¼dÃ¼kÃ§e in_channels //4 ile Ã¶lÃ§eklenebilir.\n",
    "        mid = max(int(min_mid_channels), int(in_channels) // int(reduction))\n",
    "        mid = max(mid, int(mid_floor))\n",
    "        # CA/Coord gibi attention bloklarÄ±nda bottleneck Ã§ok kÃ¼Ã§Ã¼lÃ¼rse \n",
    "        # â€œÃ¶ÄŸrenilecekâ€ ÅŸey kalmÄ±yor â†’ maske ya dÃ¼z/iÅŸe yaramaz oluyor ya da saÃ§malÄ±yor. \n",
    "        # Bu kademeli seÃ§im onu engelliyor.\n",
    "\n",
    "        act_l = act.lower()\n",
    "        if act_l == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        elif act_l == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act_l == \"silu\":\n",
    "            self.act = nn.SiLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act 'hswish', 'relu', 'silu' olmalÄ±.\")\n",
    "\n",
    "        self.shared_bottleneck_proj = nn.Conv2d(in_channels, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_norm = make_norm(norm, mid)\n",
    "        self.shared_bottleneck_refine = nn.Conv2d(mid, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_refine_norm = make_norm(norm, mid)\n",
    "\n",
    "        self.h_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(3, 1), padding=(1, 0), groups=in_channels, bias=False\n",
    "        )\n",
    "        self.w_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(1, 3), padding=(0, 1), groups=in_channels, bias=False\n",
    "        )\n",
    "\n",
    "        d = int(dilation)\n",
    "        self.h_dilated_dw = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=(3, 1),\n",
    "            padding=(d, 0),\n",
    "            dilation=(d, 1),\n",
    "            groups=in_channels,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.w_dilated_dw = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=(1, 3),\n",
    "            padding=(0, d),\n",
    "            dilation=(1, d),\n",
    "            groups=in_channels,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.h_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        self.w_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self.h_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        self.w_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "\n",
    "        self.beta = float(beta)\n",
    "\n",
    "        # â€œalpha 0â€“1 aralÄ±ÄŸÄ±nda kalsÄ±nâ€ diye parametreyi logits uzayÄ±nda tutma\n",
    "        eps = 1e-6\n",
    "        a0 = float(init_alpha)\n",
    "        a0 = min(max(a0, eps), 1.0 - eps)\n",
    "        raw0 = torch.logit(torch.tensor(a0), eps=eps) \n",
    "        # torch.logit(a0) sigmoidâ€™in tersidir.\n",
    "        # BÃ¶ylece baÅŸlangÄ±Ã§ta sigmoid(alpha_raw) = init_alpha olur.\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha_h_raw = nn.Parameter(raw0.clone())\n",
    "            self.alpha_w_raw = nn.Parameter(raw0.clone())\n",
    "        else:\n",
    "            self.register_buffer(\"alpha_h_raw\", raw0.clone())\n",
    "            self.register_buffer(\"alpha_w_raw\", raw0.clone())\n",
    "\n",
    "        self.use_spatial_gate = bool(use_spatial_gate)\n",
    "        self.spatial_gate_beta = float(spatial_gate_beta)\n",
    "        if self.use_spatial_gate:\n",
    "            self.spatial_gate_dw = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False)\n",
    "            self.spatial_gate_pw = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self._last_ah = None\n",
    "        self._last_aw = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        _, _, H, W = x.shape\n",
    "\n",
    "        # %50-%50 karÄ±ÅŸÄ±m â†’ hem stabiliteyi hem â€œsalientâ€ sinyali taÅŸÄ±r.\n",
    "        h_profile = 0.5 * (x.mean(dim=3, keepdim=True) + x.amax(dim=3, keepdim=True))\n",
    "        # mean: genel enerji/ortalama aktivasyon â†’ stabil, gÃ¼rÃ¼ltÃ¼ye dayanÄ±klÄ±\n",
    "        # max: sivri/aykÄ±rÄ± gÃ¼Ã§lÃ¼ aktivasyon â†’ â€œÃ¶nemli obje iziâ€ gibi pikleri yakalar ama gÃ¼rÃ¼ltÃ¼ye hassas\n",
    "        # Bu profileâ€™lar sonra H ve W yÃ¶nÃ¼nde attention Ã¼retmek iÃ§in â€œÃ¶zet kanalâ€ oluyor.\n",
    "        w_profile = 0.5 * (x.mean(dim=2, keepdim=True) + x.amax(dim=2, keepdim=True))\n",
    "\n",
    "        h_ms = self.h_channel_mixer(self.h_local_dw(h_profile) + self.h_dilated_dw(h_profile))\n",
    "        w_ms = self.w_channel_mixer(self.w_local_dw(w_profile) + self.w_dilated_dw(w_profile))\n",
    "        w_ms = w_ms.permute(0, 1, 3, 2)\n",
    "\n",
    "        hw = torch.cat([h_ms, w_ms], dim=2)\n",
    "\n",
    "        mid = self.act(self.shared_bottleneck_norm(self.shared_bottleneck_proj(hw)))\n",
    "        mid = self.act(self.shared_bottleneck_refine_norm(self.shared_bottleneck_refine(mid)))\n",
    "\n",
    "        mid_h, mid_w = torch.split(mid, [H, W], dim=2)\n",
    "        mid_w = mid_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        attn_h = F.hardsigmoid(self.h_attention_head(mid_h), inplace=False)\n",
    "        attn_w = F.hardsigmoid(self.w_attention_head(mid_w), inplace=False)\n",
    "\n",
    "        self._last_ah = attn_h.detach()\n",
    "        self._last_aw = attn_w.detach()\n",
    "\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw).to(device=x.device, dtype=x.dtype)\n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        scale_h = (1.0 - alpha_h) + alpha_h * attn_h\n",
    "        scale_w = (1.0 - alpha_w) + alpha_w * attn_w\n",
    "\n",
    "        scale = scale_h * scale_w\n",
    "        scale = 1.0 + self.beta * (scale - 1.0)\n",
    "\n",
    "        out = x * scale\n",
    "\n",
    "        if self.use_spatial_gate:\n",
    "            sg = self.spatial_gate_pw(self.spatial_gate_dw(x))\n",
    "            sg = F.hardsigmoid(sg, inplace=False)\n",
    "            sg = 1.0 + self.spatial_gate_beta * (sg - 1.0)\n",
    "            out = out * sg\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def last_mask_stats(self):\n",
    "        if (self._last_ah is None) or (self._last_aw is None):\n",
    "            return None\n",
    "        ah = self._last_ah\n",
    "        aw = self._last_aw\n",
    "        return {\n",
    "            \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "            \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "        }\n",
    "\n",
    "\n",
    "class CBAMChannelPlusCoord(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        ca_reduction: int = 16,\n",
    "        ca_min_hidden: int = 4,\n",
    "        ca_fusion: str = \"softmax\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        ca_temperature: float = 0.9,\n",
    "        ca_act: str = \"relu\",\n",
    "        ca_fusion_router_hidden: int = 16,\n",
    "        learnable_temperature: bool = False,\n",
    "        coord_reduction: int = 32,\n",
    "        coord_min_mid: int = 8,\n",
    "        coord_act: str = \"hswish\",\n",
    "        coord_init_alpha: float = 0.7,\n",
    "        coord_learnable_alpha: bool = True,\n",
    "        coord_beta: float = 0.35,\n",
    "        coord_dilation: int = 2,\n",
    "        coord_norm: str = \"gn\",\n",
    "        coord_use_spatial_gate: bool = False,\n",
    "        coord_spatial_gate_beta: float = 0.35,\n",
    "        residual: bool = True,\n",
    "        alpha_init: float = 0.75,\n",
    "        learnable_alpha: bool = False,\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalÄ±.\")\n",
    "\n",
    "        self.return_maps = bool(return_maps)\n",
    "        self.residual = bool(residual)\n",
    "\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=ca_reduction,\n",
    "            min_hidden=ca_min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            eps=1e-6,\n",
    "            act=ca_act,\n",
    "            bias=True,\n",
    "            fusion_router_hidden=ca_fusion_router_hidden,\n",
    "            return_fusion_weights=self.return_maps,\n",
    "        )\n",
    "\n",
    "        self.coord = CoordinateAttPlus(\n",
    "            in_channels=channels,\n",
    "            reduction=coord_reduction,\n",
    "            min_mid_channels=coord_min_mid,\n",
    "            act=coord_act,\n",
    "            init_alpha=coord_init_alpha,\n",
    "            learnable_alpha=coord_learnable_alpha,\n",
    "            beta=coord_beta,\n",
    "            dilation=coord_dilation,\n",
    "            norm=coord_norm,\n",
    "            use_spatial_gate=coord_use_spatial_gate,\n",
    "            spatial_gate_beta=coord_spatial_gate_beta,\n",
    "        )\n",
    "\n",
    "        # : Residual karÄ±ÅŸÄ±m ÅŸiddetini kontrol eden tek skaler\n",
    "        # Bu alpha blok Ã§Ä±ktÄ±sÄ±nÄ±n ne kadar uygulanacaÄŸÄ±nÄ± belirler.\n",
    "        # alpha=0 â†’ tamamen input (blok kapalÄ±)\n",
    "        # alpha=1 â†’ tamamen blok Ã§Ä±ktÄ±sÄ±arasÄ± â†’ yumuÅŸak karÄ±ÅŸÄ±m\n",
    "        # AynÄ± â€œ0â€“1 aralÄ±ÄŸÄ± garantisiâ€ iÃ§in alpha_raw logit uzayÄ±nda tutuluyor.\n",
    "\n",
    "        if self.residual:\n",
    "            eps = 1e-6\n",
    "            a0 = float(alpha_init)\n",
    "            a0 = min(max(a0, eps), 1.0 - eps)\n",
    "            raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "            if learnable_alpha:\n",
    "                self.alpha_raw = nn.Parameter(raw0)\n",
    "            else:\n",
    "                self.register_buffer(\"alpha_raw\", raw0)\n",
    "\n",
    "    # Cihaz/dtype uyumlu sabit Ã¼retme ve gÃ¼venli fallback\n",
    "    def _alpha(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # residual=True iken alpha_raw yoksa (normalde olmamalÄ±) gÃ¼venli fallback: alpha=1.0\n",
    "        if not hasattr(self, \"alpha_raw\"):\n",
    "            # hasattr kontrolÃ¼ â†’ teorik olarak residual aÃ§Ä±kken alpha_raw olmalÄ±. \n",
    "            # Ama bir bug/yanlÄ±ÅŸ init olursa crash yerine alpha=1.0 ile devam etsin diye gÃ¼venlik.\n",
    "            return x.new_tensor(1.0) \n",
    "        # x.new_tensor(1.0) â†’ x ile aynÄ± device (CPU/GPU) ve aynÄ± dtype ile 1.0 Ã¼retir.\n",
    "        # Bu sayede â€œdevice mismatchâ€ hatasÄ± olmaz.\n",
    "        return torch.sigmoid(self.alpha_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "    # Pipeline ve debug Ã§Ä±ktÄ±larÄ±\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        # y = self.ca(x) :: :: Kanal bazÄ±nda (C dimension) Ã¶lÃ§ekleme yapar. Her kanalÄ±n â€œne kadar aÃ§Ä±k kalacaÄŸÄ±nÄ±â€ belirler.\n",
    "        # y = self.coord(y) :: :: Sonra koordinat tabanlÄ± H/W attention uygular: yatay ve dikey eksen boyunca iÃ§erik + konum iliÅŸkisini iÅŸler.\n",
    "        # out = x + alpha*(y - x) (residual aÃ§Ä±k ise) :: Bu blok â€œtam bastÄ±rmaâ€ yapmasÄ±n diye yumuÅŸatÄ±r. Alpha kÃ¼Ã§Ã¼kse blok etkisi yumuÅŸak olur.\n",
    "        if self.return_maps:\n",
    "        # return_maps=True iken ekstra debug verisi dÃ¶ndÃ¼rÃ¼r:\n",
    "        # ca_map: (B,C,1,1)\n",
    "        # fusion_w: (B,2) (softmax router aÄŸÄ±rlÄ±klarÄ±)\n",
    "        # coord_stats: son H/W mask istatistikleri\n",
    "        \n",
    "            y, ca_map, fusion_w = self.ca(x)\n",
    "            y = self.coord(y)\n",
    "\n",
    "            if self.residual:\n",
    "                out = x + self._alpha(x) * (y - x)\n",
    "            else:\n",
    "                out = y\n",
    "\n",
    "            coord_stats = self.coord.last_mask_stats()\n",
    "            return out, ca_map, fusion_w, coord_stats\n",
    "\n",
    "        y, _ = self.ca(x)\n",
    "        y = self.coord(y)\n",
    "\n",
    "        if self.residual:\n",
    "            out = x + self._alpha(x) * (y - x)\n",
    "        else:\n",
    "            out = y\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e521ec4",
   "metadata": {},
   "source": [
    "## HÄ±zlÄ± sanity kontrolÃ¼\n",
    "\n",
    "AÅŸaÄŸÄ±daki hÃ¼cre; shape kontrolÃ¼ ve temel istatistikleri basitÃ§e gÃ¶sterir:\n",
    "- Ã‡Ä±kÄ±ÅŸ shapeâ€™i giriÅŸle aynÄ± mÄ±?\n",
    "- CA mask ortalama/min/max\n",
    "- Coordinate H/W mask istatistikleri\n",
    "- Router fusion aÄŸÄ±rlÄ±klarÄ± (softmax router aÃ§Ä±kken)\n",
    "\n",
    "Not: Bu sadece hÄ±zlÄ± kontrol iÃ§indir; gerÃ§ek karar iÃ§in kÃ¼Ã§Ã¼k bir eÄŸitim/validasyon koÅŸusu ve ablation gerekir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb38459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 64, 56, 56])\n",
      "out: torch.Size([2, 64, 56, 56])\n",
      "ca_map: torch.Size([2, 64, 1, 1])\n",
      "fusion_w: torch.Size([2, 2])\n",
      "coord_stats: {'a_h': {'min': 0.0, 'mean': 0.4944933354854584, 'max': 0.9390336871147156, 'std': 0.053503669798374176}, 'a_w': {'min': 0.0, 'mean': 0.5012840628623962, 'max': 0.9842378497123718, 'std': 0.06626447290182114}}\n",
      "CA stats: 0.31093117594718933 0.5261633992195129 0.7153375744819641 0.10749468952417374\n",
      "fusion_w mean: [0.5, 0.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdgn5\\AppData\\Local\\Temp\\ipykernel_18368\\4011596079.py:26: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:837.)\n",
      "  print(\"CA stats:\", float(ca_map.min()), float(ca_map.mean()), float(ca_map.max()), float(ca_map.std()))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "m = CBAMChannelPlusCoord(\n",
    "    channels=64,\n",
    "    return_maps=True,\n",
    "    residual=True,\n",
    "    alpha_init=0.75,\n",
    "    learnable_alpha=False,\n",
    "    learnable_temperature=True,\n",
    "    ca_temperature=0.9,\n",
    "    coord_beta=0.35,\n",
    "    coord_dilation=2,\n",
    "    coord_norm=\"gn\",\n",
    ")\n",
    "\n",
    "out, ca_map, fusion_w, coord_stats = m(x)\n",
    "\n",
    "print(\"x:\", x.shape)\n",
    "print(\"out:\", out.shape)\n",
    "print(\"ca_map:\", ca_map.shape)\n",
    "print(\"fusion_w:\", fusion_w.shape)\n",
    "print(\"coord_stats:\", coord_stats)\n",
    "\n",
    "print(\"CA stats:\", float(ca_map.min()), float(ca_map.mean()), float(ca_map.max()), float(ca_map.std()))\n",
    "print(\"fusion_w mean:\", fusion_w.mean(dim=0).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acc278",
   "metadata": {},
   "source": [
    "## EÄŸitim sÄ±rasÄ±nda gÃ¶zlenmesi gereken 4 sinyal\n",
    "\n",
    "1) **Ã‡Ä±kÄ±ÅŸ/GiriÅŸ std oranÄ±**\n",
    "- `std_ratio = out.std() / x.std()` Ã§ok dÃ¼ÅŸerse (Ã¶r. 0.2â€“0.3) â†’ attention muhtemelen fazla bastÄ±rÄ±yor.\n",
    "- hedef band genelde kabaca 0.6â€“1.1 arasÄ± (modele gÃ¶re deÄŸiÅŸir).\n",
    "\n",
    "2) **CA mask daÄŸÄ±lÄ±mÄ±**\n",
    "- mean ~ 0.5 civarÄ± normal.\n",
    "- min Ã§ok dÃ¼ÅŸÃ¼k (0.05 gibi) ve max Ã§ok dÃ¼ÅŸÃ¼kse â†’ Ã§ok bastÄ±rma.\n",
    "\n",
    "3) **Fusion weights**\n",
    "- Router bir tarafa â€œkilitleniyorsaâ€ (Ã¶rn sÃ¼rekli [0.99, 0.01]) â†’ ya veri buna itiyor ya da T/MLP Ã§ok agresif.\n",
    "- Ä°lk epochâ€™larda dengeli olmasÄ± normal; sonra hafif kaymasÄ± normal.\n",
    "\n",
    "4) **Coordinate H/W mask stats**\n",
    "- `a_h` ve `a_w` mean/std Ã§ok kÃ¼Ã§Ã¼k kalÄ±rsa â†’ coordinate etkisi zayÄ±f (mid dÃ¼ÅŸÃ¼k, beta Ã§ok dÃ¼ÅŸÃ¼k, alpha_h/w dÃ¼ÅŸÃ¼k).\n",
    "- Ã‡ok keskinleÅŸirse â†’ beta veya dilation ayarÄ± gÃ¶zden geÃ§irilmeli.\n",
    "\n",
    "Bu sinyaller, attentionâ€™Ä± â€œgÃ¼Ã§lÃ¼ mÃ¼, doÄŸru mu, gereksiz mi?â€ diye ayÄ±klamak iÃ§in yeterli olur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60965572",
   "metadata": {},
   "source": [
    "## Ablation planÄ± (Ã¶nerilen)\n",
    "\n",
    "AynÄ± backbone/neck Ã¼zerinde sÄ±rayla:\n",
    "\n",
    "- **Baseline:** attention yok\n",
    "- **CA only:** sadece `ChannelAttentionFusionT`\n",
    "- **Coord only:** sadece `CoordinateAttPlus`\n",
    "- **YÃ¶ntem 1:** `CA -> Coord -> residual` (bu defterin konusu)\n",
    "\n",
    "Her adÄ±mda:\n",
    "- mAP / accuracy (hedef metriÄŸin)\n",
    "- latency / FPS\n",
    "- param ve MAC\n",
    "- std_ratio, mask stats\n",
    "\n",
    "EÄŸer YÃ¶ntem 1 baselineâ€™a gÃ¶re net kazanÄ±yorsa, daha ileri karmaÅŸÄ±klÄ±k (YÃ¶ntem 2: paralel SA gibi) ancak o zaman anlamlÄ± olur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a81370",
   "metadata": {},
   "source": [
    "# ğŸ” Attention BloÄŸu Ã‡Ä±ktÄ±larÄ± â€“ Ã–zet Tablo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501b6c4",
   "metadata": {},
   "source": [
    "| Ã‡Ä±ktÄ±               | Shape                | Ne Anlama Geliyor                  | Senin Ã‡Ä±ktÄ±nda Ne SÃ¶ylÃ¼yor                                    |\n",
    "| ------------------- | -------------------- | ---------------------------------- | ------------------------------------------------------------- |\n",
    "| **x**               | `[2, 64, 56, 56]`    | GiriÅŸ feature map                  | Backboneâ€™dan gelen ham Ã¶zellikler                             |\n",
    "| **out**             | `[2, 64, 56, 56]`    | Attention + residual sonrasÄ± Ã§Ä±ktÄ± | Shape korunmuÅŸ â†’ bilgi eklenmedi, sadece yeniden aÄŸÄ±rlÄ±klandÄ± |\n",
    "| **ca_map**          | `[2, 64, 1, 1]`      | Kanal attention maskesi            | Her kanal iÃ§in tek Ã¶nem katsayÄ±sÄ±                             |\n",
    "| **fusion_w**        | `[2, 2]`             | AvgPoolâ€“MaxPool aÄŸÄ±rlÄ±klarÄ±        | Avg ve Max eÅŸit (0.5â€“0.5) â†’ tarafsÄ±z baÅŸlangÄ±Ã§                |\n",
    "| **coord_stats.a_h** | scalar istatistikler | YÃ¼kseklik (H) yÃ¶nlÃ¼ attention      | H ekseninde dengeli ama seÃ§ici odak                           |\n",
    "| **coord_stats.a_w** | scalar istatistikler | GeniÅŸlik (W) yÃ¶nlÃ¼ attention       | W ekseninde benzer ÅŸekilde stabil                             |\n",
    "| **CA stats**        | min/mean/max/std     | Kanal attention daÄŸÄ±lÄ±mÄ±           | Agresif deÄŸil, bastÄ±rma yok                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79ba14",
   "metadata": {},
   "source": [
    "# ğŸ“Œ DetaylÄ± Ama KÄ±sa Yorumlar\n",
    "\n",
    "| Ã–lÃ§Ã¼m                   | DeÄŸer       | Yorumu                            |\n",
    "| ----------------------- | ----------- | --------------------------------- |\n",
    "| `ca_map.mean â‰ˆ 0.53`    | Orta seviye | Kanallar hafif gÃ¼Ã§lendirilmiÅŸ     |\n",
    "| `ca_map.std â‰ˆ 0.11`     | DÃ¼ÅŸÃ¼k       | Kanal attention stabil            |\n",
    "| `fusion_w = [0.5, 0.5]` | Dengeli     | Avg / Max arasÄ±nda taraf tutmuyor |\n",
    "| `a_h.mean â‰ˆ 0.49`       | Dengeli     | H ekseninde yumuÅŸak attention     |\n",
    "| `a_w.mean â‰ˆ 0.50`       | Dengeli     | W ekseninde yumuÅŸak attention     |\n",
    "| `a_h.max â‰ˆ 0.94`        | YÃ¼ksek      | BazÄ± bÃ¶lgelerde net odak var      |\n",
    "| `a_w.max â‰ˆ 0.98`        | YÃ¼ksek      | Uzamsal seÃ§icilik mevcut          |\n",
    "| `std (H/W) â‰ˆ 0.05â€“0.06` | DÃ¼ÅŸÃ¼k       | GÃ¼rÃ¼ltÃ¼sÃ¼z spatial attention      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d513d07",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ Channel Attention (CA) tarafÄ±\n",
    "## âŒ KÃ¶tÃ¼ durumlar ve anlamlarÄ±\n",
    "| GÃ¶zlenen deÄŸer           | Bu ne demek                 | Neden kÃ¶tÃ¼             |\n",
    "| ------------------------ | --------------------------- | ---------------------- |\n",
    "| `ca_map.mean < 0.3`      | Kanallar aÅŸÄ±rÄ± bastÄ±rÄ±lÄ±yor | Bilgi kaybÄ±, mAP dÃ¼ÅŸer |\n",
    "| `ca_map.mean > 0.8`      | Her kanal Ã¶nemli sanÄ±lÄ±yor  | Attention etkisizleÅŸir |\n",
    "| `ca_map.std > 0.25`      | Kanal seÃ§imi Ã§ok sert       | EÄŸitim kararsÄ±zlaÅŸÄ±r   |\n",
    "| `ca_map.min â‰ˆ 0` Ã§ok sÄ±k | Kanallar Ã¶lÃ¼yor             | Gradient akÄ±ÅŸÄ± bozulur |\n",
    "\n",
    "\n",
    "### ğŸ› ï¸ NasÄ±l dÃ¼zeltilir?\n",
    "\n",
    "Ã‡ok kÃ¼Ã§Ã¼kse â†’ temperature â†‘ veya alpha â†“\n",
    "\n",
    "Ã‡ok bÃ¼yÃ¼kse â†’ temperature â†“ veya gateâ€™i hardsigmoid â†’ sigmoid\n",
    "\n",
    "std Ã§ok yÃ¼ksekse â†’ residual alpha_init dÃ¼ÅŸÃ¼\n",
    "\n",
    "# 2ï¸âƒ£ Fusion Weights (fusion_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494e0a0",
   "metadata": {},
   "source": [
    "## âŒ KÃ¶tÃ¼ durumlar \n",
    "| Durum                              | AnlamÄ±                   | Sorun                  |\n",
    "| ---------------------------------- | ------------------------ | ---------------------- |\n",
    "| `[1.0, 0.0]` sabit                 | Sadece avg kullanÄ±lÄ±yor  | Max bilgisi boÅŸa       |\n",
    "| `[0.0, 1.0]` sabit                 | Sadece max kullanÄ±lÄ±yor  | Global baÄŸlam kaybolur |\n",
    "| Erken aÅŸamada uÃ§ deÄŸerlere gitmesi | Router erken kilitlenmiÅŸ | Genelleme bozulur      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f21ee3",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ NasÄ±l dÃ¼zeltilir?\n",
    "\n",
    "Router son katmanÄ±nÄ± zero-init\n",
    "\n",
    "Router hidden Ã§ok bÃ¼yÃ¼kse kÃ¼Ã§Ã¼lt\n",
    "\n",
    "Early trainingâ€™de LR dÃ¼ÅŸÃ¼r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42318c13",
   "metadata": {},
   "source": [
    "# 3ï¸âƒ£ Coordinate Attention â€“ H / W istatistikleri \n",
    "## âŒ KÃ¶tÃ¼ durumlar\n",
    "\n",
    "| GÃ¶zlem              | AnlamÄ±                     | SonuÃ§                      |\n",
    "| ------------------- | -------------------------- | -------------------------- |\n",
    "| `mean < 0.3`        | Uzamsal bastÄ±rma Ã§ok fazla | KÃ¼Ã§Ã¼k objeler kaybolur     |\n",
    "| `mean > 0.8`        | Her yer Ã¶nemli             | Spatial attention anlamsÄ±z |\n",
    "| `std > 0.15`        | GÃ¼rÃ¼ltÃ¼lÃ¼ spatial mask     | False positive artar       |\n",
    "| `min = 0` her yerde | Sert spatial gate          | Detection head zarar gÃ¶rÃ¼r |\n",
    "\n",
    "### ğŸ› ï¸ NasÄ±l dÃ¼zeltilir?\n",
    "\n",
    "Ã‡ok agresifse â†’ beta â†“\n",
    "\n",
    "Etkisizse â†’ beta â†‘\n",
    "\n",
    "GÃ¼rÃ¼ltÃ¼lÃ¼yse â†’ dilated conv sayÄ±sÄ±nÄ± azalt\n",
    "\n",
    "Small object kayboluyorsa â†’ use_spatial_gate=False\n",
    "\n",
    "# 4ï¸âƒ£ Residual Alpha (alpha_raw)\n",
    "## âŒ KÃ¶tÃ¼ durumlar\n",
    "| Durum                             | AnlamÄ±                          |\n",
    "| --------------------------------- | ------------------------------- |\n",
    "| `alpha â‰ˆ 1`                       | Attention her ÅŸeyi eziyor       |\n",
    "| `alpha â‰ˆ 0`                       | Attention boÅŸa Ã§alÄ±ÅŸÄ±yor        |\n",
    "| Learnable alpha hÄ±zlÄ± 1â€™e gitmesi | Model attentionâ€™a aÅŸÄ±rÄ± baÄŸÄ±mlÄ± |\n",
    "\n",
    "**KÃ¶tÃ¼ attention; Ã§ok baskÄ±layan, erken kilitlenen ve gÃ¼rÃ¼ltÃ¼ Ã¼reten attentionâ€™dÄ±rBizim Ã§Ä±ktÄ±larÄ±mÄ±z ise yumuÅŸak, dengeli ve Ã¶ÄŸrenmeye aÃ§Ä±k bir attention gÃ¶steriyor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
