{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02f5740",
   "metadata": {},
   "source": [
    "----\n",
    "-----\n",
    "# CBAM + CoordinateAttPlus (Adım-2) — GERÇEK ÖZET (YOLO Stabilite Guardrail’leri)\n",
    "\n",
    "## Bu defter ne anlatıyor?\n",
    "Bu notebook, Adım-2 ile eklenen **stabilite odaklı guardrail’leri** ve bunların\n",
    "**YOLO / detection** gibi hassas mimarilerde neden gerekli olduğunu açıklar.\n",
    "\n",
    "- **Detay seviyesi yüksektir**: Kod kararları “neden-sonuç” ilişkisiyle anlatılır.\n",
    "- Daha hızlı/yüzeysel akış için:  \n",
    "  `Attention Mekanizmaları\\CBAM + Cordinat Attention Block (5)\\Yöntem -2\\CBAM+CA_Yüzeysel.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "## Ne eklendi? (Adım-2’de gelen ana eklemeler)\n",
    "1. **Learnable Temperature (T) + Clamp**: T uçmasın, CA sertleşmesin/boşa düşmesin; AMP uyumu.\n",
    "2. **Coordinate head init**: Küçük std + bias=0 ile erken eğitimde agresif maskeleri engelleme.\n",
    "3. **Alpha ile 1’e karıştırma**: `scale = 1 + α(attn-1)` → sert gate yerine kademeli etki.\n",
    "4. **Global Beta yumuşatma**: `scale = 1 + β(scale-1)` → tüm bloğun agresifliğini tek knob ile kontrol.\n",
    "5. **Scale Clamp**: `scale ∈ [scale_min, scale_max]` → sınırsız bastırma/güçlendirme yok.\n",
    "6. **Residual + Monitor (EMA) + Rescue**: Over-suppression ölç → `alpha_eff` otomatik düşür.\n",
    "\n",
    "---\n",
    "\n",
    "## Hangi problemleri çözüyor?\n",
    "- Erken eğitimde attention’ın feature’ları “öldürmesi” (over-suppression)\n",
    "- Küçük batch’te dengesiz davranış\n",
    "- AMP/FP16’de device/dtype karışıklığı\n",
    "- Maskelerin 0/1’e yapışması (saturasyon)\n",
    "- Scale’in kontrol dışına çıkması\n",
    "\n",
    "---\n",
    "\n",
    "## YOLO için neden güvenli?\n",
    "- Giriş/çıkış shape aynı (drop-in block)\n",
    "- AMP/FP16 uyumu (device/dtype taşımaları)\n",
    "- Guardrail’ler sayesinde enerji kontrol altında\n",
    "- Residual bağlantı ile bilgi kaybı toparlanıyor\n",
    "- Training’de monitor/rescue ile otomatik stabilizasyon var\n",
    "\n",
    "---\n",
    "\n",
    "## Tasarım prensipleri (kısa)\n",
    "1. **Attention sınırsız olamaz** → clamp şart.\n",
    "2. **Agresiflik kontrol knob’ları olmalı** → alpha/beta.\n",
    "3. **Residual enerji dengeleyici** olmalı.\n",
    "4. **Monitor sinyali öğrenmeden ayrı** tutulmalı (detach/no_grad).\n",
    "5. **Detection’da stabilite > ham güç**.\n",
    " -----\n",
    " ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae39ffb",
   "metadata": {},
   "source": [
    "## Adım 1 in üstüne eklenen adım 2 kodlarını aşağıya bırakıyorum.Önce eklenenleri detaylı olarak inceleyelim.Sonrasında genel kod yapısına bakalım.Yüzeysel olarak kod yapısının ve işleyişin anlatıldığı notebook ise:\n",
    "> ### Attention Mekanizmaları\\CBAM + Cordinat Attenti,on Block (5)\\Yöntem -2\\CBAM+CA_Yüzeysel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a6840",
   "metadata": {},
   "source": [
    "-----\n",
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f45505",
   "metadata": {},
   "source": [
    "## Kod-1) get_T Fonksiyonu \n",
    "```python\n",
    "   def get_T(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            T = F.softplus(self.t_raw) + self.eps\n",
    "        else:\n",
    "            T = self.T\n",
    "        T = T.to(device=x.device, dtype=x.dtype)\n",
    "        return T.clamp(self.t_min, self.t_max)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88a25e",
   "metadata": {},
   "source": [
    "Bu parça “learnable temperature” (T) işini YOLO’da güvenli hale getirmek için revize edildi. Önceki sürümde T ya sabit buffer’dı ya da softplus(t_raw) ile öğreniliyordu ama iki risk vardı:\n",
    "\n",
    "* T uçabiliyor (çok küçülürse CA aşırı sertleşir, çok büyürse CA etkisizleşir)\n",
    "\n",
    "* device / dtype uyumsuzluğu (özellikle AMP/FP16 ve GPU’da: T CPU-float kalırsa broadcast sırasında saçma şeyler olur ya da performans bozulur)\n",
    "\n",
    "Bu revizyon bunların ikisini aynı anda çözüyor.\n",
    "\n",
    "> ### T = T.to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "* T’yi x ile aynı cihaz (CPU/GPU) ve aynı veri tipi (fp32/fp16/bf16) yapıyor.\n",
    "\n",
    "* YOLO + AMP’de bu kritik: fp16 bir tensörle fp32/CPU scalar karışırsa gereksiz cast, bazen uyarı, bazen hız düşüşü olur.\n",
    "\n",
    "\n",
    "> ### return T.clamp(self.t_min, self.t_max)\n",
    "\n",
    "Asıl revizyonun kalbi bu: T’yi güvenli aralığa kilitliyoruz.\n",
    "\n",
    "* Kuralımız: T ∈ [0.5, 3.0] (veya ne verdiysek)\n",
    "\n",
    "#### Bu clamp neyi engeller?\n",
    "\n",
    "* T çok küçük → z/T patlar → sigmoid/hardsigmoid aşırı saturate → CA “hard gate” gibi olur → over-suppression riski artar.\n",
    "\n",
    "- T çok büyük → z/T küçülür → sigmoid 0.5 civarında kalır → CA etkisi “boşa” düşer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00973ac3",
   "metadata": {},
   "source": [
    "## Neden revize edildi?\n",
    "\n",
    "* Stabilite: T’nin öğrenme sırasında uçmasını engeller.\n",
    "\n",
    "* YOLO uyumluluğu: AMP + GPU + FP16’de dtype/device uyumsuzluğu riskini kapatır.\n",
    "\n",
    "* Davranış kontrolü: CA’nın “sertlik” knob’u olan T’yi mantıklı aralıkta tutar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8789e3",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Kod-2) Attention bias ayarlama\n",
    "```python\n",
    "        nn.init.normal_(self.h_attention_head.weight, mean=0.0, std=float(head_init_std))\n",
    "        nn.init.normal_(self.w_attention_head.weight, mean=0.0, std=float(head_init_std))\n",
    "        if self.h_attention_head.bias is not None:\n",
    "            nn.init.zeros_(self.h_attention_head.bias)\n",
    "        if self.w_attention_head.bias is not None:\n",
    "            nn.init.zeros_(self.w_attention_head.bias)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe6428",
   "metadata": {},
   "source": [
    "**Bu kısım CoordinateAttPlus’un iki “attention head” conv’unu (h_attention_head, w_attention_head) agresif başlamasın diye özel initialize ediyor. YOLO tarafında en çok patlayan şeylerden biri: attention bloğu ilk iterasyonlarda “çok güçlü” başlıyor ve feature’ları gereksiz bastırıyor. Burayı revize etmenin hedefi o riski düşürmek.**\n",
    "\n",
    "### 1) Ağırlıkları küçük std ile normal dağılımdan başlatma\n",
    "\n",
    "* mean=0.0 → ağırlıkların ortalaması 0.\n",
    "\n",
    "* std=head_init_std (örn 0.01) → ağırlıklar çok küçük değerlerle başlar.\n",
    "\n",
    "#### Etkisi ne?\n",
    "\n",
    "* Head conv çıktısı (self.h_attention_head(mid_h) ve self.w_attention_head(mid_w)) başlangıçta küçük olur.\n",
    "\n",
    "* Sonra hardsigmoid’e girince sonuç genelde 0.5 civarında takılır (çok yukarı/çok aşağı gitmez).\n",
    "\n",
    "* Yani attention maskesi “başta deli gibi aç/kapat” yapmaz; daha yumuşak başlar.\n",
    "\n",
    "\n",
    "### 2) Bias’ları sıfırlama\n",
    "\n",
    "* Bias varsa 0 yapıyoruz.\n",
    "\n",
    "#### Etkisi ne?\n",
    "\n",
    "Bias pozitif/negatif başlarsa maskeyi daha en baştan kaydırır:\n",
    "\n",
    "* pozitif bias → hardsigmoid’i yukarı iter → daha çok “aç”\n",
    "\n",
    "- negatif bias → aşağı iter → daha çok “kapat”\n",
    "\n",
    "- Bias=0 → başlangıç “tarafsız”.\n",
    "\n",
    "#### Neden bu revizyon önemli?\n",
    "\n",
    "Bizim Coordinate tarafında scale şu şekilde büyüyor:\n",
    "\n",
    "* head → hardsigmoid → attn_h/attn_w\n",
    "\n",
    "* sonra scale = ... ve en sonda x ile çarpılıyor.\n",
    "\n",
    "**Bu zincirde head çok agresif başlarsa, scale da agresif olur ve feature bastırma / patlama riski büyür. Küçük std + bias=0, özellikle detection’da (batch küçükken) daha stabil başlatır.**\n",
    "\n",
    "----\n",
    "## Kod-3) Scale & Beta Ayarlaması\n",
    "```python\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw).to(device=x.device, dtype=x.dtype)\n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        scale_h = (1.0 - alpha_h) + alpha_h * attn_h\n",
    "        scale_w = (1.0 - alpha_w) + alpha_w * attn_w\n",
    "\n",
    "        scale = scale_h * scale_w\n",
    "        scale = 1.0 + self.beta * (scale - 1.0)\n",
    "        scale = scale.clamp(self.scale_min, self.scale_max)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48563466",
   "metadata": {},
   "source": [
    "**Burası CoordinateAttPlus’un “attention’ı ne kadar devreye sokacağım” kısmı. Yani attn_h ve attn_w maskelerini direkt yapıştırmak yerine, önce karıştırıp yumuşatıyor, sonra da global beta ile etkisini azaltıyor, en sonda da scale clamp ile güvenli aralıkta tutuyor.**\n",
    "\n",
    "* self.alpha_h_raw ve self.alpha_w_raw aslında “ham” parametre (logit gibi düşünelim).\n",
    "\n",
    "* sigmoid(...) ile bunları 0 ile 1 arasına sıkıştırıyoruz: alpha_h ∈ (0,1), alpha_w ∈ (0,1).\n",
    "\n",
    "* .to(device=..., dtype=...) de x ile aynı cihazda ve aynı türde olsun diye. Yoksa GPU/CPU veya fp16/fp32 karışır, hata veya gereksiz cast olur.\n",
    "\n",
    "####  Mantık: alpha bir “güç düğmesi”.\n",
    "\n",
    "alpha = 0 → attention’ı kapat (scale ≈ 1)\n",
    "\n",
    "alpha = 1 → attention’ı full uygula (scale ≈ attn)\n",
    "\n",
    "### “Attention ile 1’i karıştırma” (asıl kritik fikir)\n",
    "```python\n",
    "scale_h = (1.0 - alpha_h) + alpha_h * attn_h\n",
    "scale_w = (1.0 - alpha_w) + alpha_w * attn_w\n",
    "```\n",
    "Bu formül şu demek:\n",
    "\n",
    "- scale_h = 1 ile attn_h arasında bir karışım.\n",
    "\n",
    "- scale_w = 1 ile attn_w arasında bir karışım.\n",
    "\n",
    "Daha açık yazarsak:\n",
    "\n",
    "- scale_h = 1 + alpha_h * (attn_h - 1)\n",
    "\n",
    "* scale_w = 1 + alpha_w * (attn_w - 1)\n",
    "\n",
    "Yani attention “1’den saparsa” (1 = hiçbir şey yapma), sapma miktarını alpha ile ayarlıyoruz.\n",
    "\n",
    "Örnek:\n",
    "\n",
    "* attn_h = 0.2 (çok bastırıyor)\n",
    "\n",
    "* alpha_h = 0.2 ise:\n",
    "\n",
    "* scale_h = 1 + 0.2*(0.2 - 1) = 1 - 0.16 = 0.84\n",
    "\n",
    "**yani “0.2’ye vurmak” yerine “0.84’e vuruyoruz\" → bastırmayı yumuşatıyor.**\n",
    "\n",
    "### beta ile “global yumuşatma”\n",
    "```python\n",
    "scale = 1.0 + self.beta * (scale - 1.0)\n",
    "```\n",
    "\n",
    "\n",
    "Bu da aynı CA’daki beta mantığı:\n",
    "\n",
    "* beta = 0 → her şeyi kapat: scale = 1\n",
    "\n",
    "* beta = 1 → scale aynen kalsın\n",
    "\n",
    "* 0 < beta < 1 → scale’i 1’e doğru çek (agresifliği azalt)\n",
    "\n",
    "Yani “alpha” kanal bazlı/gate bazlı güç düğmesi gibi, “beta” da tüm scale’in genel agresiflik çarpanı gibi.\n",
    "\n",
    "### Clamp: güvenlik bariyeri\n",
    "```python\n",
    "scale = scale.clamp(self.scale_min, self.scale_max)\n",
    "```\n",
    "\n",
    "*Bu çok kritik.*\n",
    "\n",
    "* attention maskeleri bazen uç değerlere gidebilir (özellikle erken eğitimde).\n",
    "\n",
    "* clamp ile scale’i güvenli aralığa kilitliyoruz.\n",
    "\n",
    "Mesela scale_min=0.6, scale_max=1.6 ise:\n",
    "\n",
    "* en fazla %40 bastırma\n",
    "\n",
    "* en fazla %60 güçlendirme\n",
    "\n",
    "**YOLO/detection gibi hassas yapılarda bu tip “guardrail” stabiliteyi ciddi artırır.**\n",
    "\n",
    "### Özet (tek cümle)\n",
    "\n",
    "**Bu blok: attn_h/attn_w maskelerini direkt uygulamak yerine önce 1 ile karıştırıp (alpha), sonra genel etkiyi yumuşatıp (beta), en sonda da güvenli aralığa sabitleyerek (clamp) YOLO’da patlamayı ve aşırı bastırmayı önlüyor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97c7ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Kod-4) Residual Karışım Şiddetini Kontrol Etmek\n",
    "```python\n",
    "        if self.residual:\n",
    "            eps = 1e-6\n",
    "            a0 = float(alpha_init)\n",
    "            a0 = min(max(a0, eps), 1.0 - eps)\n",
    "            raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "            if learnable_alpha:\n",
    "                self.alpha_raw = nn.Parameter(raw0)\n",
    "            else:\n",
    "                self.register_buffer(\"alpha_raw\", raw0)\n",
    "\n",
    "        self.register_buffer(\"r_ema\", torch.tensor(1.0))\n",
    "```\n",
    "\n",
    "Bu blok iki şey için revize edildi:\n",
    "\n",
    "* Residual karışımının şiddetini (alpha) stabil ve kontrol edilebilir başlatmak\n",
    "\n",
    "* Over-suppression (fazla bastırma) takibi için EMA state’i (r_ema) tutmak\n",
    "\n",
    "\n",
    "#### Residual Alpha’nın Başlatılması\n",
    "\n",
    "Residual açıkken çıkış şu formdadır:\n",
    " * out = x + alpha * (y - x)\n",
    "\n",
    "\n",
    "Buradaki `alpha`, attention bloğunun çıktıya ne kadar karışacağını belirleyen temel kontrol düğmesidir.\n",
    "\n",
    "- `alpha_init`: Bloğun başlangıçtaki etkisini belirler.\n",
    "- `eps`: Sayısal kararlılık için kullanılır. `logit(0)` veya `logit(1)` gibi sonsuz değerlere düşmemek için alpha değeri `(eps, 1-eps)` aralığına sıkıştırılır.\n",
    "- `torch.logit(a0)`: Alpha değeri “raw” (logit) uzayına taşınır.  \n",
    "  Forward sırasında `sigmoid(alpha_raw)` uygulanarak alpha’nın **her zaman (0,1)** aralığında kalması garanti edilir.\n",
    "\n",
    "Bu sayede:\n",
    "- Alpha ne tamamen kapanır ne de tamamen kilitlenir.\n",
    "- Eğitim sırasında stabil ve yumuşak bir şekilde ayarlanabilir.\n",
    "\n",
    "`learnable_alpha=True` ise:\n",
    "- `alpha_raw` öğrenilebilir bir parametre olur.\n",
    "- Model, attention bloğunu gerektiğinde açıp kısmayı öğrenebilir.\n",
    "\n",
    "`learnable_alpha=False` ise:\n",
    "- Alpha sabit kalır.\n",
    "- Ancak `register_buffer` ile modele bağlı bir state olarak saklanır ve cihazla birlikte taşınır.\n",
    "\n",
    "\n",
    "#### r_ema (Exponential Moving Average) State\n",
    "```python\n",
    "self.register_buffer(\"r_ema\", torch.tensor(1.0))\n",
    "```\n",
    "\n",
    "\n",
    "Bu satır, **over-suppression tespiti ve rescue mekanizması** için eklenmiştir.\n",
    "\n",
    "- `r_ema`, çıktı enerjisinin (std oranı) üssel hareketli ortalamasını tutar.\n",
    "- Tipik olarak:\n",
    "  - `r_out = out_std / x_std`\n",
    "  - `r_ema = ema * r_ema + (1 - ema) * r_out`\n",
    "\n",
    "Neden buffer?\n",
    "- Gradient gerektirmez (parametre değil).\n",
    "- Eğitim sırasında güncellenir.\n",
    "- Checkpoint alındığında modelin durumu ile birlikte saklanır.\n",
    "\n",
    "Neden başlangıç değeri `1.0`?\n",
    "- “Başlangıçta bastırma yok” varsayımıyla başlar.\n",
    "- Aksi halde eğitim başında gereksiz ve agresif rescue tetiklenebilirdi.\n",
    "\n",
    "\n",
    "#### Genel Etki\n",
    "\n",
    "Bu iki yapı birlikte şunu sağlar:\n",
    "\n",
    "- Residual attention **kontrollü, stabil ve öğrenilebilir** olur.\n",
    "- Aşırı bastırma durumları **ölçülür ve gerektiğinde otomatik yumuşatılır**.\n",
    "- YOLO gibi hassas detection mimarilerinde eğitim stabilitesi ciddi biçimde artar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265379a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20e59a",
   "metadata": {},
   "source": [
    "## Kod-5)  `_alpha()` fonksiyonu ne yapıyor?\n",
    "\n",
    "Bu fonksiyonun tek işi: **residual karışım katsayısını (`alpha`) güvenli ve doğru tipte üretmek.**  \n",
    "**Kısaca: Bu fonksiyon, residual karışımda attention’ın etkisini 0–1 arasında güvenli bir alpha ile ayarlamak için var; residual yoksa da patlamasın diye varsayılan 1.0 döndürür.**\n",
    "Residual karışım dediğimiz şey şu formül:\n",
    "\n",
    "- `out = x + alpha * (y - x)`\n",
    "\n",
    "Burada `alpha` küçülürse blok “az etkiler”, büyürse blok “daha çok etkiler”.\n",
    "\n",
    "\n",
    "### Kodun adım adım açıklaması\n",
    "\n",
    "```python\n",
    "def _alpha(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    if (not self.residual) or (not hasattr(self, \"alpha_raw\")):\n",
    "        return x.new_tensor(1.0)\n",
    "    return torch.sigmoid(self.alpha_raw).to(device=x.device, dtype=x.dtype)\n",
    "```\n",
    "\n",
    "### 1) if (not self.residual) ...\n",
    "\n",
    "* Eğer residual=False ise zaten karışım yok (blok çıktısı direkt kullanılacak).\n",
    "\n",
    "* Bu durumda alpha hesaplamanın anlamı yok; fonksiyon 1.0 döndürerek “tam uygula” gibi davranıyor.\n",
    "\n",
    "**Not: Bizim forward’da residual=False iken genelde out=y dönüyor; burada 1.0 dönmesi “zararsız güvenli default”.**\n",
    "\n",
    "### 2) or (not hasattr(self, \"alpha_raw\"))\n",
    "\n",
    "- Normalde residual=True ise __init__ içinde alpha_raw oluşturulmuş olmalı.\n",
    "\n",
    "- Ama bir bug/yanlış config olursa alpha_raw yoksa patlamasın diye fallback koyduk.\n",
    "\n",
    "- Bu durumda da 1.0 döndürüp “karışımı tamamen aç” diyoruz.\n",
    "\n",
    "### 3) return x.new_tensor(1.0)\n",
    "\n",
    "* x ile aynı device/dtype’ta yeni bir tensor üretir.\n",
    "\n",
    ">Örn x CUDA fp16 ise alpha da CUDA fp16 olur.\n",
    "\n",
    "* Böylece out = x + alpha*(y-x) yaparken device/dtype mismatch yemeyiz.\n",
    "\n",
    "### 4) torch.sigmoid(self.alpha_raw)\n",
    "\n",
    "* alpha_raw ham parametre/buffer (logit uzayı).\n",
    "\n",
    "* sigmoid ile bunu [0, 1] aralığına map ediyoruz.\n",
    "\n",
    "**Bu sayede alpha her zaman mantıklı aralıkta kalıyor; negatif ya da 5 gibi saçma değer çıkmıyor.**\n",
    "\n",
    "### 5) .to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "* alpha_raw CPU’da buffer olabilir veya fp32 olabilir.\n",
    "\n",
    "* Burada alphayı x’in çalıştığı yere ve tipe taşıyoruz.\n",
    "\n",
    "* Bu da YOLO gibi mixed precision / GPU akışında stabil ve sorunsuz.\n",
    "\n",
    "## Kısacası\n",
    "\n",
    "* Residual kapalıysa: alpha=1.0 (fallback / zarar vermez)\n",
    "\n",
    "* Residual açık + alpha_raw var: alpha = sigmoid(alpha_raw) (0–1 arası öğrenilebilir karışım)\n",
    "\n",
    "* Device/dtype uyumu: x.new_tensor ve .to(...) ile garanti altına alınmış\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2430f5a",
   "metadata": {},
   "source": [
    "## Kod-6) Her örnek için ayrı ayrı global std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d89b23",
   "metadata": {},
   "source": [
    "```python \n",
    "    @staticmethod\n",
    "    def _std_per_sample(x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.float().flatten(1).std(dim=1).mean()\n",
    "```\n",
    "\n",
    "**Std, feature’ların yayılımını/enerjisini temsil ediyor. Eğer attention’dan sonra std ciddi şekilde düşüyorsa, bu “attention feature’ları fazla bastırdıbilgi öldü” demek. Biz de giriş–çıkış std’lerini karşılaştırarak bunu otomatik fark ediyoruz ve attention’ı yumuşatıyoruz.**\n",
    "\n",
    "###     @staticmethod \n",
    "* Bu fonksiyon sınıfın içinde dursun ama sınıfın içindeki hiçbir şeye (self’e) dokunmasın.\n",
    "* sadece verdiğin x tensörüne bakıp hesap yapıyor.\n",
    "    \n",
    "### x.float()\n",
    "\n",
    "* Ölçümü float32 ile yapıyor.\n",
    "\n",
    "* fp16/bf16 kullanıyorsan std hesabı daha stabil olur (sayısal taşma/yuvarlama azalır).\n",
    "\n",
    "### flatten(1)\n",
    "\n",
    "* Tensor şekli (B, C, H, W) ise bunu (B, C*H*W) yapar.\n",
    "\n",
    "* Yani her örneğin tüm kanal+uzamsal değerlerini tek uzun vektör gibi düşünür.\n",
    "\n",
    "### .std(dim=1)\n",
    "\n",
    "* Her örnek için (yani her satır için) standart sapmayı hesaplar.\n",
    "\n",
    "- Çıktı şekli (B,) olur: her sample’ın std’si.\n",
    "\n",
    "### .mean()\n",
    "\n",
    "* Bu (B,) değerlerinin ortalamasını alır.\n",
    "\n",
    "* Çıktı: tek bir scalar (0-dim tensor). “Batch ortalama std”.\n",
    "\n",
    "## Neden bunu yaptık?\n",
    "\n",
    "* Eski yaklaşım genelde x.std() gibi tüm batch’i tek havuz yapar. Batch küçükse çok oynar.\n",
    "\n",
    "* Bu yöntem “önce sample bazında ölç, sonra ortalama al” dediği için batch boyutuna daha az hassas ve daha stabil.\n",
    "\n",
    "### Bunu eklemeseydik iki şey olurdu:\n",
    "\n",
    "Over-suppression monitor daha dengesiz / gürültülü çalışırdı.\n",
    "* Özellikle YOLO’da batch küçükken (B=1–8) x.std() gibi “her şeyi tek havuza atan” ölçüm, batch içeriğine göre zıplar. Bu zıplama şu zinciri bozar:\n",
    "\n",
    "r_out = out_std / x_std yanlış oynar\n",
    "\n",
    "* r_ema yanlış güncellenir\n",
    "\n",
    "alpha_eff gereksiz düşer/artar\n",
    "* Sonuç: blok bazen gereksiz “kurtarmaya” girer, bazen de bastırmayı kaçırır.\n",
    "\n",
    "**Batch büyüklüğüne bağımlılık artardı.Aynı model, aynı veri dağılımı; sadece batch değişince rescue davranışı değişirdi. Bu istemediğin bir şey**.\n",
    "\n",
    "\n",
    "### Nerede çağırıyoruz?\n",
    "\n",
    "* Bu fonksiyon sadece monitor açıkken ve training modundayken çağrılıyor.\n",
    "\n",
    "Bizim yazdığımız sınıfta iki yerde:\n",
    "\n",
    "#### A) return_maps=True branch’i içinde\n",
    "```python\n",
    "if self.training and self.monitor:\n",
    "    x_std = self._std_per_sample(x)\n",
    "    y_std = self._std_per_sample(y)\n",
    "\n",
    "    out_tmp = x + alpha * (y - x)\n",
    "    out_std = self._std_per_sample(out_tmp)\n",
    "\n",
    "    r_block = y_std / (x_std + 1e-12)\n",
    "    r_out   = out_std / (x_std + 1e-12)\n",
    "\n",
    "    self._update_r_ema(r_out)\n",
    "    alpha_eff = self._compute_alpha_eff(x, alpha)\n",
    "```\n",
    "\n",
    "#### B) return_maps=False “fast path” içinde\n",
    "```python\n",
    "if self.training and self.monitor:\n",
    "    x_std = self._std_per_sample(x)\n",
    "    out_tmp = x + alpha * (y - x)\n",
    "    out_std = self._std_per_sample(out_tmp)\n",
    "    r_out = out_std / (x_std + 1e-12)\n",
    "\n",
    "    self._update_r_ema(r_out)\n",
    "    alpha_eff = self._compute_alpha_eff(x, alpha)\n",
    "```\n",
    "**Özet: _std_per_sample() sadece “rescue/monitor mekanizmasının std ölçümü” için var.Monitor kapalıysa veya model.eval() ise hiç çalışmıyor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1fa91",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Kod-7) `_update_r_ema` ne yapıyor?\n",
    "```python\n",
    "    @torch.no_grad()\n",
    "    def _update_r_ema(self, r_out: torch.Tensor):\n",
    "        r_det = r_out.detach().to(device=self.r_ema.device, dtype=self.r_ema.dtype)\n",
    "        self.r_ema.mul_(self.ema_m).add_((1.0 - self.ema_m) * r_det)\n",
    "```\n",
    "**Bu fonksiyon, attention’dan çıkan anlık ölçümü (r_out) zıplamasın diye yumuşatıp (EMA) r_ema içinde saklar.**\n",
    "\n",
    "Bu fonksiyon **over-suppression monitor** için tuttuğumuz `r_ema` değerini **EMA (Exponential Moving Average)** ile güncelliyor.\n",
    "\n",
    "- **Girdi:** `r_out`  \n",
    "  `r_out` genelde `out_std / x_std` gibi bir oran. Yani “çıktı enerjisi girişe göre ne kadar azaldı/çoğaldı?” sinyali.\n",
    "\n",
    "- **Amaç:**  \n",
    "  Bu oran tek forward’da çok oynayabilir (batch küçük, veri değişken).  \n",
    "  Bu yüzden **anlık `r_out` yerine yumuşatılmış `r_ema`** kullanıyoruz.\n",
    "\n",
    "\n",
    "### Satır satır:\n",
    "\n",
    "- `@torch.no_grad()`  \n",
    "  Bu fonksiyonun çalışması **grad graph’a girmesin**. Çünkü bu bir “monitor / state update”, eğitim sinyalinin parçası değil.\n",
    "\n",
    "- `r_det = r_out.detach().to(device=self.r_ema.device, dtype=self.r_ema.dtype)`  \n",
    "  `r_out` üzerinde:\n",
    "  - `detach()` ile graph’tan koparıyoruz  \n",
    "  - `r_ema` hangi cihazda/dtype’taysa oraya taşıyoruz  \n",
    "  Böylece device/dtype mismatch hatası yaşamazsın.\n",
    "\n",
    "- `self.r_ema.mul_(self.ema_m).add_((1.0 - self.ema_m) * r_det)`  \n",
    "  EMA güncellemesi:\n",
    "\n",
    "  `r_ema = ema_m * r_ema + (1 - ema_m) * r_out`\n",
    "\n",
    "  Buradaki `mul_` ve `add_` **in-place** çalışır; buffer olan `r_ema` direkt güncellenir.\n",
    "\n",
    "\n",
    "### Bu neden önemli?\n",
    "\n",
    "* `r_out` anlık ölçüm olduğu için gürültülü olabilir.  \n",
    "* `r_ema` ile bunu yumuşatınca `alpha_eff` gibi rescue kararları **daha stabil** olur (YOLO gibi küçük batch’lerde kritik).\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783881b",
   "metadata": {},
   "source": [
    "# Kod-8) _compute_alpha_eff \n",
    "```python\n",
    "    def _compute_alpha_eff(self, x: torch.Tensor, alpha: torch.Tensor) -> torch.Tensor:\n",
    "        ratio = (self.r_ema.detach() / max(self.r_min, 1e-12)).clamp(0.0, 1.0)\n",
    "        ratio = ratio.to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if self.rescue_mode == \"ratio_floor\":\n",
    "            ratio = ratio.clamp(self.min_rescue_ratio, 1.0)\n",
    "            return alpha * ratio\n",
    "\n",
    "        alpha_eff = alpha * ratio\n",
    "        return alpha_eff.clamp(self.alpha_eff_min, 1.0)\n",
    "```\n",
    "**Bu blok “over-suppression rescue” kısmının kalbi. Amaç: blok çok bastırıyorsa residual karışımı otomatik yumuşatmak (alpha’yı efektif olarak azaltmak ya da en azından kontrol altına almak).**\n",
    "\n",
    "## 1) ratio hesabı: bastırma sinyali → ölçek faktörü\n",
    "```python\n",
    "ratio = (self.r_ema.detach() / max(self.r_min, 1e-12)).clamp(0.0, 1.0)\n",
    "ratio = ratio.to(device=x.device, dtype=x.dtype)\n",
    "```\n",
    "### `ratio` kavramı neyi temsil ediyor?\n",
    "\n",
    "- **`self.r_ema`**  \n",
    "  EMA ile yumuşatılmış *çıktı / girdi enerji oranı*.  \n",
    "  Genelde şu şekilde ölçülür:\n",
    ">r_out = out_std / x_std\n",
    "\n",
    "\n",
    "Bu değer:\n",
    "- 1’e yakınsa → enerji korunuyor\n",
    "- Küçükse → blok fazla bastırıyor (over-suppression)\n",
    "\n",
    "\n",
    "### `self.r_min` ne demek?\n",
    "\n",
    "- **Kabul edilebilir minimum oran eşiği**\n",
    "- Örnek: `r_min = 0.45`\n",
    "\n",
    ">r_ema < 0.45 → \"fazla bastırma var\" sinyali\n",
    "\n",
    "\n",
    "### `r_ema / r_min` neden hesaplanıyor?\n",
    "\n",
    "Bu oran **rescue ölçeğini** üretir:\n",
    "\n",
    "- `r_ema == r_min` → `ratio = 1`\n",
    "- `r_ema < r_min`  → `ratio < 1`  → `alpha` düşürülür\n",
    "- `r_ema > r_min`  → `ratio > 1` ama **üstten 1.0 clamp** olduğu için büyütülmez\n",
    "\n",
    "Amaç:\n",
    "- Sadece **azaltıcı** etki yapmak\n",
    "- Asla alpha’yı şişirmemek\n",
    "\n",
    "\n",
    "### Neden `.detach()` kullanılıyor?\n",
    "\n",
    "```python\n",
    "self.r_ema.detach()\n",
    "```\n",
    "detach() şunu yapar:\n",
    "\n",
    "* r_ema üzerinden gradient akışını keser\n",
    "\n",
    "* Monitor sinyalinin geri yayılım zincirine karışmasını engeller\n",
    "\n",
    "Bunu yapmazsan:\n",
    "\n",
    "* Model, “performansı artırmak” için r_ema yolunu kullanıp sistemi kandırmaya çalışabilir.\n",
    "\n",
    "* “Rescue” mekanizması öğrenilen bir shortcut haline gelebilir.\n",
    "\n",
    "* Training stabilitesi bozulabilir (özellikle mixed precision + detection).\n",
    "\n",
    "**Özet: detach() = “bu sinyal öğrenme hattında değil, kontrol hattında”.**\n",
    "\n",
    "\n",
    "### 3) Neden .clamp(0.0, 1.0)?\n",
    "```python\n",
    "ratio = ratio.clamp(0.0, 1.0)\n",
    "```\n",
    "\n",
    "\n",
    "Bu clamp, rescue’nun sadece azaltıcı olmasını garanti eder:\n",
    "\n",
    "* ratio > 1 olursa: alpha_eff = alpha * ratio büyür\n",
    "→ blok daha agresif olur (rescue mantığına ters).\n",
    "\n",
    "* ratio < 0 olursa: işaret bozulur\n",
    "→ alpha_eff negatif bile olabilir, karışımı tersine çevirir (tam saçmalar).\n",
    "\n",
    "Bu yüzden:\n",
    "\n",
    "* Üst sınır 1.0: “asla büyütme yok”\n",
    "\n",
    "* Alt sınır 0.0: “negatif saçmalık yok”\n",
    "\n",
    "Sonuç:\n",
    "\n",
    "* Rescue etkisi yalnızca alpha’yı düşürür, hiçbir zaman yükseltmez.\n",
    "----\n",
    "\n",
    "# Kod-9) `forward()` burada ne yapıyor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3123212",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "Bu `forward`, bloğun iki işi aynı anda yapmasını sağlıyor:\n",
    "\n",
    "1) **Asıl iş:** `CA -> Coordinate -> (opsiyonel residual karışım)` ile `out` üretmek  \n",
    "2) **Kontrol işi (opsiyonel):** Eğer `training` ve `monitor=True` ise, “fazla bastırma var mı?” diye ölçüp `alpha_eff` ile bloğu otomatik yumuşatmak\n",
    "\n",
    "Kod iki farklı “çalışma modu” içeriyor:\n",
    "\n",
    "- `return_maps=True` → debug/analiz modu (map’leri ve istatistikleri de döndürür)\n",
    "- `return_maps=False` → hızlı yol (YOLO’da default böyle olacak)\n",
    "\n",
    "\n",
    "\n",
    "## A) `return_maps=True` yolu (debug/analiz)\n",
    "\n",
    "### 1) CA ve Coordinate çalıştırılır\n",
    "```python\n",
    "y_ca, ca_map, fusion_w = self.ca(x)\n",
    "y = self.coord(y_ca)\n",
    "```\n",
    "* y_ca: CA uygulanmış feature\n",
    "\n",
    "* ca_map: kanal maskesi (B,C,1,1)\n",
    "\n",
    "* fusion_w: avg/max karışım ağırlıkları (B,2)\n",
    "\n",
    "* y: coordinate ile uzamsal ölçeklenmiş nihai blok çıktısı\n",
    "\n",
    "### 2) Residual kapalıysa erken çıkış\n",
    "```python\n",
    "if not self.residual:\n",
    "    coord_stats = self.coord.last_mask_stats()\n",
    "    return y, ca_map, fusion_w, coord_stats, None\n",
    "```\n",
    "Residual yoksa:\n",
    "\n",
    "* “alpha, rescue, EMA” gibi şeyler anlamsız (karışım yok)\n",
    "\n",
    "* direkt y döner, sadece istatistik verilir\n",
    "### 3) Residual varsa alpha hazırlanır\n",
    "```python\n",
    "alpha = self._alpha(x)\n",
    "alpha_eff = alpha\n",
    "```\n",
    "- alpha: temel karışım katsayısı (sigmoid(alpha_raw))\n",
    "\n",
    "* alpha_eff: başlangıçta alpha ile aynı; monitor devreye girerse düşürülebilir\n",
    "\n",
    "### 4) Monitor açıkken (ve sadece training’de) over-suppression ölçümü\n",
    "```python \n",
    "if self.training and self.monitor:\n",
    "```\n",
    "**Bu blok inference’ta çalışmaz; training sırasında da monitor kapalıysa hiç çalışmaz.**\n",
    "\n",
    "#### 4.1) Standart sapmalar ölçülür\n",
    "```python\n",
    "x_std = self._std_per_sample(x)\n",
    "y_std = self._std_per_sample(y)\n",
    "```\n",
    "\n",
    "\n",
    "* x_std: giriş enerjisi\n",
    "\n",
    "* y_std: blok çıkışının enerjisi (CA+Coord sonrası)\n",
    "\n",
    "#### 4.2) “residual karışım yapılmadan önce” out hesaplanır\n",
    "```python\n",
    "out_tmp = x + alpha * (y - x)\n",
    "out_std = self._std_per_sample(out_tmp)\n",
    "```\n",
    "\n",
    "\n",
    "Buradaki out_tmp şunu simüle eder:\n",
    "\n",
    "**“Eğer rescue yokken sadece alpha ile karıştırsaydım, çıktı enerjisi ne olurdu?”**\n",
    "\n",
    "#### 4.3) İki oran çıkarılır\n",
    "```python\n",
    "r_block = (y_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "r_out   = (out_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "```\n",
    "\n",
    "\n",
    "* r_block: bloğun kendi çıktısı girişe göre ne kadar bastırıyor?\n",
    "\n",
    "* r_out: residual karışım sonrası (rescue uygulanmadan önce) ne kadar bastırıyor?\n",
    "\n",
    "**Not: +1e-12 sıfıra bölmeyi önler. clamp(0,10) aşırı uçları güvene alır.**\n",
    "\n",
    "#### 4.4) EMA güncellenir ve alpha_eff hesaplanır\n",
    "```python\n",
    "self._update_r_ema(r_out)\n",
    "alpha_eff = self._compute_alpha_eff(x, alpha)\n",
    "```\n",
    "\n",
    "\n",
    "* r_ema: r_out’un zamanla yumuşatılmış hali\n",
    "\n",
    "* alpha_eff: rescue mode’a göre alpha düşürülebilir:\n",
    "\n",
    "* ratio_floor: sadece ratio tabanlı yumuşatma (bloğu “tam kapatmayı” engellemez)\n",
    "\n",
    "* alpha_floor: alpha_eff alt sınırla clamp (daha agresif)\n",
    "\n",
    "#### 4.5) Debug amaçlı monitor_stats hazırlanır\n",
    "```python\n",
    "monitor_stats = {...}\n",
    "```\n",
    "\n",
    "\n",
    "**Bunlar tamamen raporlama içindir, öğrenmeye dahil edilmez (detach ile scalar’a çevriliyor).**\n",
    "\n",
    "### 5) Final çıktı hesaplanır ve döndürülür\n",
    "```python\n",
    "out = x + alpha_eff * (y - x)\n",
    "coord_stats = self.coord.last_mask_stats()\n",
    "return out, ca_map, fusion_w, coord_stats, monitor_stats\n",
    "```\n",
    "* Final karışım artık alpha_eff ile yapılır\n",
    "\n",
    "* “rescue etkisi” burada gerçekten uygulanmış olur\n",
    "\n",
    "\n",
    "## B) return_maps=False yolu (fast path)\n",
    "\n",
    "Bu yol YOLO’ya entegrede default olacak.\n",
    "\n",
    "### 1) CA ve Coord çalışır (map’ler alınmaz)\n",
    "```python\n",
    "y_ca, _ = self.ca(x)\n",
    "y = self.coord(y_ca)\n",
    "```\n",
    "\n",
    "\n",
    "### 2) Residual kapalıysa direkt y döner\n",
    "```python\n",
    "if not self.residual:\n",
    "    return y\n",
    "```\n",
    "\n",
    "### 3) Residual varsa alpha ve (opsiyonel) rescue\n",
    "```python\n",
    "alpha = self._alpha(x)\n",
    "alpha_eff = alpha\n",
    "\n",
    "if self.training and self.monitor:\n",
    "    ...\n",
    "    self._update_r_ema(r_out)\n",
    "    alpha_eff = self._compute_alpha_eff(x, alpha)\n",
    "```\n",
    "\n",
    "\n",
    "* Fast path’te ekstra olarak y_std/r_block tutulmuyor; sadece r_out ile rescue yapılacak kadar ölçüm var.\n",
    "\n",
    "### 4) Final out\n",
    "```python\n",
    "out = x + alpha_eff * (y - x)\n",
    "return out\n",
    "```\n",
    "\n",
    "**Özet: Bu forward’un revize edilme sebebi ne?**\n",
    "\n",
    "* YOLO gibi detection modellerinde attention blokları bazen feature’ı fazla bastırabiliyor.\n",
    "\n",
    "Bu forward şunları garanti ediyor:\n",
    "\n",
    "* Monitor kapalıysa: klasik residual attention davranışı (hızlı)\n",
    "\n",
    "* Monitor açıksa ve training’deyse: bastırmayı ölçer, alpha_eff ile bloğun etkisini otomatik yumuşatır\n",
    "\n",
    "* return_maps=True ise: analiz için gerekli tüm map/istatistikleri döndürür\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfc164",
   "metadata": {},
   "source": [
    "-----\n",
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd4fb65",
   "metadata": {},
   "source": [
    "## Çıktıların Anlamı (Adım Adım)\n",
    "\n",
    "Aşağıdaki çıktı, **CBAM (Channel Attention) + Coordinate Attention + Residual + Monitor** hattının **sağlıklı ve stabil** çalıştığını gösteren tipik bir örnektir. Her satırı tek tek yorumlayalım.\n",
    "\n",
    "\n",
    "\n",
    "## Tensor Boyutları\n",
    "\n",
    "### `x: torch.Size([2, 64, 56, 56])`\n",
    "- **Giriş feature map’i**\n",
    "- `B=2` (batch)\n",
    "- `C=64` (kanal)\n",
    "- `H=W=56` (uzamsal çözünürlük)\n",
    "\n",
    "Bu, YOLO/Detection backbone’larında çok tipik bir ara katman boyutudur.\n",
    "\n",
    "\n",
    "### `out: torch.Size([2, 64, 56, 56])`\n",
    "- **Final çıktı**\n",
    "- Girişle **aynı boyutta**\n",
    "- Bu şunu garanti eder:\n",
    "  - Residual bağlantı **boyut uyumlu**\n",
    "  - Blok “drop-in replacement” olarak backbone’a takılabilir\n",
    "\n",
    "\n",
    "\n",
    "## Channel Attention Çıktıları\n",
    "\n",
    "### `ca_map: torch.Size([2, 64, 1, 1])`\n",
    "- Kanal bazlı attention maskesi\n",
    "- Her kanal için **tek bir skalar ağırlık**\n",
    "- `sigmoid / hardsigmoid` sonrası değerler\n",
    "\n",
    "Bu şu anlama gelir:\n",
    "- Hangi kanalların **önemli**, hangilerinin **bastırılacağı** öğreniliyor\n",
    "- Uzamsal bilgi yok, **saf kanal seçimi**\n",
    "\n",
    "\n",
    "\n",
    "### `fusion_w: torch.Size([2, 2])`\n",
    "- AvgPool + MaxPool **birleştirme ağırlıkları**\n",
    "- Her sample için:\n",
    "  - `[w_avg, w_max]`\n",
    "- Örnek:\n",
    ">[0.5, 0.5]\n",
    "\n",
    "gibi değerler:\n",
    "- Başlangıçta dengeli\n",
    "- Router agresifleşmemiş\n",
    "- Sağlıklı bir başlangıç davranışı\n",
    "\n",
    "\n",
    "## Coordinate Attention İstatistikleri\n",
    "\n",
    "### `coord_stats`\n",
    "\n",
    "```python\n",
    "{\n",
    "'a_h': {...},\n",
    "'a_w': {...}\n",
    "}\n",
    "```\n",
    "Bu istatistikler coordinate attention head’lerinden çıkan maskelerin dağılımını gösterir.\n",
    "\n",
    "### a_h (Height yönü)\n",
    "\n",
    "* mean ≈ 0.50\n",
    "\n",
    "* std ≈ 0.0038\n",
    "\n",
    "* min ≈ 0.46, max ≈ 0.53\n",
    "\n",
    "### a_w (Width yönü)\n",
    "\n",
    "* mean ≈ 0.50\n",
    "\n",
    "* std ≈ 0.0033\n",
    "\n",
    "* min ≈ 0.48, max ≈ 0.52\n",
    "\n",
    "Yorum:\n",
    "\n",
    "* Maskeler çok agresif değil\n",
    "\n",
    "* Ne 0’a çökme var ne 1’e yapışma\n",
    "\n",
    "- Coordinate head init + scale clamp doğru çalışıyor\n",
    "\n",
    "* YOLO için ideal: “yumuşak yönsel vurgu”\n",
    "\n",
    "## Monitor Mekanizması\n",
    "```python\n",
    "monitor_stats keys\n",
    "['x_std', 'y_std', 'out_std_pre', 'r_block', 'r_out_pre',\n",
    " 'r_ema', 'alpha', 'alpha_eff', 'rescue_mode']\n",
    "```\n",
    "\n",
    "\n",
    "* Bu liste, over-suppression kontrolünün aktif olduğunu gösterir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d126014",
   "metadata": {},
   "source": [
    "## Monitor İstatistikleri – Özet Tablo\n",
    "\n",
    "| Anahtar | Değer | Ne Anlama Geliyor | Yorum |\n",
    "|------|------|------------------|-------|\n",
    "| **x_std** | ≈ 1.00 | Giriş tensörünün enerji seviyesi | Referans nokta, normalize civarı |\n",
    "| **y_std** | ≈ 0.66 | CA + Coordinate sonrası enerji | Attention çalışıyor, ama agresif değil |\n",
    "| **out_std_pre** | ≈ 0.74 | Residual karışım sonrası enerji | Enerji geri kazanılmış |\n",
    "| **r_block** | ≈ 0.66 | Bloğun tek başına bastırma oranı | r_min=0.45 üstünde, güvenli |\n",
    "| **r_out_pre** | ≈ 0.74 | Residual sonrası bastırma oranı | Asıl kritik metrik, sağlıklı |\n",
    "| **r_ema** | ≈ 0.99 | EMA ile yumuşatılmış oran | Uzun vadede enerji korunuyor |\n",
    "| **alpha** | 0.75 | Temel residual karışım oranı | Model attention’a güveniyor |\n",
    "| **alpha_eff** | 0.75 | Rescue sonrası etkili alpha | Müdahale gerekmedi |\n",
    "| **rescue_mode** | ratio_floor | Rescue stratejisi | Yumuşak, zorlamayan koruma |\n",
    "\n",
    "---\n",
    "\n",
    "## Coordinate Attention Maskeleri – Özet Tablo\n",
    "\n",
    "| Maske | Min | Mean | Max | Std | Yorum |\n",
    "|-----|-----|------|-----|-----|------|\n",
    "| **a_h (Height)** | ≈ 0.46 | ≈ 0.50 | ≈ 0.53 | ≈ 0.0038 | Yönsel vurgu var, aşırı değil |\n",
    "| **a_w (Width)** | ≈ 0.48 | ≈ 0.50 | ≈ 0.52 | ≈ 0.0033 | Stabil, yumuşak davranış |\n",
    "\n",
    "---\n",
    "\n",
    "## Genel Değerlendirme – Tek Cümlelik Okuma\n",
    "\n",
    "| Kriter | Sonuç |\n",
    "|------|------|\n",
    "| Attention agresif mi? | ❌ Hayır |\n",
    "| Over-suppression var mı? | ❌ Hayır |\n",
    "| Residual enerji dengeliyor mu? | ✅ Evet |\n",
    "| Rescue mekanizması gerekli mi? | ❌ Şu an değil |\n",
    "| YOLO backbone uyumu | ✅ Güvenli |\n",
    "\n",
    "**Özet:**  \n",
    "Bu tablo seti, attention bloğunun **enerji dostu**, **kontrollü** ve **YOLO için doğrudan kullanılabilir** olduğunu net şekilde gösteriyor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511da7a",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "-----\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
