{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5152d6ee",
   "metadata": {},
   "source": [
    "# CBAM + CoordinateAttPlus (Adım-2)\n",
    "\n",
    "## Amaç\n",
    "\n",
    "Bu adımın amacı, **CBAM + Coordinate Attention** yapısını  \n",
    "**YOLO / detection mimarilerinde stabil, güvenli ve patlamayan** bir attention bloğu\n",
    "haline getirmektir.\n",
    "\n",
    "Özellikle:\n",
    "- küçük batch,\n",
    "- AMP / FP16,\n",
    "- erken eğitim evresi\n",
    "\n",
    "gibi senaryolarda attention’ın **feature’ları aşırı bastırmasını** engellemek hedeflenmiştir.\n",
    "\n",
    "---\n",
    "\n",
    "## Ne eklendi? (Adım-2 ile gelen ana guardrail’ler)\n",
    "\n",
    "### 1) Learnable Temperature (T) + Clamp\n",
    "Channel Attention tarafındaki temperature (T):\n",
    "\n",
    "- Öğrenilebilir veya sabit olabilir\n",
    "- Her forward’da **[t_min, t_max]** aralığına clamp edilir\n",
    "- Input ile **aynı device ve dtype**’a taşınır\n",
    "\n",
    "**Amaç:**  \n",
    "T’nin aşırı küçülüp attention’ı “hard gate”e çevirmesini  \n",
    "ya da aşırı büyüyüp attention’ı etkisizleştirmesini önlemek.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Coordinate Attention Head’leri için güvenli init\n",
    "Height ve Width attention head’leri:\n",
    "\n",
    "- Küçük standart sapmalı normal dağılım ile başlatılır\n",
    "- Bias’lar sıfırlanır\n",
    "\n",
    "**Amaç:**  \n",
    "Eğitimin ilk adımlarında maskelerin 0 veya 1’e yapışmasını engellemek.  \n",
    "YOLO’da erken iterasyon stabilitesini artırmak.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Alpha ile “1’e karıştırma” (yumuşak attention)\n",
    "Coordinate attention maskeleri **direkt uygulanmaz**.\n",
    "\n",
    "Bunun yerine:\n",
    "* scale = 1 + alpha * (attn - 1)\n",
    "\n",
    "\n",
    "- `alpha → 0` : attention kapalıya yakın\n",
    "- `alpha → 1` : attention tam uygulanır\n",
    "\n",
    "**Amaç:**  \n",
    "Attention’ın “var / yok” gibi sert davranması yerine,\n",
    "etkisini **kontrollü ve kademeli** vermek.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Global Beta ile ek yumuşatma\n",
    "Alpha’dan sonra tüm scale şu şekilde yumuşatılır:\n",
    "* scale = 1 + beta * (scale - 1)\n",
    "\n",
    "\n",
    "- `beta = 0` → attention tamamen kapalı\n",
    "- `beta = 1` → scale aynen uygulanır\n",
    "\n",
    "**Amaç:**  \n",
    "Tüm attention bloğunun agresifliğini tek bir global düğme ile ayarlamak.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Scale Clamp (güvenlik bariyeri)\n",
    "Son scale değeri:\n",
    "* scale ∈ [scale_min, scale_max]\n",
    "aralığına kilitlenir.\n",
    "\n",
    "Örnek:\n",
    "- en fazla %40 bastırma\n",
    "- en fazla %60 güçlendirme\n",
    "\n",
    "**Amaç:**  \n",
    "Attention maskeleri ne yaparsa yapsın,\n",
    "feature enerjisinin **kontrol dışına çıkmasını** engellemek.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) Residual + Over-Suppression Monitor (EMA tabanlı)\n",
    "Residual karışım şu formdadır:\n",
    "* out = x + alpha_eff * (y - x)\n",
    "\n",
    "\n",
    "Eğitim sırasında:\n",
    "- giriş / çıkış enerji oranı ölçülür\n",
    "- EMA ile yumuşatılır\n",
    "- blok **fazla bastırıyorsa**, `alpha_eff` otomatik düşürülür\n",
    "\n",
    "**Amaç:**  \n",
    "Attention’ın feature’ları “öldürdüğü” durumlarda\n",
    "bloğun kendi kendini yumuşatması.\n",
    "\n",
    "---\n",
    "\n",
    "## Hangi problemleri çözüyor?\n",
    "\n",
    "- ❌ Erken eğitimde attention patlaması  \n",
    "- ❌ Küçük batch’te over-suppression  \n",
    "- ❌ AMP / FP16 device–dtype uyumsuzluğu  \n",
    "- ❌ Attention’ın kontrolsüz agresifleşmesi  \n",
    "\n",
    "---\n",
    "\n",
    "## YOLO için neden güvenli?\n",
    "\n",
    "- Giriş / çıkış boyutları korunur (drop-in block)\n",
    "- AMP ve GPU uyumlu\n",
    "- Attention hiçbir zaman sınırsız bastıramaz\n",
    "- Residual enerji dengesini geri kazandırır\n",
    "- Eğitim sırasında otomatik “rescue” vardır\n",
    "\n",
    "---\n",
    "\n",
    "## Tasarım Prensipleri (5 Madde)\n",
    "\n",
    "1. **Attention asla sınırsız olmamalı**\n",
    "2. **Her agresiflik için bir yumuşatma düğmesi olmalı**\n",
    "3. **Residual enerji dengeleyici olarak kullanılmalı**\n",
    "4. **Monitor sinyali öğrenmeden ayrı tutulmalı**\n",
    "5. **Detection’da stabilite, ham güçten daha önemlidir**\n",
    "\n",
    "---\n",
    "\n",
    "## Tek Cümlelik Özet\n",
    "\n",
    "Bu blok, CBAM + Coordinate Attention’ı  \n",
    "**YOLO için enerji dostu, kontrollü ve kendi kendini dengeleyen**\n",
    "bir attention yapısına dönüştürür.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1057f2",
   "metadata": {},
   "source": [
    "-----\n",
    "----\n",
    "----\n",
    "# Şimdi ise kod tarafından yorumlayarak gidelim.Kod tarafında bulunan yorumlara dikkat ediniz.Burda yer almayan açıklamalar için Yöntem-1 klasöründen ulaşabilirsiniz.Kod içerisinde nelerin bulunup bulunmadığı detaylı biçimde anlatılmıştır.\n",
    "\n",
    "\n",
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16627287",
   "metadata": {},
   "source": [
    "# 1) Büyük resim şeması ( Channel Attention )\n",
    "**Bu modül her feature map için önce avg ve max ile kanalları özetliyor, sonra aynı MLP ile iki ayrı kanal skor seti çıkarıyor (avg tabanlı ve max tabanlı). Ardından ya basitçe topluyor ya da softmax router ile her sample için avg–max karışım ağırlığı öğrenip z’yi oluşturuyor. Sonra z’yi T ile sertlik ayarı yapıp gate’ten geçirerek kanal maskesi üretiyor ve en son beta ile 1’e yaklaştırıp feature’ı öldürmeyecek güvenli ölçekle x’i çarpıyor.**\n",
    "```bash\n",
    "x (B,C,H,W)\n",
    "   |\n",
    "   |-- AvgPool over (H,W) ---> avg_s (B,C,1,1) ----\\\n",
    "   |                                                \\\n",
    "   |-- MaxPool over (H,W) ---> max_s (B,C,1,1) ----->  MLP (aynı MLP)\n",
    "                                                     /        \\\n",
    "                                             a=MLP(avg_s)    m=MLP(max_s)\n",
    "                                             (B,C,1,1)       (B,C,1,1)\n",
    "                                                     \\        /\n",
    "                                                      \\      /\n",
    "                                           fusion (sum veya softmax router)\n",
    "                                                       |\n",
    "                                                       v\n",
    "                                                   z (B,C,1,1)\n",
    "                                                       |\n",
    "                                            divide by T (sertlik ayarı)\n",
    "                                                       |\n",
    "                                           gate (sigmoid/hardsigmoid)\n",
    "                                                       |\n",
    "                                                   ca (B,C,1,1)\n",
    "                                                       |\n",
    "                                       scale_ca = 1 + beta*(ca-1)\n",
    "                                                       |\n",
    "                                                       v\n",
    "                                       y = x * scale_ca  (B,C,H,W)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def softplus_inverse(y:torch.Tensor , eps:float = 1e-6)->torch.Tensor:\n",
    "    return torch.log(torch.clamp(torch.exp(y) -1.0 , min=eps))\n",
    "# Bu kod softplus fonksiyonunun tanımlanmış halidir.\n",
    "# İleride öğrenilebilir temp tanımlarken kullanacağız.\n",
    "# Bu haliyle aslında :: direkt y ve eps değerlerini yazarak kullanım sağlıyoruz.\n",
    "# Kullanım ise  :: :: ::  t_inv = softplus_inverse(t0, eps=self.eps) 'dir\n",
    "\n",
    "def _get_gate(gate: str):\n",
    "    # Gate türünü string olarak alıyoruz (örn: \"sigmoid\", \"hardsigmoid\")\n",
    "    # Ama dışarıdan büyük/küçük harf karışık gelebilir.\n",
    "    # Bu yüzden normalize edip lower() yapıyoruz.\n",
    "    g = gate.lower()\n",
    "    # Klasik sigmoid:\n",
    "    # - Çıkış aralığı (0, 1)\n",
    "    # - Yumuşak ama pahalı (exp içerir)\n",
    "    # - Attention maskelerinde sık kullanılır\n",
    "    if g == \"sigmoid\":\n",
    "        return torch.sigmoid\n",
    "    # HardSigmoid:\n",
    "    # - Sigmoid'in parçalı-lineer, daha ucuz versiyonu\n",
    "    # - Mobil / YOLO / hız kritik senaryolarda tercih edilir\n",
    "    # - Saturation daha kontrollüdür, FP16'da daha stabil olabilir\n",
    "    if g == \"hardsigmoid\":\n",
    "        return F.hardsigmoid\n",
    "    # Buraya düşüyorsa:\n",
    "    # - Kullanıcı desteklenmeyen bir gate ismi vermiştir\n",
    "    # - Sessizce yanlış davranmak yerine FAIL FAST yapıyoruz\n",
    "    # - Böylece config hataları erken yakalanır\n",
    "    raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "\n",
    "def _get_act(act: str):\n",
    "    # Aktivasyon fonksiyonunu string ile seçiyoruz\n",
    "    # Aynı şekilde case-insensitive olması için lower()\n",
    "    a = act.lower()\n",
    "    # ReLU:\n",
    "    # - Basit, hızlı\n",
    "    # - Negatifleri sıfırlar\n",
    "    # - Attention MLP'lerinde hâlâ yaygın\n",
    "    # inplace=True:\n",
    "    # - Ekstra tensor allocation yok\n",
    "    # - Bellek açısından daha verimli\n",
    "    if a == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    # SiLU (Swish):\n",
    "    # - x * sigmoid(x)\n",
    "    # - ReLU'dan daha yumuşak\n",
    "    # - Gradient akışı daha stabil\n",
    "    # - Modern CNN/YOLO varyantlarında daha çok tercih edilir\n",
    "    if a == \"silu\":\n",
    "        return nn.SiLU(inplace=True)\n",
    "    # Desteklenmeyen aktivasyon verilirse:\n",
    "    # - Sessiz fallback YOK\n",
    "    # - Bilerek exception fırlatıyoruz\n",
    "    # - Yanlış deneylerin önüne geçer\n",
    "    raise ValueError(\"act 'relu' veya 'silu' olmalı.\")\n",
    "\n",
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",\n",
    "        gate: str = \"sigmoid\",\n",
    "        temperature: float = 0.9,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        act: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "        fusion_router_hidden: int = 16,\n",
    "        return_fusion_weights: bool = False,\n",
    "        t_min: float = 0.5,\n",
    "        t_max: float = 3.0,\n",
    "        router_temperature: float = 1.5,\n",
    "        beta_ca: float = 0.35,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalı.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalı.\")\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalı.\")\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if fusion == \"softmax\" and fusion_router_hidden < 1:\n",
    "            raise ValueError(\"fusion_router_hidden >= 1 olmalı.\")\n",
    "        if t_min <= 0 or t_max <= 0 or t_min > t_max:\n",
    "            raise ValueError(\"T clamp aralığı hatalı.\")\n",
    "        if router_temperature <= 0:\n",
    "            raise ValueError(\"router_temperature pozitif olmalı.\")\n",
    "        if beta_ca < 0:\n",
    "            raise ValueError(\"beta_ca >= 0 olmalı.\")\n",
    "\n",
    "        # Küçük sayısal kararlılık sabiti (softplus/log/ bölme vb. işlemlerde patlamayı önler)\n",
    "        self.eps = float(eps)\n",
    "\n",
    "        # AvgPool + MaxPool bilgisini nasıl birleştireceğimizi belirler (örn: \"sum\" / \"softmax\" gibi)\n",
    "        self.fusion = fusion\n",
    "\n",
    "        # Debug/analiz için router'ın (fusion) ağırlıklarını dışarı döndürmek istersek True\n",
    "        self.return_fusion_weights = return_fusion_weights\n",
    "\n",
    "        # Kanal maskesinin son gate fonksiyonu (sigmoid veya hardsigmoid) seçilir\n",
    "        self.gate_fn = _get_gate(gate)\n",
    "\n",
    "        # Temperature clamp sınırları: CA'nın sertliğini kontrol eder (T bu aralıkta tutulur)\n",
    "        self.t_min = float(t_min)\n",
    "        self.t_max = float(t_max)\n",
    "\n",
    "        # Router (avg/max birleştirme) tarafında softmax kullanılıyorsa onun sıcaklığı (softmax keskinliği)\n",
    "        self.Tr = float(router_temperature)\n",
    "\n",
    "        # CA çıktısını yumuşatma katsayısı (maskeyi 1'e doğru çeker; agresifliği azaltır)\n",
    "        self.beta_ca = float(beta_ca)\n",
    "\n",
    "        # CA içindeki \"MLP\" ara kanal sayısı (reduction ile düşürür, ama min_hidden altına inmez)\n",
    "        hidden = max(int(min_hidden), int(channels) // int(reduction))\n",
    "\n",
    "        # Global Average Pooling: (B,C,H,W) -> (B,C,1,1) kanal özetini çıkarır\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Global Max Pooling: (B,C,H,W) -> (B,C,1,1) en güçlü aktivasyonları yakalar\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # 1x1 conv \"fc1\": kanalları hidden boyutuna indirir (SE/CBAM tarzı bottleneck)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=bias)\n",
    "\n",
    "        # Ara aktivasyon (relu veya silu)\n",
    "        self.act = _get_act(act)\n",
    "\n",
    "        # 1x1 conv \"fc2\": hidden'dan tekrar channels'a çıkarır (kanal maskesi logits üretimi)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        if self.fusion == \"softmax\":\n",
    "            # Softmax fusion: avg/max bilgisinden öğrenilebilir şekilde fusion ağırlıkları üretmek için küçük bir router.\n",
    "            # Giriş 2*channels: (avg_pool, max_pool) concat edildiği varsayımıyla.\n",
    "            self.fusion_router = nn.Sequential(\n",
    "                nn.Conv2d(2 * channels, fusion_router_hidden, kernel_size=1, bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(hidden, channels, kernel_size=1, bias=bias),\n",
    "            )\n",
    "            # Son katmanı sıfırdan başlatıyoruz:\n",
    "            # Başlangıçta logits ~ 0 olsun -> softmax tarafsız başlasın (genelde 0.5/0.5 gibi),\n",
    "            # erken eğitimde agresif yönlenme / over-suppression riskini azaltır.\n",
    "            last = self.fusion_router[-1]\n",
    "            nn.init.zeros_(last.weight)\n",
    "            if last.bias is not None:\n",
    "                nn.init.zeros_(last.bias)\n",
    "        else:\n",
    "            # fusion=\"sum\" gibi modlarda router'a gerek yok (sabit birleştirme).\n",
    "            self.fusion_router = None\n",
    "\n",
    "        self.learnable_temperature = bool(learnable_temperature)\n",
    "\n",
    "        if self.learnable_temperature:\n",
    "            # İsteğe bağlı strict kontrol (istersen aç)\n",
    "            # if not (self.t_min <= float(temperature) <= self.t_max):\n",
    "            #     raise ValueError(f\"temperature [{self.t_min}, {self.t_max}] aralığında olmalı (learnable_temperature=True).\")\n",
    "            # Stabil başlangıç: temperature'ı clamp edip t_raw'ı onunla başlat.\n",
    "            t0 = float(temperature)\n",
    "            # Sınırların tam üstüne yapışmayı engellemek için küçük güvenlik payı\n",
    "            lo = self.t_min + self.eps\n",
    "            hi = self.t_max - self.eps\n",
    "            # Eğer kullanıcı ters aralık verdiyse zaten yukarıda kontrol var; yine de defansif:\n",
    "            if lo >= hi:\n",
    "                lo = self.t_min\n",
    "                hi = self.t_max\n",
    "            t0 = min(max(t0, lo), hi)\n",
    "            t_inv = softplus_inverse(torch.tensor(t0), eps=self.eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            # Fixed temperature için de clamp'li başlatmak istersen (opsiyonel):\n",
    "            # t0 = float(temperature)\n",
    "            # t0 = min(max(t0, self.t_min), self.t_max)\n",
    "            # self.register_buffer(\"T\", torch.tensor(t0))\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def get_T(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            if self.learnable_temperature:\n",
    "                T = F.softplus(self.t_raw) + self.eps\n",
    "            else:\n",
    "                T = self.T\n",
    "            T = T.to(device=x.device, dtype=x.dtype)\n",
    "            T_clamped = T.clamp(self.t_min, self.t_max)\n",
    "            # Opsiyonel: clamp'e yapışma kontrolü (debug)\n",
    "            # if self.training and getattr(self, \"monitor_T\", False):\n",
    "            #     with torch.no_grad():\n",
    "            #         self._T_value = float(T_clamped.detach().cpu())\n",
    "            #         self._T_hit_min = bool((T_clamped <= (self.t_min + 1e-6)).item())\n",
    "            #         self._T_hit_max = bool((T_clamped >= (self.t_max - 1e-6)).item())\n",
    "            return T_clamped\n",
    "    \n",
    "    def mlp(self,s:torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (B,C,H,W) -> (B,C,1,1): kanal başına global özet (ortalama aktivasyon)\n",
    "        avg_s = self.avg_pool(x)\n",
    "        # (B,C,H,W) -> (B,C,1,1): kanal başına global özet (en güçlü aktivasyon)\n",
    "        max_s = self.max_pool(x)\n",
    "        # Avg özetini MLP'den geçir: (B,C,1,1) -> (B,C,1,1) kanal logits/score üretir\n",
    "        a = self.mlp(avg_s)\n",
    "        # Max özetini MLP'den geçir: (B,C,1,1) -> (B,C,1,1) kanal logits/score üretir\n",
    "        m = self.mlp(max_s)\n",
    "        # İsteğe bağlı debug çıktısı: fusion ağırlıkları (sum modunda yok)\n",
    "        self.fusion_w = None\n",
    "        if self.fusion == \"sum\":\n",
    "            # Basit birleştirme: Avg ve Max katkılarını eşit kabul et\n",
    "            z = a + m\n",
    "        else: # Her sample için “avg mı daha güvenilir, max mı daha güvenilir?”\n",
    "            # Router tabanlı birleştirme: Avg/Max için ağırlık öğren\n",
    "            # Not: concat edilen şey genelde (a,m) veya (avg_s,max_s) olabilir;\n",
    "            # biz burada (avg_s,max_s) ile router'a \"ham özet\" veriyoruz.\n",
    "            s_cat = torch.cat([avg_s, max_s], dim=1)  # (B,2C,1,1)\n",
    "            # Router logits üretir: (B,2C,1,1) -> (B,2,1,1) veya benzeri; flatten ile (B,2)\n",
    "            logits = self.fusion_router(s_cat).flatten(1)\n",
    "            # logits → router’ın ham kararı\n",
    "            # / Tr → bu kararın yumuşatılması / keskinleştirilmesi\n",
    "            # softmax → avg vs max ağırlıkları\n",
    "            # Temperature (Tr): softmax kararının keskinliğini ayarlar\n",
    "            # Tr, fusion router’ın avg–max kararını ne kadar keskin vereceğini kontrol eden sıcaklıktır.\n",
    "            # Tr küçük -> daha keskin (biri baskın), Tr büyük -> daha yumuşak (dengeli)\n",
    "            fusion_w = torch.softmax(logits / self.Tr, dim=1) # (B,2) # Bu şu demek: B tane satır var, her satırda 2 tane sayı var.\n",
    "            # Avg ve Max için ağırlıkları (B,1,1,1) şekline getir (broadcast için)\n",
    "            w0 = fusion_w[:, 0].view(-1, 1, 1, 1)  # avg ağırlığı\n",
    "            # : → “satırların hepsini al”\n",
    "            # 0 → “sadece 0. sütunu al”\n",
    "            # Yani: “Bütün satırlardan, sadece ilk elemanı seç.”\n",
    "            w1 = fusion_w[:, 1].view(-1, 1, 1, 1)  # max ağırlığı\n",
    "            # Ağırlıklı birleşim: z = w_avg * a + w_max * m\n",
    "            z = w0 * a + w1 * m\n",
    "        # Temperature'ı al: learnable/sabit olabilir, device/dtype uyumlu ve clamp'li gelir\n",
    "        T = self.get_T(x)\n",
    "        # Kanal attention maskesi:\n",
    "        # z/T ile \"sertlik\" ayarlanır; ardından gate (sigmoid/hardsigmoid) ile (0,1) aralığına sıkıştırılır\n",
    "        ca = self.gate_fn(z / T)\n",
    "        # CA'yı yumuşatma (beta_ca):\n",
    "        # ca doğrudan x'e vurulmaz; 1 etrafında karıştırılır -> agresif bastırmayı azaltır\n",
    "        # beta_ca=0  => scale_ca = 1 (CA etkisi kapalı)\n",
    "        # beta_ca=1  => scale_ca = ca (CA tam uygulanır)\n",
    "        scale_ca = 1.0 + self.beta_ca * (ca - 1.0)\n",
    "        # Feature'ı kanal ölçeğiyle yeniden ağırlıklandır\n",
    "        y = x * scale_ca\n",
    "        # İstenirse debug amaçlı fusion ağırlıklarını da döndür\n",
    "        # (fusion=\"sum\" ise fusion_w None olabilir)\n",
    "        if self.return_fusion_weights and (fusion_w is not None):\n",
    "            return y, ca, fusion_w\n",
    "        # Default dönüş: çıktı ve kanal maskesi\n",
    "        return y, ca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519a573",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbbd1d1",
   "metadata": {},
   "source": [
    "# 2) Büyük resim şeması ( Coordinate Attention Plus )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea5d16",
   "metadata": {},
   "source": [
    "**Bu modül, feature map’i iki ayrı eksende özetliyor: biri “yükseklik profili” (H boyunca), diğeri “genişlik profili” (W boyunca). Her profil hem mean hem max ile çıkarılıyor (daha sağlam istatistik). Sonra her eksende hem local depthwise hem dilated depthwise ile çok-ölçekli (multi-scale) bilgi toplanıyor, 1x1 mixer ile kanallar karıştırılıyor. H ve W çıktıları tek bir “shared bottleneck”te birleştirilip işleniyor (C→mid). Ardından tekrar H ve W olarak split edilip iki ayrı head ile attn_h ve attn_w maskeleri üretiliyor. Maskeler direkt vurulmuyor; alpha ile 1’e karıştırılıyor (yumuşak attention), sonra h ve w çarpılıp global beta ile tekrar 1’e yaklaştırılıyor ve scale clamp ile güvenlik bariyerine alınarak x ile çarpılıyor. İstersen en sonda opsiyonel spatial gate ile ekstra bir uzamsal gate daha uygulanıyor.**\n",
    "```bash\n",
    "x (B,C,H,W)\n",
    "   |\n",
    "   |-- H-profile (W üzerinden özet) ------------------------------\\\n",
    "   |        h_profile = 0.5*(mean over W + max over W)            |\n",
    "   |        => (B,C,H,1)                                          |\n",
    "   |                                                              |\n",
    "   |-- W-profile (H üzerinden özet) ---------------------------\\  |\n",
    "            w_profile = 0.5*(mean over H + max over H)          | |\n",
    "            => (B,C,1,W)                                        | |\n",
    "                                                                | |\n",
    "           (Multi-scale yönsel conv: local DW + dilated DW)     | |\n",
    "                |                                               | |\n",
    "      h_local_dw(3x1) + h_dilated_dw(3x1,d)                     | |\n",
    "                |                                               | |\n",
    "           h_channel_mixer (1x1)                                | |\n",
    "                |                                               | |\n",
    "              h_ms (B,C,H,1)                                    | |\n",
    "                                                                | |\n",
    "                                  w_local_dw(1x3) + w_dilated_dw(1x3,d)\n",
    "                                                   |\n",
    "                                            w_channel_mixer (1x1)\n",
    "                                                   |\n",
    "                                           w_ms (B,C,1,W)\n",
    "                                                   |\n",
    "                                     permute -> (B,C,W,1) -------/\n",
    "                                                   |\n",
    "                         concat along \"height axis\" (dim=2)\n",
    "                         hw = cat([h_ms, w_ms_perm]) -> (B,C,H+W,1)\n",
    "                                                   |\n",
    "                                   shared bottleneck (C -> mid)\n",
    "                      proj1x1 + norm + act  -> refine1x1 + norm + act\n",
    "                                                   |\n",
    "                                      mid (B,mid,H+W,1)\n",
    "                                                   |\n",
    "                    split back into H and W parts (dim=2)\n",
    "                     mid_h: (B,mid,H,1)      mid_w: (B,mid,W,1)\n",
    "                                                   |\n",
    "                               permute mid_w -> (B,mid,1,W)\n",
    "                                                   |\n",
    "                       head convs (mid -> C) + hardsigmoid gate\n",
    "                attn_h = head_h(mid_h) -> (B,C,H,1)  in (0,1)\n",
    "                attn_w = head_w(mid_w) -> (B,C,1,W)  in (0,1)\n",
    "                                                   |\n",
    "                          alpha ile 1’e karıştırma (yumuşatma)\n",
    "           alpha_h = sigmoid(alpha_h_raw)  (scalar)   alpha_w = sigmoid(alpha_w_raw)\n",
    "           scale_h = (1-alpha_h) + alpha_h*attn_h     -> (B,C,H,1)\n",
    "           scale_w = (1-alpha_w) + alpha_w*attn_w     -> (B,C,1,W)\n",
    "                                                   |\n",
    "                             birleşik ölçek (broadcast ile çarpılır)\n",
    "                       scale = scale_h * scale_w -> (B,C,H,W)\n",
    "                                                   |\n",
    "                        global beta ile ekstra yumuşatma\n",
    "                       scale = 1 + beta*(scale-1)\n",
    "                                                   |\n",
    "                          clamp güvenlik bariyeri\n",
    "                    scale = clamp(scale_min, scale_max)\n",
    "                                                   |\n",
    "                                                   v\n",
    "                                   out = x * scale  (B,C,H,W)\n",
    "                                                   |\n",
    "                           (opsiyonel) spatial gate (ek kapı)\n",
    "                 sg = DW(3x3) + PW(1x1) -> hardsigmoid -> 1’e karıştır\n",
    "                           out = out * sg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSwish(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # HSwish (Hard-Swish) aktivasyonu:\n",
    "        # - Swish'e benzer ama daha ucuzdur (relu6 ile parçalı-lineer)\n",
    "        # - YOLO / mobil CNN'lerde sık kullanılır (hız + stabil gradient)\n",
    "        # - Formül: x * relu6(x + 3) / 6\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0\n",
    "\n",
    "\n",
    "def make_norm(norm: str, ch: int):\n",
    "    # Normalizasyon seçici:\n",
    "    # - \"bn\"   : BatchNorm2d (batch istatistikleri; küçük batch'te oynayabilir)\n",
    "    # - \"gn\"   : GroupNorm (batch bağımsız; detection/küçük batch'te daha stabil)\n",
    "    # - \"none\" : Identity (norm kapalı)\n",
    "    norm = norm.lower()\n",
    "    if norm == \"bn\":\n",
    "        return nn.BatchNorm2d(ch)\n",
    "\n",
    "    if norm == \"gn\":\n",
    "        # GN: num_groups (g) kanalları tam bölmeli -> ch % g == 0\n",
    "        # g'yi 32'den başlatıp bölünebilir hale gelene kadar düşürüyoruz.\n",
    "        g = min(32, ch)\n",
    "        while ch % g != 0 and g > 2:\n",
    "            g //= 2\n",
    "        # Hâlâ bölünmüyorsa fallback:\n",
    "        # - ch çiftse g=2 (iki grup)\n",
    "        # - ch tekse g=1 (tek grup, LN benzeri)\n",
    "        if ch % g != 0:\n",
    "            g = 2 if (ch % 2 == 0) else 1\n",
    "        return nn.GroupNorm(g, ch)\n",
    "    if norm == \"none\":\n",
    "        return nn.Identity()\n",
    "    raise ValueError(\"norm 'none', 'bn', 'gn' dışında olamaz.\")\n",
    "\n",
    "\n",
    "class CoordinateAttPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,          # Giriş kanal sayısı (C)\n",
    "        reduction: int = 32,       # Bottleneck oranı: C -> mid (yaklaşık C/reduction)\n",
    "        min_mid_channels: int = 8, # mid için alt sınır (çok küçülmesin)\n",
    "        act: str = \"hswish\",       # Aktivasyon: \"hswish\" / \"relu\" / \"silu\"\n",
    "        init_alpha: float = 0.7,   # Başlangıç alpha hedefi (sigmoid sonrası ~0.7)\n",
    "        learnable_alpha: bool = True, # alpha öğrenilebilir mi?\n",
    "        beta: float = 0.35,        # Global yumuşatma: scale'i 1'e çeker (agresifliği azaltır)\n",
    "        dilation: int = 2,         # Dilated DW conv dilation (receptive field büyütür)\n",
    "        norm: str = \"gn\",          # Norm seçimi: bn/gn/none\n",
    "        use_spatial_gate: bool = False,  # Ek spatial gate aç/kapat\n",
    "        spatial_gate_beta: float = 0.35, # Spatial gate yumuşatma katsayısı\n",
    "        scale_min: float = 0.6,    # Final scale clamp alt sınırı (en fazla bastırma)\n",
    "        scale_max: float = 1.6,    # Final scale clamp üst sınırı (en fazla güçlendirme)\n",
    "        head_init_std: float = 0.01, # Head init std: maskeler başlangıçta sakin kalsın\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # -------------------------\n",
    "        # Parametre validasyonları (fail-fast)\n",
    "        # -------------------------\n",
    "        if in_channels < 1:\n",
    "            raise ValueError(\"in_channels >= 1 olmalı.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalı.\")\n",
    "        if dilation < 1:\n",
    "            raise ValueError(\"dilation >= 1 olmalı.\")\n",
    "        if scale_min <= 0 or scale_max <= 0 or scale_min > scale_max:\n",
    "            raise ValueError(\"scale clamp aralığı hatalı.\")\n",
    "        if head_init_std <= 0:\n",
    "            raise ValueError(\"head_init_std pozitif olmalı.\")\n",
    "        # Global yumuşatma ve clamp sınırları (guardrail)\n",
    "        self.beta = float(beta)\n",
    "        self.scale_min = float(scale_min)\n",
    "        self.scale_max = float(scale_max)\n",
    "        # -------------------------\n",
    "        # Bottleneck kanal hesabı (mid)\n",
    "        # -------------------------\n",
    "        # mid: squeeze boyutu; çok küçülürse bilgi kaybı artar.\n",
    "        # mid_floor: in_channels'a göre \"alt limit\" koyuyoruz (8..32 arası, yaklaşık C/4)\n",
    "        mid_floor = max(8, min(32, int(in_channels) // 4))\n",
    "        mid = max(int(min_mid_channels), int(in_channels) // int(reduction))\n",
    "        mid = max(mid, int(mid_floor))  # en az mid_floor olsun\n",
    "        # -------------------------\n",
    "        # Aktivasyon seçimi\n",
    "        # -------------------------\n",
    "        act_l = act.lower()\n",
    "        if act_l == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        elif act_l == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act_l == \"silu\":\n",
    "            self.act = nn.SiLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act 'hswish', 'relu', 'silu' olmalı.\")\n",
    "        # -------------------------\n",
    "        # Shared bottleneck: h ve w yolları ortak bir bottleneck'te işleniyor\n",
    "        # -------------------------\n",
    "        # (C -> mid) projeksiyon + norm\n",
    "        self.shared_bottleneck_proj = nn.Conv2d(in_channels, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_norm = make_norm(norm, mid)\n",
    "        # mid içinde bir refine (1x1) daha + norm\n",
    "        self.shared_bottleneck_refine = nn.Conv2d(mid, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_refine_norm = make_norm(norm, mid)\n",
    "        # -------------------------\n",
    "        # Yönsel (H/W) çok-ölçekli DW konvlar\n",
    "        # -------------------------\n",
    "        # Local DW: küçük receptive field (yakın komşuluk)\n",
    "        self.h_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(3, 1), padding=(1, 0),\n",
    "            groups=in_channels, bias=False\n",
    "        )\n",
    "        self.w_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(1, 3), padding=(0, 1),\n",
    "            groups=in_channels, bias=False\n",
    "        )\n",
    "        # Dilated DW: daha geniş receptive field (uzak bağlam)\n",
    "        # padding=(d,0)/(0,d) seçimi, stride=1 iken boyutu korumak için:\n",
    "        # effective_kernel = (k-1)*d + 1 -> k=3, d=2 => 5; padding=2 ile same korunur.\n",
    "        d = int(dilation)\n",
    "        self.h_dilated_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(3, 1), padding=(d, 0),\n",
    "            dilation=(d, 1), groups=in_channels, bias=False\n",
    "        )\n",
    "        self.w_dilated_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(1, 3), padding=(0, d),\n",
    "            dilation=(1, d), groups=in_channels, bias=False\n",
    "        )\n",
    "        # -------------------------\n",
    "        # Kanal karıştırma (1x1): DW çıktılarında kanallar arası etkileşim yok;\n",
    "        # 1x1 ile kanalları tekrar karıştırıyoruz.\n",
    "        # -------------------------\n",
    "        self.h_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        self.w_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        # -------------------------\n",
    "        # Attention head'leri: mid -> C maskeleri üretir (h ve w ayrı)\n",
    "        # -------------------------\n",
    "        self.h_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        self.w_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "\n",
    "        # Head init: maskeler eğitim başında agresifleşmesin (0.5 civarı yumuşak başlasın)\n",
    "        nn.init.normal_(self.h_attention_head.weight, mean=0.0, std=float(head_init_std))\n",
    "        nn.init.normal_(self.w_attention_head.weight, mean=0.0, std=float(head_init_std))\n",
    "        if self.h_attention_head.bias is not None:\n",
    "            nn.init.zeros_(self.h_attention_head.bias)\n",
    "        if self.w_attention_head.bias is not None:\n",
    "            nn.init.zeros_(self.w_attention_head.bias)\n",
    "\n",
    "        # -------------------------\n",
    "        # Alpha init: 1 ile attention'ı karıştırma gücü\n",
    "        # alpha_raw logit uzayında tutulur; forward'da sigmoid ile (0,1)'e gelir.\n",
    "        # -------------------------\n",
    "        eps = 1e-6\n",
    "        a0 = float(init_alpha)\n",
    "        a0 = min(max(a0, eps), 1.0 - eps)  # logit(0/1) -> inf olmasın\n",
    "        raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "\n",
    "        if learnable_alpha:\n",
    "            # Öğrenilebilir alpha: model eğitimde maskeyi ne kadar uygulatacağını ayarlar\n",
    "            self.alpha_h_raw = nn.Parameter(raw0.clone())\n",
    "            self.alpha_w_raw = nn.Parameter(raw0.clone())\n",
    "        else:\n",
    "            # Sabit alpha: state'e girsin, device ile taşınsın diye buffer\n",
    "            self.register_buffer(\"alpha_h_raw\", raw0.clone())\n",
    "            self.register_buffer(\"alpha_w_raw\", raw0.clone())\n",
    "        # -------------------------\n",
    "        # Opsiyonel spatial gate: ekstra bir 3x3 DW + 1x1 ile uzamsal gate\n",
    "        # (İstersen aç; detection’da over-suppression riskine dikkat.)\n",
    "        # -------------------------\n",
    "        self.use_spatial_gate = bool(use_spatial_gate)\n",
    "        self.spatial_gate_beta = float(spatial_gate_beta)\n",
    "        if self.use_spatial_gate:\n",
    "            self.spatial_gate_dw = nn.Conv2d(\n",
    "                in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False\n",
    "            )\n",
    "            self.spatial_gate_pw = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        # Debug için son maskeleri saklamak (stats almak için)\n",
    "        self._last_ah = None\n",
    "        self._last_aw = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B, C, H, W)\n",
    "        _, _, H, W = x.shape\n",
    "        # -------------------------\n",
    "        # 1) Coordinate profile çıkarma\n",
    "        # -------------------------\n",
    "        # h_profile: W boyunca özet -> (B, C, H, 1)\n",
    "        # w_profile: H boyunca özet -> (B, C, 1, W)\n",
    "        # mean + max karışımı: hem genel seviye hem de güçlü aktivasyonlar taşınır\n",
    "        h_profile = 0.5 * (x.mean(dim=3, keepdim=True) + x.amax(dim=3, keepdim=True))\n",
    "        w_profile = 0.5 * (x.mean(dim=2, keepdim=True) + x.amax(dim=2, keepdim=True))\n",
    "        # -------------------------\n",
    "        # 2) Multi-scale yönsel konv (local + dilated) + channel mixer\n",
    "        # -------------------------\n",
    "        h_ms = self.h_channel_mixer(self.h_local_dw(h_profile) + self.h_dilated_dw(h_profile))\n",
    "        w_ms = self.w_channel_mixer(self.w_local_dw(w_profile) + self.w_dilated_dw(w_profile))\n",
    "        # w_ms: (B,C,1,W) -> concat için (B,C,W,1) gibi hizalama\n",
    "        w_ms = w_ms.permute(0, 1, 3, 2)\n",
    "        # h_ms: (B,C,H,1), w_ms: (B,C,W,1) -> dim=2 boyunca birleştir -> (B,C,H+W,1)\n",
    "        hw = torch.cat([h_ms, w_ms], dim=2)\n",
    "        # -------------------------\n",
    "        # 3) Shared bottleneck ile ortak işleme (C -> mid)\n",
    "        # -------------------------\n",
    "        mid = self.act(self.shared_bottleneck_norm(self.shared_bottleneck_proj(hw)))\n",
    "        mid = self.act(self.shared_bottleneck_refine_norm(self.shared_bottleneck_refine(mid)))\n",
    "        # -------------------------\n",
    "        # 4) mid'i tekrar H ve W parçalarına ayır\n",
    "        # -------------------------\n",
    "        mid_h, mid_w = torch.split(mid, [H, W], dim=2)\n",
    "        mid_w = mid_w.permute(0, 1, 3, 2)  # (B,mid,W,1) -> (B,mid,1,W)\n",
    "        # -------------------------\n",
    "        # 5) Head'lerle maskeleri üret (0..1)\n",
    "        # -------------------------\n",
    "        attn_h = F.hardsigmoid(self.h_attention_head(mid_h), inplace=False)  # (B,C,H,1)\n",
    "        attn_w = F.hardsigmoid(self.w_attention_head(mid_w), inplace=False)  # (B,C,1,W)\n",
    "        # Debug için sakla\n",
    "        self._last_ah = attn_h.detach()\n",
    "        self._last_aw = attn_w.detach()\n",
    "        # -------------------------\n",
    "        # 6) Alpha ile 1'e karıştırma (yumuşak uygula)\n",
    "        # -------------------------\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw).to(device=x.device, dtype=x.dtype)  # scalar\n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw).to(device=x.device, dtype=x.dtype)  # scalar\n",
    "\n",
    "        scale_h = (1.0 - alpha_h) + alpha_h * attn_h   # 1 ile attn_h arasında karışım\n",
    "        scale_w = (1.0 - alpha_w) + alpha_w * attn_w   # 1 ile attn_w arasında karışım\n",
    "        # -------------------------\n",
    "        # 7) Global beta yumuşatma + clamp guardrail\n",
    "        # -------------------------\n",
    "        scale = scale_h * scale_w\n",
    "        scale = 1.0 + self.beta * (scale - 1.0)                # scale'i 1'e çek\n",
    "        scale = scale.clamp(self.scale_min, self.scale_max)    # aşırı bastırma/boost engeli\n",
    "        # Uygula\n",
    "        out = x * scale\n",
    "        # -------------------------\n",
    "        # 8) Opsiyonel spatial gate\n",
    "        # -------------------------\n",
    "        if self.use_spatial_gate:\n",
    "            sg = self.spatial_gate_pw(self.spatial_gate_dw(x))\n",
    "            sg = F.hardsigmoid(sg, inplace=False)\n",
    "            sg = 1.0 + self.spatial_gate_beta * (sg - 1.0)\n",
    "            out = out * sg\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def last_mask_stats(self):\n",
    "        # Debug: son attn_h/attn_w maskelerinin basit istatistikleri\n",
    "        if (self._last_ah is None) or (self._last_aw is None):\n",
    "            return None\n",
    "        ah = self._last_ah\n",
    "        aw = self._last_aw\n",
    "        return {\n",
    "            \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "            \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc8056",
   "metadata": {},
   "source": [
    "# 3-) Büyük Resim Şeması"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894cae48",
   "metadata": {},
   "source": [
    "```bash\n",
    "Girdi: x  (B,C,H,W)\n",
    "  |\n",
    "  |------------------------------------|\n",
    "  |                                    |\n",
    "  |     [ChannelAttentionFusionT]      |\n",
    "  |   (CBAM Channel + T + beta_ca)     |\n",
    "  |                                    |\n",
    "  |  y_ca, ca_map, (fusion_w?) = CA(x) |\n",
    "  |        (B,C,H,W)    (B,C,1,1)      |\n",
    "  |                                    |\n",
    "  |------------------------------------|\n",
    "                  |\n",
    "                  v\n",
    "     [CoordinateAttPlus]  (CoordAtt + alpha_h/alpha_w + beta + clamp + opsiyonel spatial gate)\n",
    "                  |\n",
    "                  v\n",
    "              y = Coord(y_ca)          (B,C,H,W)\n",
    "                  |\n",
    "                  v\n",
    "         if residual == False:\n",
    "              return y  (veya debug ise maps/statlar)\n",
    "                  |\n",
    "                  v\n",
    "      residual == True  ->  Residual Mixer\n",
    "                  |\n",
    "                  v\n",
    "        alpha = sigmoid(alpha_raw)      (0..1)   (device/dtype uyumlu)\n",
    "        alpha_eff = alpha               (başlangıç)\n",
    "                  |\n",
    "                  v\n",
    "      if training AND monitor:\n",
    "          x_std    = std_per_sample(x)\n",
    "          y_std    = std_per_sample(y)\n",
    "          out_tmp  = x + alpha*(y-x)        (residual karışımın ham hali)\n",
    "          out_std  = std_per_sample(out_tmp)\n",
    "\n",
    "          r_block  = clamp(y_std / x_std, 0..10)\n",
    "          r_out    = clamp(out_std / x_std, 0..10)\n",
    "\n",
    "          r_ema <- EMA update(r_out)\n",
    "          alpha_eff <- compute_alpha_eff(alpha, r_ema, r_min, rescue_mode)\n",
    "          (monitor_stats doldur)\n",
    "                  |\n",
    "                  v\n",
    "        out = x + alpha_eff * (y - x)      (B,C,H,W)\n",
    "                  |\n",
    "                  v\n",
    "      return out\n",
    "      (+ return_maps ise: ca_map, fusion_w, coord_stats, monitor_stats da döner)\n",
    "```\n",
    "**Not: fusion_w sadece CA tarafında ca_fusion=\"softmax\" ve return_maps=True iken anlamlı.**\n",
    "\n",
    "* Bu blok bir feature map alıyor. Önce Channel Attention çalışıyor. Burada model şuna bakıyor: “Hangi kanallar önemli?” Bunun için her kanalın ortalamasını ve maksimumunu alıyor, küçük bir ağdan geçiriyor ve her kanal için bir önem değeri üretiyor. Bu değerler direkt kullanılmıyor, 1’e doğru yumuşatılıyor ki feature map bir anda ölmesin. Sonuçta x, kanal bazında hafifçe güçlenmiş ya da bastırılmış oluyor.\n",
    "\n",
    "* Sonra bu çıktı Coordinate Attention’a giriyor. Burada model şuna bakıyor: “Bu bilgi daha çok dikey yönde mi önemli, yatay yönde mi?” Bunun için yükseklik ve genişlik yönlerinde ayrı ayrı özetler çıkarıyor, hem yakın çevreyi hem de biraz daha geniş alanı gören konvolüsyonlardan geçiriyor. Buradan iki maske çıkıyor: biri H yönü, biri W yönü için. Bu maskeler de yine yumuşak şekilde uygulanıyor, aşırı bastırma engelleniyor.\n",
    "\n",
    "* Eğer residual kapalıysa, bu noktada çıkan feature map direkt çıktı oluyor.\n",
    "\n",
    "* Residual açıksa, blok x ile attention’dan geçmiş y arasında karışım yapıyor. Yani tamamen y’ye atlamıyor, x’ten y’ye doğru kontrollü bir adım atıyor. Bu adımın büyüklüğünü alpha belirliyor.\n",
    "\n",
    "* Eğitim sırasında izleme açıksa, blok kendi kendini kontrol ediyor: “Attention’dan sonra feature map çok mu düzleşti?” diye bakıyor. Bunu standart sapma ile ölçüyor. Eğer feature fazla bastırılmışsa, alpha’yı otomatik olarak küçültüyor. Böylece blok kendini yumuşatıyor ama tamamen kapanmıyor.\n",
    "\n",
    "**Sonuçta bu yapı, attention uygular ama feature’ı öldürmez. Hem kanal bazında, hem yön bazında bakar, üstüne bir de kendini denetleyip gerektiğinde geri adım atar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b091ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAMChannelPlusCoord(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,                 # Giriş/çıkış kanal sayısı (C). Blok drop-in olacağı için sabit kalır.\n",
    "        # -------------------------\n",
    "        # Channel Attention (CA / CBAM channel) parametreleri\n",
    "        # -------------------------\n",
    "        ca_reduction: int = 16,         # CA içindeki bottleneck oranı: C -> hidden (≈ C/ca_reduction)\n",
    "        ca_min_hidden: int = 4,         # hidden alt sınırı: C küçükse MLP tamamen “çökmesin”\n",
    "        ca_fusion: str = \"softmax\",     # AvgPool+MaxPool nasıl birleşecek: \"sum\" sabit, \"softmax\" learnable router\n",
    "        ca_gate: str = \"sigmoid\",       # Kanal maskesi gate fonksiyonu: sigmoid / hardsigmoid\n",
    "        ca_temperature: float = 0.9,    # CA temperature başlangıç değeri (sertlik kontrolü); learnable olabilir\n",
    "        ca_act: str = \"relu\",           # CA MLP aktivasyonu (relu/silu gibi)\n",
    "        ca_fusion_router_hidden: int = 16,  # softmax fusion router ara kanal sayısı (küçük MLP)\n",
    "        learnable_temperature: bool = False, # CA temperature öğrenilsin mi? (True: t_raw parametre; False: buffer)\n",
    "        ca_t_min: float = 0.5,               # CA temperature clamp alt sınırı (T çok küçülüp hard gate olmasın)\n",
    "        ca_t_max: float = 3.0,               # CA temperature clamp üst sınırı (T çok büyüyüp etkisizleşmesin)\n",
    "        ca_router_temperature: float = 1.5,  # Fusion router softmax sıcaklığı (avg vs max karar keskinliği)\n",
    "        beta_ca: float = 0.35,               # CA çıktısını 1'e yaklaştırma (agresifliği yumuşatma): scale=1+beta*(ca-1)\n",
    "        # -------------------------\n",
    "        # Coordinate Attention (CoordAttPlus) parametreleri\n",
    "        # -------------------------\n",
    "        coord_reduction: int = 32,       # Coord bottleneck oranı: C -> mid\n",
    "        coord_min_mid: int = 8,          # mid alt sınırı\n",
    "        coord_act: str = \"hswish\",       # Coord içindeki aktivasyon (hswish/relu/silu)\n",
    "        coord_init_alpha: float = 0.7,   # Coord alpha başlangıcı (1'e karıştırma gücü; başlangıçta attention ne kadar devrede)\n",
    "        coord_learnable_alpha: bool = True,  # Coord alpha öğrenilsin mi? (h ve w için ayrı raw parametre)\n",
    "        coord_beta: float = 0.35,        # Coord global yumuşatma: scale=1+beta*(scale-1)\n",
    "        coord_dilation: int = 2,         # Coord dilated DW conv dilation (receptive field büyütür)\n",
    "        coord_norm: str = \"gn\",          # Coord norm tipi (bn/gn/none). Küçük batch'te GN daha stabil.\n",
    "        coord_use_spatial_gate: bool = False,   # Opsiyonel ek spatial gate (ek bir “kapı” daha)\n",
    "        coord_spatial_gate_beta: float = 0.35,  # Spatial gate yumuşatma katsayısı (1'e karıştırma)\n",
    "        coord_scale_min: float = 0.6,    # Coord final scale clamp alt sınırı (en fazla bastırma)\n",
    "        coord_scale_max: float = 1.6,    # Coord final scale clamp üst sınırı (en fazla güçlendirme)\n",
    "        coord_head_init_std: float = 0.01, # Coord head init std (maskeler başta 0/1'e yapışmasın)\n",
    "        # -------------------------\n",
    "        # Blok seviyesi Residual + Monitor/Rescue parametreleri\n",
    "        # -------------------------\n",
    "        residual: bool = True,           # True: out = x + alpha_eff*(y-x) (enerji kurtarma / stabilite)\n",
    "        alpha_init: float = 0.75,        # Residual karışım başlangıcı (sigmoid sonrası hedef). Bloğun gücü.\n",
    "        learnable_alpha: bool = False,   # Residual alpha öğrenilsin mi? (True: parametre; False: buffer)\n",
    "        monitor: bool = False,           # Training’de over-suppression izleme aç/kapat (std oranı ölçümü)\n",
    "        r_min: float = 0.45,             # Kabul edilen minimum enerji oranı eşiği (r_out < r_min => rescue devreye girer)\n",
    "        ema_momentum: float = 0.95,      # r_ema güncelleme momentumu (yüksek = daha yavaş ama daha stabil)\n",
    "        min_rescue_ratio: float = 0.2,   # ratio_floor modunda ratio için alt taban (tamamen sıfırlamasın diye)\n",
    "        alpha_eff_min: float = 0.2,      # alpha_floor modunda alpha_eff alt sınırı (bloğu tamamen kapatmamak için)\n",
    "        rescue_mode: str = \"ratio_floor\",# Rescue stratejisi (\"ratio_floor\" / \"alpha_floor\" gibi)\n",
    "        return_maps: bool = False,       # Debug modu: ca_map, fusion_w, coord_stats, monitor_stats döndür\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.return_maps = bool(return_maps)\n",
    "        self.residual = bool(residual)\n",
    "\n",
    "        self.monitor = bool(monitor)\n",
    "        self.r_min = float(r_min)\n",
    "        self.ema_m = float(ema_momentum)\n",
    "        self.min_rescue_ratio = float(min_rescue_ratio)\n",
    "        self.alpha_eff_min = float(alpha_eff_min)\n",
    "        self.rescue_mode = str(rescue_mode)\n",
    "\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=ca_reduction,\n",
    "            min_hidden=ca_min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            eps=1e-6,\n",
    "            act=ca_act,\n",
    "            bias=True,\n",
    "            fusion_router_hidden=ca_fusion_router_hidden,\n",
    "            return_fusion_weights=self.return_maps,\n",
    "            t_min=ca_t_min,\n",
    "            t_max=ca_t_max,\n",
    "            router_temperature=ca_router_temperature,\n",
    "            beta_ca=beta_ca,\n",
    "        )\n",
    "\n",
    "        self.coord = CoordinateAttPlus(\n",
    "            in_channels=channels,\n",
    "            reduction=coord_reduction,\n",
    "            min_mid_channels=coord_min_mid,\n",
    "            act=coord_act,\n",
    "            init_alpha=coord_init_alpha,\n",
    "            learnable_alpha=coord_learnable_alpha,\n",
    "            beta=coord_beta,\n",
    "            dilation=coord_dilation,\n",
    "            norm=coord_norm,\n",
    "            use_spatial_gate=coord_use_spatial_gate,\n",
    "            spatial_gate_beta=coord_spatial_gate_beta,\n",
    "            scale_min=coord_scale_min,\n",
    "            scale_max=coord_scale_max,\n",
    "            head_init_std=coord_head_init_std,\n",
    "        )\n",
    "\n",
    "        if residual:\n",
    "            #Amaç aynı — kısıtlı bir değeri (alpha veya T) güvenli ve öğrenilebilir yapmak; \n",
    "            # bunun için alpha’da sigmoid–logit, temperature’da softplus–inverse kullanıyoruz.\n",
    "            eps = 1e-6\n",
    "            a0 = float(alpha_init) \n",
    "            # alpha_init bizim verdiğimiz başlangıç “karışım gücü”. Ama bu değer 0 veya 1 olursa problem çıkıyo\n",
    "            # alpha = 0 demek: residual karışım tamamen kapalı → out = x\n",
    "            # alpha = 1 demek: residual karışım full açık → out = y\n",
    "            # İşte bu yüzden “tam 0” veya “tam 1” değerlerini yasaklıyoruz.\n",
    "            # alpha_init = 1 verdin → a0 = 1-eps olur. Yani: sonsuzluklardan kaçış / sayısal guardrail.\n",
    "            a0 = min(max(a0,eps),1.0-eps)\n",
    "            raw0 = torch.logit(torch.tensor(a0),eps=eps)\n",
    "            # sigmoid(z) = a ise\n",
    "            # logit(a) = z\n",
    "            # “Ben başlangıçta alpha = a0 istiyorum → o zaman alpha_raw = logit(a0) olmalı.”\n",
    "            if learnable_alpha:\n",
    "                self.alpha_raw = nn.Parameter(raw0) # nn.Parameter(raw0) → optimizer bunu günceller → alpha öğrenilir.\n",
    "            else:\n",
    "                self.register_buffer(\"alpha_raw\",raw0)\n",
    "        self.register_buffer(\"r_ema\",torch.tensor(1.0))\n",
    "\n",
    "    def alpha(self,x:torch.tensor) -> torch.Tensor:\n",
    "        # Bu fonksiyonun amacı residual karışım katsayısı olan alphayı \n",
    "        # “her durumda güvenli ve uyumlu” şekilde üretmek.\n",
    "        # Residual karışım için alphayı (0–1) aralığında tutup, \n",
    "        # YOLO/AMP’de patlamasın diye x ile aynı cihaz/tipte döndürüyor.\n",
    "        if (not self.residual) or (not hasattr(self,\"alpha_raw\")): # Residual yoksa veya alpha_raw yoksa\n",
    "            # “Alpha’ya ihtiyaç yok” demek.\n",
    "            # 1.0 döndürerek varsayılan güvenli davranış veriyor.\n",
    "            return x.new_tensor(1.0)\n",
    "        return torch.sigmoid(self.alpha_raw).to(device=x.device,dtype=x.dtype) # alpha_raw logit uzayında tutuluyor.\n",
    "        # sigmoid(alpha_raw) ile alpha’yı (0,1) aralığına kilitliyor.\n",
    "\n",
    "    @staticmethod\n",
    "    def _std_per_sample(x: torch.Tensor) -> torch.Tensor:\n",
    "        # Bu fonksiyon \"her örnek (sample) için\" standart sapmayı hesaplayıp,\n",
    "        # sonra batch içindeki örneklerin ortalamasını döndürür.\n",
    "        # Amaç: monitor/rescue mantığında \"x'in aktivasyon yayılımı (std)\" gibi bir büyüklük ölçmek.\n",
    "        # Yani: tensor ne kadar dalgalı/kontrastlı, aktivasyonlar ne kadar saçılmış?\n",
    "        # x.float():\n",
    "            # - x'in dtype'ı fp16/bf16 olabilir.\n",
    "            # - std gibi istatistiksel hesaplar düşük hassasiyette daha gürültülü/instabil olur.\n",
    "            # - O yüzden float32'ye çeviriyoruz (daha stabil numerik hesap).\n",
    "        x_float = x.float()\n",
    "        # flatten(1):\n",
    "            # - x genelde (B, C, H, W) gibi bir tensordur.\n",
    "            # - flatten(1) demek: batch boyutunu (B) koru, geri kalan her şeyi tek vektöre düzleştir.\n",
    "            # - Sonuç shape: (B, C*H*W)\n",
    "            # - Böylece her sample için tek bir uzun vektör elde ederiz.\n",
    "        x_flat = x_float.flatten(1)\n",
    "        # std(dim=1, unbiased=False):\n",
    "            # - dim=1: her sample'ın (C*H*W) vektörü üzerinde std hesapla → sonuç shape (B,)\n",
    "            # - unbiased=False (correction=0):\n",
    "            #   * \"popülasyon std\" gibi hesaplar; N-1 düzeltmesi yapmaz.\n",
    "            #   * N çok küçükse (özellikle N=1 gibi saçma uç durumlarda) unbiased=True std NaN üretebilir.\n",
    "            #   * unbiased=False bu NaN riskini ciddi azaltır ve daha stabil davranır.\n",
    "        per_sample_std = x_flat.std(dim=1, unbiased=False)  # shape: (B,)\n",
    "        # mean():\n",
    "            # - batch içindeki tüm örneklerin std değerlerini ortalar\n",
    "            # - tek skaler döner: \"batch'in ortalama std'si\"\n",
    "        return per_sample_std.mean()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_r_ema(self, r_out: torch.Tensor):\n",
    "        # Bu fonksiyon r_out’u filtreleyip (yumuşatıp) r_ema’ya yazar ki rescue mekanizması her adım kafayı yemesin.\n",
    "        # Her forward’da ölçtüğün r_out değerini zıplamasın diye yumuşatıp r_ema içine yazmak.\n",
    "        # r_out, attention’dan SONRA feature’ların ne kadar “canlı kaldığını” gösteren tek bir sayıdır.\n",
    "        r_det = r_out.detach().to(device=self.r_ema.device, dtype=self.r_ema.dtype)\n",
    "        self.r_ema.mul_(self.ema_m).add_((1.0 - self.ema_m) * r_det)\n",
    "        # mul_(x) :: “Kendini x ile çarp, yeni tensor üretme.” \n",
    "            # a = a * 0.9  ::  ama yeni a oluşturmaz, aynı a değişir\n",
    "        # add_(x) :: “Kendine x ekle, yeni tensor üretme.” \n",
    "            # a = a + b     # ama yine aynı a değişir\n",
    "     \n",
    "    ## KESİNLİKLE DİKKAT \n",
    "    # Bu fonksiyon, r_ema düşükse (feature map fazla “ölmüşse”) alphayı otomatik azaltıp bloğu\n",
    "            #  yumuşatır;\n",
    "    #  ama min_rescue_ratio sayesinde alpha’nın tamamen sıfırlanmasına izin vermez.\n",
    "    def compute_alpha_eff(self,x:torch.Tensor , alpha:torch.Tensor) -> torch.Tensor:\n",
    "        # attention fazla bastırıyorsa alpha’yı otomatik kısmak\n",
    "        # İçeride yapılan iş aslında tek mekanizma: alpha’yı, ratio diye bir güvenlik katsayısıyla çarpıp küçültmek.\n",
    "        # r_ema: Son çıktının “enerji oranı” EMA’sı. (genelde out_std / x_std)\n",
    "        # r_min: “Benim kabul ettiğim minimum enerji oranı” eşiği.\n",
    "        ratio = (self.r_ema.detach() / max(self.r_min,1e-12)).clamp(0.0,1.0)\n",
    "        # Eğer r_ema >= r_min ise :: r_ema / r_min >= 1 olur → clamp ile 1.0’a çekilir → kısma yok\n",
    "        # Eğer r_ema < r_min ise  :: oran 1’den küçük olur → clamp ile 0–1 arası kalır → alpha küçülür → blok yumuşar\n",
    "        ratio = ratio.to(device=x.device,dtype=x.dtype)\n",
    "        # AMP/FP16 vs için. alpha * ratio yaparken mismatch yemeyesin.Bunu istemiyoruz.\n",
    "        if self.rescue_mode == \"ratio_floor\":\n",
    "            ratio = ratio.clamp(self.min_rescue_ratio,1.0)\n",
    "            # ratio çok küçülürse (ör. 0.01), alpha neredeyse sıfırlanır → blok “tam kapanır”\n",
    "            # min_rescue_ratio ile “en az şu kadar açık kalsın” diyoruz. :: Yani rescue çalışsa bile alpha tamamen ölmez.\n",
    "            return alpha * ratio\n",
    "        alpha_eff = alpha * ratio\n",
    "        # Bu modda önce çarpıp sonra clamp ediyorsun.\n",
    "        return alpha_eff.clamp(self.min_rescue_ratio , 1.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        monitor_stats = None\n",
    "\n",
    "        if self.return_maps:\n",
    "            y_ca, ca_map, fusion_w = self.ca(x)\n",
    "            y = self.coord(y_ca)\n",
    "\n",
    "            if not self.residual:\n",
    "                coord_stats = self.coord.last_mask_stats()\n",
    "                return y, ca_map, fusion_w, coord_stats, None\n",
    "\n",
    "            alpha = self._alpha(x)\n",
    "            alpha_eff = alpha\n",
    "\n",
    "            if self.training and self.monitor:\n",
    "                x_std = self._std_per_sample(x)\n",
    "                y_std = self._std_per_sample(y)\n",
    "\n",
    "                out_tmp = x + alpha * (y - x)\n",
    "                out_std = self._std_per_sample(out_tmp)\n",
    "\n",
    "                r_block = (y_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "                r_out = (out_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "\n",
    "                self._update_r_ema(r_out)\n",
    "                alpha_eff = self._compute_alpha_eff(x, alpha)\n",
    "\n",
    "                monitor_stats = {\n",
    "                    \"x_std\": float(x_std.detach()),\n",
    "                    \"y_std\": float(y_std.detach()),\n",
    "                    \"out_std_pre\": float(out_std.detach()),\n",
    "                    \"r_block\": float(r_block.detach()),\n",
    "                    \"r_out_pre\": float(r_out.detach()),\n",
    "                    \"r_ema\": float(self.r_ema.detach()),\n",
    "                    \"alpha\": float(alpha.detach()),\n",
    "                    \"alpha_eff\": float(alpha_eff.detach()),\n",
    "                    \"rescue_mode\": self.rescue_mode,\n",
    "                }\n",
    "\n",
    "            out = x + alpha_eff * (y - x)\n",
    "            coord_stats = self.coord.last_mask_stats()\n",
    "            return out, ca_map, fusion_w, coord_stats, monitor_stats\n",
    "\n",
    "        y_ca, _ = self.ca(x)\n",
    "        y = self.coord(y_ca)\n",
    "\n",
    "        if not self.residual:\n",
    "            return y\n",
    "\n",
    "        alpha = self._alpha(x)\n",
    "        alpha_eff = alpha\n",
    "\n",
    "        if self.training and self.monitor:\n",
    "            x_std = self._std_per_sample(x)\n",
    "            out_tmp = x + alpha * (y - x)\n",
    "            out_std = self._std_per_sample(out_tmp)\n",
    "            r_out = (out_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "\n",
    "            self._update_r_ema(r_out)\n",
    "            alpha_eff = self._compute_alpha_eff(x, alpha)\n",
    "\n",
    "        out = x + alpha_eff * (y - x)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
