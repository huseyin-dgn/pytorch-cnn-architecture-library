{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013d0cf0",
   "metadata": {},
   "source": [
    "# CBAM Channel + Coordinate Attention — Adım 1’den Adım 2’ye Geçiş\n",
    "\n",
    "## İçindekiler\n",
    "1. Amaç ve kapsam  \n",
    "2. Adım 1: Baseline mimari  \n",
    "3. Adım 2 hedefleri  \n",
    "4. Adım 2 değişiklikleri (modül modül)  \n",
    "5. Kontrol listesi  \n",
    "6. Adım 1 kodu  \n",
    "7. Adım 2 kodu  \n",
    "8. Hızlı test  \n",
    "9. Çıktıları okuma notları  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef8516",
   "metadata": {},
   "source": [
    "## 1) Amaç ve kapsam\n",
    "\n",
    "Bu defter, **Adım 1**’deki CBAM Channel + Coordinate Attention bloğunu alıp, **Adım 2**’de YOLO eğitiminde daha stabil olacak şekilde yaptığın ekleri **başlık başlık** ve **kodun adım adım mantığıyla** açıklar.\n",
    "\n",
    "Kural: Burada amaç, blok agresifliğini kontrol etmek ve “çok bastırma” durumunda kendini güvenli tarafa çekmektir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0d487",
   "metadata": {},
   "source": [
    "## 2) Adım 1: Baseline mimari\n",
    "\n",
    "Adım 1 zinciri:\n",
    "- CA: (avg/max) → MLP → sum/softmax router → ca_map → `y = x * ca_map`\n",
    "- Coord: H/W profilleri → dw + dilated dw → karıştır → bottleneck → attn_h/attn_w → scale → `out = x * scale`\n",
    "- Birleşim: `out = x + alpha*(y - x)` (residual açıksa)\n",
    "\n",
    "Bu versiyon çalışır ama YOLO’da bazı durumlarda “çok bastırma” (std düşmesi) riski taşır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e546c",
   "metadata": {},
   "source": [
    "## 3) Adım 2 hedefleri\n",
    "\n",
    "Adım 2’de hedef:\n",
    "- CA çarpımını yumuşat (beta_ca)\n",
    "- Learnable T uçmasın (T clamp)\n",
    "- Router softmax erken kilitlenmesin (Tr)\n",
    "- Coord ölçeği uçmasın (scale clamp)\n",
    "- Coord head’ler agresif başlamasın (small std init + bias=0)\n",
    "- Eğitimde bastırmayı ölç ve gerekirse residual karışımı otomatik yumuşat (EMA + alpha_eff)\n",
    "- Monitor hesapları sadece training’de ve monitor=True iken çalışsın\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df592b0",
   "metadata": {},
   "source": [
    "## 4) Adım 2 değişiklikleri (modül modül)\n",
    "\n",
    "### 4A) ChannelAttentionFusionT (CA)\n",
    "**Yeni parametreler:**\n",
    "- `t_min, t_max`: learnable temperature güvenliği\n",
    "- `router_temperature (Tr)`: router softmax yumuşatma\n",
    "- `beta_ca`: CA’yı yumuşak ölçek şeklinde uygulama\n",
    "\n",
    "**Ne değişti?**\n",
    "- Adım 1: `y = x * ca`\n",
    "- Adım 2: `scale_ca = 1 + beta_ca*(ca-1)` ve `y = x * scale_ca`\n",
    "\n",
    "Ayrıca router tarafı: `softmax(logits / Tr)`.\n",
    "\n",
    "### 4B) CoordinateAttPlus (Coord)\n",
    "**Yeni parametreler:**\n",
    "- `scale_min, scale_max`: `scale` clamp\n",
    "- `head_init_std`: attention head init’i yumuşatma\n",
    "\n",
    "**Ne değişti?**\n",
    "- scale hesaplandıktan sonra clamp var.\n",
    "- head weight küçük std ile init, bias=0.\n",
    "\n",
    "### 4C) CBAMChannelPlusCoord (Birleşim)\n",
    "**Yeni mekanizma: monitor + rescue**\n",
    "- `r_out = std(out_tmp) / std(x)` ölçülür.\n",
    "- `r_ema` ile EMA tutulur.\n",
    "- `alpha_eff` rescue_mode’a göre hesaplanır.\n",
    "\n",
    "**rescue_mode:**\n",
    "- `ratio_floor`: ratio tabandan kesilir, `alpha_eff = alpha * ratio`\n",
    "- `alpha_floor`: `alpha_eff` doğrudan alt sınıra clamp (daha agresif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd98607",
   "metadata": {},
   "source": [
    "## 5) Kontrol listesi\n",
    "\n",
    "- [ ] CA: `get_T(x)` içinde `clamp(t_min, t_max)`\n",
    "- [ ] Router: `softmax(logits / Tr)`\n",
    "- [ ] CA: `y = x * (1 + beta_ca*(ca-1))`\n",
    "- [ ] Coord: `scale.clamp(scale_min, scale_max)`\n",
    "- [ ] Coord head init: küçük std + bias=0\n",
    "- [ ] Monitor: sadece `training and monitor=True`\n",
    "- [ ] EMA: `r_ema` buffer + update\n",
    "- [ ] Rescue: `alpha_eff` hesaplama ve `rescue_mode`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae8770",
   "metadata": {},
   "source": [
    "## 6) Adım 1 kodu (baseline)\n",
    "Bu hücre Adım 1’in referans halidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454a1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def softplus_inverse(y: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return torch.log(torch.clamp(torch.exp(y) - 1.0, min=eps))\n",
    "\n",
    "\n",
    "def _get_gate(gate: str):\n",
    "    g = gate.lower()\n",
    "    if g == \"sigmoid\":\n",
    "        return torch.sigmoid\n",
    "    if g == \"hardsigmoid\":\n",
    "        return F.hardsigmoid\n",
    "    raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "\n",
    "def _get_act(act: str):\n",
    "    a = act.lower()\n",
    "    if a == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    if a == \"silu\":\n",
    "        return nn.SiLU(inplace=True)\n",
    "    raise ValueError(\"act 'relu' veya 'silu' olmalı.\")\n",
    "\n",
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",\n",
    "        gate: str = \"sigmoid\",\n",
    "        temperature: float = 0.9,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        act: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "        fusion_router_hidden: int = 16,\n",
    "        return_fusion_weights: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalı.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalı.\")\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalı.\")\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if fusion == \"softmax\" and fusion_router_hidden < 1:\n",
    "            raise ValueError(\"fusion_router_hidden >= 1 olmalı.\")\n",
    "\n",
    "        self.eps = float(eps)\n",
    "        self.fusion = fusion\n",
    "        self.return_fusion_weights = bool(return_fusion_weights)\n",
    "        self.gate_fn = _get_gate(gate)\n",
    "\n",
    "        hidden = max(int(min_hidden), int(channels) // int(reduction))\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=bias)\n",
    "        self.act = _get_act(act)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        if self.fusion == \"softmax\":\n",
    "            self.fusion_router = nn.Sequential(\n",
    "                nn.Conv2d(2 * channels, fusion_router_hidden, kernel_size=1, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(fusion_router_hidden, 2, kernel_size=1, bias=True),\n",
    "            )\n",
    "            last = self.fusion_router[-1]\n",
    "            nn.init.zeros_(last.weight)\n",
    "            nn.init.zeros_(last.bias)\n",
    "        else:\n",
    "            self.fusion_router = None\n",
    "\n",
    "        self.learnable_temperature = bool(learnable_temperature)\n",
    "        if self.learnable_temperature:\n",
    "            t0 = torch.tensor(float(temperature))\n",
    "            t_inv = softplus_inverse(t0, eps=self.eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def get_T(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_s = self.avg_pool(x)\n",
    "        max_s = self.max_pool(x)\n",
    "\n",
    "        a = self.mlp(avg_s)\n",
    "        m = self.mlp(max_s)\n",
    "\n",
    "        fusion_w = None\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            s_cat = torch.cat([avg_s, max_s], dim=1)\n",
    "            logits = self.fusion_router(s_cat).flatten(1)  # (B, 2)\n",
    "            fusion_w = torch.softmax(logits, dim=1)\n",
    "            w0 = fusion_w[:, 0].view(-1, 1, 1, 1)\n",
    "            w1 = fusion_w[:, 1].view(-1, 1, 1, 1)\n",
    "            z = w0 * a + w1 * m\n",
    "\n",
    "        T = self.get_T().to(device=x.device, dtype=x.dtype)\n",
    "        ca = self.gate_fn(z / T)\n",
    "        y = x * ca\n",
    "\n",
    "        if self.return_fusion_weights and (fusion_w is not None):\n",
    "            return y, ca, fusion_w\n",
    "        return y, ca\n",
    "\n",
    "\n",
    "class HSwish(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0\n",
    "\n",
    "\n",
    "def make_norm(norm: str, ch: int):\n",
    "    norm = norm.lower()\n",
    "    if norm == \"bn\":\n",
    "        return nn.BatchNorm2d(ch)\n",
    "    if norm == \"gn\":\n",
    "        g = min(32, ch)\n",
    "        while ch % g != 0 and g > 2:\n",
    "            g //= 2\n",
    "        if ch % g != 0:\n",
    "            g = 2 if (ch % 2 == 0) else 1\n",
    "        return nn.GroupNorm(g, ch)\n",
    "    if norm == \"none\":\n",
    "        return nn.Identity()\n",
    "    raise ValueError(\"norm 'none', 'bn', 'gn' dışında olamaz.\")\n",
    "\n",
    "\n",
    "class CoordinateAttPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        reduction: int = 32,\n",
    "        min_mid_channels: int = 8,\n",
    "        act: str = \"hswish\",\n",
    "        init_alpha: float = 0.7,\n",
    "        learnable_alpha: bool = True,\n",
    "        beta: float = 0.35,\n",
    "        dilation: int = 2,\n",
    "        norm: str = \"gn\",\n",
    "        use_spatial_gate: bool = False,\n",
    "        spatial_gate_beta: float = 0.35,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if in_channels < 1:\n",
    "            raise ValueError(\"in_channels >= 1 olmalı.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalı.\")\n",
    "        if dilation < 1:\n",
    "            raise ValueError(\"dilation >= 1 olmalı.\")\n",
    "\n",
    "        mid_floor = max(8, min(32, int(in_channels) // 4))\n",
    "        mid = max(int(min_mid_channels), int(in_channels) // int(reduction))\n",
    "        mid = max(mid, int(mid_floor))\n",
    "\n",
    "        act_l = act.lower()\n",
    "        if act_l == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        elif act_l == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act_l == \"silu\":\n",
    "            self.act = nn.SiLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act 'hswish', 'relu', 'silu' olmalı.\")\n",
    "\n",
    "        self.shared_bottleneck_proj = nn.Conv2d(in_channels, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_norm = make_norm(norm, mid)\n",
    "        self.shared_bottleneck_refine = nn.Conv2d(mid, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_refine_norm = make_norm(norm, mid)\n",
    "\n",
    "        self.h_local_dw = nn.Conv2d(in_channels, in_channels, kernel_size=(3, 1), padding=(1, 0), groups=in_channels, bias=False)\n",
    "        self.w_local_dw = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 3), padding=(0, 1), groups=in_channels, bias=False)\n",
    "\n",
    "        d = int(dilation)\n",
    "        self.h_dilated_dw = nn.Conv2d(in_channels, in_channels, kernel_size=(3, 1), padding=(d, 0), dilation=(d, 1), groups=in_channels, bias=False)\n",
    "        self.w_dilated_dw = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 3), padding=(0, d), dilation=(1, d), groups=in_channels, bias=False)\n",
    "\n",
    "        self.h_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        self.w_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self.h_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        self.w_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "\n",
    "        self.beta = float(beta)\n",
    "\n",
    "        eps = 1e-6\n",
    "        a0 = float(init_alpha)\n",
    "        a0 = min(max(a0, eps), 1.0 - eps)\n",
    "        raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha_h_raw = nn.Parameter(raw0.clone())\n",
    "            self.alpha_w_raw = nn.Parameter(raw0.clone())\n",
    "        else:\n",
    "            self.register_buffer(\"alpha_h_raw\", raw0.clone())\n",
    "            self.register_buffer(\"alpha_w_raw\", raw0.clone())\n",
    "\n",
    "        self.use_spatial_gate = bool(use_spatial_gate)\n",
    "        self.spatial_gate_beta = float(spatial_gate_beta)\n",
    "        if self.use_spatial_gate:\n",
    "            self.spatial_gate_dw = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False)\n",
    "            self.spatial_gate_pw = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self._last_ah = None\n",
    "        self._last_aw = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        _, _, H, W = x.shape\n",
    "\n",
    "        h_profile = 0.5 * (x.mean(dim=3, keepdim=True) + x.amax(dim=3, keepdim=True))\n",
    "        w_profile = 0.5 * (x.mean(dim=2, keepdim=True) + x.amax(dim=2, keepdim=True))\n",
    "\n",
    "        h_ms = self.h_channel_mixer(self.h_local_dw(h_profile) + self.h_dilated_dw(h_profile))\n",
    "        w_ms = self.w_channel_mixer(self.w_local_dw(w_profile) + self.w_dilated_dw(w_profile))\n",
    "        w_ms = w_ms.permute(0, 1, 3, 2)\n",
    "\n",
    "        hw = torch.cat([h_ms, w_ms], dim=2)\n",
    "\n",
    "        mid = self.act(self.shared_bottleneck_norm(self.shared_bottleneck_proj(hw)))\n",
    "        mid = self.act(self.shared_bottleneck_refine_norm(self.shared_bottleneck_refine(mid)))\n",
    "\n",
    "        mid_h, mid_w = torch.split(mid, [H, W], dim=2)\n",
    "        mid_w = mid_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        attn_h = F.hardsigmoid(self.h_attention_head(mid_h), inplace=False)\n",
    "        attn_w = F.hardsigmoid(self.w_attention_head(mid_w), inplace=False)\n",
    "\n",
    "        self._last_ah = attn_h.detach()\n",
    "        self._last_aw = attn_w.detach()\n",
    "\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw).to(device=x.device, dtype=x.dtype)\n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        scale_h = (1.0 - alpha_h) + alpha_h * attn_h\n",
    "        scale_w = (1.0 - alpha_w) + alpha_w * attn_w\n",
    "\n",
    "        scale = scale_h * scale_w\n",
    "        scale = 1.0 + self.beta * (scale - 1.0)\n",
    "\n",
    "        out = x * scale\n",
    "\n",
    "        if self.use_spatial_gate:\n",
    "            sg = self.spatial_gate_pw(self.spatial_gate_dw(x))\n",
    "            sg = F.hardsigmoid(sg, inplace=False)\n",
    "            sg = 1.0 + self.spatial_gate_beta * (sg - 1.0)\n",
    "            out = out * sg\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def last_mask_stats(self):\n",
    "        if (self._last_ah is None) or (self._last_aw is None):\n",
    "            return None\n",
    "        ah = self._last_ah\n",
    "        aw = self._last_aw\n",
    "        return {\n",
    "            \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "            \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "        }\n",
    "\n",
    "\n",
    "class CBAMChannelPlusCoord(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        ca_reduction: int = 16,\n",
    "        ca_min_hidden: int = 4,\n",
    "        ca_fusion: str = \"softmax\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        ca_temperature: float = 0.9,\n",
    "        ca_act: str = \"relu\",\n",
    "        ca_fusion_router_hidden: int = 16,\n",
    "        learnable_temperature: bool = False,\n",
    "        coord_reduction: int = 32,\n",
    "        coord_min_mid: int = 8,\n",
    "        coord_act: str = \"hswish\",\n",
    "        coord_init_alpha: float = 0.7,\n",
    "        coord_learnable_alpha: bool = True,\n",
    "        coord_beta: float = 0.35,\n",
    "        coord_dilation: int = 2,\n",
    "        coord_norm: str = \"gn\",\n",
    "        coord_use_spatial_gate: bool = False,\n",
    "        coord_spatial_gate_beta: float = 0.35,\n",
    "        residual: bool = True,\n",
    "        alpha_init: float = 0.75,\n",
    "        learnable_alpha: bool = False,\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalı.\")\n",
    "\n",
    "        self.return_maps = bool(return_maps)\n",
    "        self.residual = bool(residual)\n",
    "\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=ca_reduction,\n",
    "            min_hidden=ca_min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            eps=1e-6,\n",
    "            act=ca_act,\n",
    "            bias=True,\n",
    "            fusion_router_hidden=ca_fusion_router_hidden,\n",
    "            return_fusion_weights=self.return_maps,\n",
    "        )\n",
    "\n",
    "        self.coord = CoordinateAttPlus(\n",
    "            in_channels=channels,\n",
    "            reduction=coord_reduction,\n",
    "            min_mid_channels=coord_min_mid,\n",
    "            act=coord_act,\n",
    "            init_alpha=coord_init_alpha,\n",
    "            learnable_alpha=coord_learnable_alpha,\n",
    "            beta=coord_beta,\n",
    "            dilation=coord_dilation,\n",
    "            norm=coord_norm,\n",
    "            use_spatial_gate=coord_use_spatial_gate,\n",
    "            spatial_gate_beta=coord_spatial_gate_beta,\n",
    "        )\n",
    "\n",
    "        if self.residual:\n",
    "            eps = 1e-6\n",
    "            a0 = float(alpha_init)\n",
    "            a0 = min(max(a0, eps), 1.0 - eps)\n",
    "            raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "            if learnable_alpha:\n",
    "                self.alpha_raw = nn.Parameter(raw0)\n",
    "            else:\n",
    "                self.register_buffer(\"alpha_raw\", raw0)\n",
    "\n",
    "    def _alpha(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not hasattr(self, \"alpha_raw\"):\n",
    "            return x.new_tensor(1.0)\n",
    "        return torch.sigmoid(self.alpha_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.return_maps:\n",
    "            y, ca_map, fusion_w = self.ca(x)\n",
    "            y = self.coord(y)\n",
    "            out = x + self._alpha(x) * (y - x) if self.residual else y\n",
    "            coord_stats = self.coord.last_mask_stats()\n",
    "            return out, ca_map, fusion_w, coord_stats\n",
    "\n",
    "        y, _ = self.ca(x)\n",
    "        y = self.coord(y)\n",
    "        out = x + self._alpha(x) * (y - x) if self.residual else y\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f8621",
   "metadata": {},
   "source": [
    "## 7) Adım 2 kodu (eklemeler dahil)\n",
    "Bu hücre Adım 2’nin tam halidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81cc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def softplus_inverse(y: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    return torch.log(torch.clamp(torch.exp(y) - 1.0, min=eps))\n",
    "\n",
    "\n",
    "def _get_gate(gate: str):\n",
    "    g = gate.lower()\n",
    "    if g == \"sigmoid\":\n",
    "        return torch.sigmoid\n",
    "    if g == \"hardsigmoid\":\n",
    "        return F.hardsigmoid\n",
    "    raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "\n",
    "def _get_act(act: str):\n",
    "    a = act.lower()\n",
    "    if a == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    if a == \"silu\":\n",
    "        return nn.SiLU(inplace=True)\n",
    "    raise ValueError(\"act 'relu' veya 'silu' olmalı.\")\n",
    "\n",
    "\n",
    "class ChannelAttentionFusionT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        fusion: str = \"softmax\",\n",
    "        gate: str = \"sigmoid\",\n",
    "        temperature: float = 0.9,\n",
    "        learnable_temperature: bool = False,\n",
    "        eps: float = 1e-6,\n",
    "        act: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "        fusion_router_hidden: int = 16,\n",
    "        return_fusion_weights: bool = False,\n",
    "        t_min: float = 0.5,\n",
    "        t_max: float = 3.0,\n",
    "        router_temperature: float = 1.5,\n",
    "        beta_ca: float = 0.35,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalı.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalı.\")\n",
    "        if fusion not in (\"sum\", \"softmax\"):\n",
    "            raise ValueError(\"fusion 'sum' veya 'softmax' olmalı.\")\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature pozitif olmalı.\")\n",
    "        if fusion == \"softmax\" and fusion_router_hidden < 1:\n",
    "            raise ValueError(\"fusion_router_hidden >= 1 olmalı.\")\n",
    "        if t_min <= 0 or t_max <= 0 or t_min > t_max:\n",
    "            raise ValueError(\"T clamp aralığı hatalı.\")\n",
    "        if router_temperature <= 0:\n",
    "            raise ValueError(\"router_temperature pozitif olmalı.\")\n",
    "        if beta_ca < 0:\n",
    "            raise ValueError(\"beta_ca >= 0 olmalı.\")\n",
    "\n",
    "        self.eps = float(eps)\n",
    "        self.fusion = fusion\n",
    "        self.return_fusion_weights = bool(return_fusion_weights)\n",
    "        self.gate_fn = _get_gate(gate)\n",
    "\n",
    "        self.t_min = float(t_min)\n",
    "        self.t_max = float(t_max)\n",
    "        self.Tr = float(router_temperature)\n",
    "        self.beta_ca = float(beta_ca)\n",
    "\n",
    "        hidden = max(int(min_hidden), int(channels) // int(reduction))\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=bias)\n",
    "        self.act = _get_act(act)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        if self.fusion == \"softmax\":\n",
    "            self.fusion_router = nn.Sequential(\n",
    "                nn.Conv2d(2 * channels, fusion_router_hidden, kernel_size=1, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(fusion_router_hidden, 2, kernel_size=1, bias=True),\n",
    "            )\n",
    "            last = self.fusion_router[-1]\n",
    "            nn.init.zeros_(last.weight)\n",
    "            nn.init.zeros_(last.bias)\n",
    "        else:\n",
    "            self.fusion_router = None\n",
    "\n",
    "        self.learnable_temperature = bool(learnable_temperature)\n",
    "\n",
    "        if self.learnable_temperature:\n",
    "            # İsteğe bağlı strict kontrol (istersen aç)\n",
    "            # if not (self.t_min <= float(temperature) <= self.t_max):\n",
    "            #     raise ValueError(f\"temperature [{self.t_min}, {self.t_max}] aralığında olmalı (learnable_temperature=True).\")\n",
    "            # Stabil başlangıç: temperature'ı clamp edip t_raw'ı onunla başlat.\n",
    "            t0 = float(temperature)\n",
    "            # Sınırların tam üstüne yapışmayı engellemek için küçük güvenlik payı\n",
    "            lo = self.t_min + self.eps\n",
    "            hi = self.t_max - self.eps\n",
    "            # Eğer kullanıcı ters aralık verdiyse zaten yukarıda kontrol var; yine de defansif:\n",
    "            if lo >= hi:\n",
    "                lo = self.t_min\n",
    "                hi = self.t_max\n",
    "            t0 = min(max(t0, lo), hi)\n",
    "            t_inv = softplus_inverse(torch.tensor(t0), eps=self.eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            # Fixed temperature için de clamp'li başlatmak istersen (opsiyonel):\n",
    "            # t0 = float(temperature)\n",
    "            # t0 = min(max(t0, self.t_min), self.t_max)\n",
    "            # self.register_buffer(\"T\", torch.tensor(t0))\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "    def get_T(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            T = F.softplus(self.t_raw) + self.eps\n",
    "        else:\n",
    "            T = self.T\n",
    "        T = T.to(device=x.device, dtype=x.dtype)\n",
    "        return T.clamp(self.t_min, self.t_max)\n",
    "\n",
    "    def mlp(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(s)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg_s = self.avg_pool(x)\n",
    "        max_s = self.max_pool(x)\n",
    "\n",
    "        a = self.mlp(avg_s)\n",
    "        m = self.mlp(max_s)\n",
    "\n",
    "        fusion_w = None\n",
    "        if self.fusion == \"sum\":\n",
    "            z = a + m\n",
    "        else:\n",
    "            s_cat = torch.cat([avg_s, max_s], dim=1)\n",
    "            logits = self.fusion_router(s_cat).flatten(1)  # (B,2)\n",
    "            fusion_w = torch.softmax(logits / self.Tr, dim=1)\n",
    "            w0 = fusion_w[:, 0].view(-1, 1, 1, 1)\n",
    "            w1 = fusion_w[:, 1].view(-1, 1, 1, 1)\n",
    "            z = w0 * a + w1 * m\n",
    "\n",
    "        T = self.get_T(x)\n",
    "        ca = self.gate_fn(z / T)\n",
    "\n",
    "        scale_ca = 1.0 + self.beta_ca * (ca - 1.0)\n",
    "        y = x * scale_ca\n",
    "\n",
    "        if self.return_fusion_weights:\n",
    "            return y, ca, fusion_w\n",
    "        return y, ca\n",
    "\n",
    "\n",
    "class HSwish(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0\n",
    "\n",
    "\n",
    "def make_norm(norm: str, ch: int):\n",
    "    norm = norm.lower()\n",
    "    if norm == \"bn\":\n",
    "        return nn.BatchNorm2d(ch)\n",
    "    if norm == \"gn\":\n",
    "        g = min(32, ch)\n",
    "        while ch % g != 0 and g > 2:\n",
    "            g //= 2\n",
    "        if ch % g != 0:\n",
    "            g = 2 if (ch % 2 == 0) else 1\n",
    "        return nn.GroupNorm(g, ch)\n",
    "    if norm == \"none\":\n",
    "        return nn.Identity()\n",
    "    raise ValueError(\"norm 'none', 'bn', 'gn' dışında olamaz.\")\n",
    "\n",
    "\n",
    "class CoordinateAttPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        reduction: int = 32,\n",
    "        min_mid_channels: int = 8,\n",
    "        act: str = \"hswish\",\n",
    "        init_alpha: float = 0.7,\n",
    "        learnable_alpha: bool = True,\n",
    "        beta: float = 0.35,\n",
    "        dilation: int = 2,\n",
    "        norm: str = \"gn\",\n",
    "        use_spatial_gate: bool = False,\n",
    "        spatial_gate_beta: float = 0.35,\n",
    "        scale_min: float = 0.6,\n",
    "        scale_max: float = 1.6,\n",
    "        head_init_std: float = 0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if in_channels < 1:\n",
    "            raise ValueError(\"in_channels >= 1 olmalı.\")\n",
    "        if reduction < 1:\n",
    "            raise ValueError(\"reduction >= 1 olmalı.\")\n",
    "        if dilation < 1:\n",
    "            raise ValueError(\"dilation >= 1 olmalı.\")\n",
    "        if scale_min <= 0 or scale_max <= 0 or scale_min > scale_max:\n",
    "            raise ValueError(\"scale clamp aralığı hatalı.\")\n",
    "        if head_init_std <= 0:\n",
    "            raise ValueError(\"head_init_std pozitif olmalı.\")\n",
    "\n",
    "        self.beta = float(beta)\n",
    "        self.scale_min = float(scale_min)\n",
    "        self.scale_max = float(scale_max)\n",
    "\n",
    "        mid_floor = max(8, min(32, int(in_channels) // 4))\n",
    "        mid = max(int(min_mid_channels), int(in_channels) // int(reduction))\n",
    "        mid = max(mid, int(mid_floor))\n",
    "\n",
    "        act_l = act.lower()\n",
    "        if act_l == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        elif act_l == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act_l == \"silu\":\n",
    "            self.act = nn.SiLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act 'hswish', 'relu', 'silu' olmalı.\")\n",
    "\n",
    "        self.shared_bottleneck_proj = nn.Conv2d(in_channels, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_norm = make_norm(norm, mid)\n",
    "        self.shared_bottleneck_refine = nn.Conv2d(mid, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_refine_norm = make_norm(norm, mid)\n",
    "\n",
    "        self.h_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(3, 1), padding=(1, 0),\n",
    "            groups=in_channels, bias=False\n",
    "        )\n",
    "        self.w_local_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(1, 3), padding=(0, 1),\n",
    "            groups=in_channels, bias=False\n",
    "        )\n",
    "\n",
    "        d = int(dilation)\n",
    "        self.h_dilated_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(3, 1), padding=(d, 0),\n",
    "            dilation=(d, 1), groups=in_channels, bias=False\n",
    "        )\n",
    "        self.w_dilated_dw = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=(1, 3), padding=(0, d),\n",
    "            dilation=(1, d), groups=in_channels, bias=False\n",
    "        )\n",
    "\n",
    "        self.h_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        self.w_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self.h_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        self.w_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "\n",
    "        nn.init.normal_(self.h_attention_head.weight, mean=0.0, std=float(head_init_std))\n",
    "        nn.init.normal_(self.w_attention_head.weight, mean=0.0, std=float(head_init_std))\n",
    "        if self.h_attention_head.bias is not None:\n",
    "            nn.init.zeros_(self.h_attention_head.bias)\n",
    "        if self.w_attention_head.bias is not None:\n",
    "            nn.init.zeros_(self.w_attention_head.bias)\n",
    "\n",
    "        eps = 1e-6\n",
    "        a0 = float(init_alpha)\n",
    "        a0 = min(max(a0, eps), 1.0 - eps)\n",
    "        raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha_h_raw = nn.Parameter(raw0.clone())\n",
    "            self.alpha_w_raw = nn.Parameter(raw0.clone())\n",
    "        else:\n",
    "            self.register_buffer(\"alpha_h_raw\", raw0.clone())\n",
    "            self.register_buffer(\"alpha_w_raw\", raw0.clone())\n",
    "\n",
    "        self.use_spatial_gate = bool(use_spatial_gate)\n",
    "        self.spatial_gate_beta = float(spatial_gate_beta)\n",
    "        if self.use_spatial_gate:\n",
    "            self.spatial_gate_dw = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False)\n",
    "            self.spatial_gate_pw = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self._last_ah = None\n",
    "        self._last_aw = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        _, _, H, W = x.shape\n",
    "\n",
    "        h_profile = 0.5 * (x.mean(dim=3, keepdim=True) + x.amax(dim=3, keepdim=True))\n",
    "        w_profile = 0.5 * (x.mean(dim=2, keepdim=True) + x.amax(dim=2, keepdim=True))\n",
    "\n",
    "        h_ms = self.h_channel_mixer(self.h_local_dw(h_profile) + self.h_dilated_dw(h_profile))\n",
    "        w_ms = self.w_channel_mixer(self.w_local_dw(w_profile) + self.w_dilated_dw(w_profile))\n",
    "        w_ms = w_ms.permute(0, 1, 3, 2)\n",
    "\n",
    "        hw = torch.cat([h_ms, w_ms], dim=2)\n",
    "\n",
    "        mid = self.act(self.shared_bottleneck_norm(self.shared_bottleneck_proj(hw)))\n",
    "        mid = self.act(self.shared_bottleneck_refine_norm(self.shared_bottleneck_refine(mid)))\n",
    "\n",
    "        mid_h, mid_w = torch.split(mid, [H, W], dim=2)\n",
    "        mid_w = mid_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        attn_h = F.hardsigmoid(self.h_attention_head(mid_h), inplace=False)\n",
    "        attn_w = F.hardsigmoid(self.w_attention_head(mid_w), inplace=False)\n",
    "\n",
    "        self._last_ah = attn_h.detach()\n",
    "        self._last_aw = attn_w.detach()\n",
    "\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw).to(device=x.device, dtype=x.dtype)\n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        scale_h = (1.0 - alpha_h) + alpha_h * attn_h\n",
    "        scale_w = (1.0 - alpha_w) + alpha_w * attn_w\n",
    "\n",
    "        scale = scale_h * scale_w\n",
    "        scale = 1.0 + self.beta * (scale - 1.0)\n",
    "        scale = scale.clamp(self.scale_min, self.scale_max)\n",
    "\n",
    "        out = x * scale\n",
    "\n",
    "        if self.use_spatial_gate:\n",
    "            sg = self.spatial_gate_pw(self.spatial_gate_dw(x))\n",
    "            sg = F.hardsigmoid(sg, inplace=False)\n",
    "            sg = 1.0 + self.spatial_gate_beta * (sg - 1.0)\n",
    "            out = out * sg\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def last_mask_stats(self):\n",
    "        if (self._last_ah is None) or (self._last_aw is None):\n",
    "            return None\n",
    "        ah = self._last_ah\n",
    "        aw = self._last_aw\n",
    "        return {\n",
    "            \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "            \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "        }\n",
    "\n",
    "\n",
    "class CBAMChannelPlusCoord(nn.Module):\n",
    "    \"\"\"\n",
    "    rescue_mode:\n",
    "      - \"ratio_floor\": ratio'yu [min_rescue, 1] aralığında tutar. alpha_eff = alpha * ratio\n",
    "      - \"alpha_floor\": alpha_eff'i direkt [alpha_eff_min, 1] aralığına clamp eder (daha agresif)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        ca_reduction: int = 16,\n",
    "        ca_min_hidden: int = 4,\n",
    "        ca_fusion: str = \"softmax\",\n",
    "        ca_gate: str = \"sigmoid\",\n",
    "        ca_temperature: float = 0.9,\n",
    "        ca_act: str = \"relu\",\n",
    "        ca_fusion_router_hidden: int = 16,\n",
    "        learnable_temperature: bool = False,\n",
    "        ca_t_min: float = 0.5,\n",
    "        ca_t_max: float = 3.0,\n",
    "        ca_router_temperature: float = 1.5,\n",
    "        beta_ca: float = 0.35,\n",
    "        coord_reduction: int = 32,\n",
    "        coord_min_mid: int = 8,\n",
    "        coord_act: str = \"hswish\",\n",
    "        coord_init_alpha: float = 0.7,\n",
    "        coord_learnable_alpha: bool = True,\n",
    "        coord_beta: float = 0.35,\n",
    "        coord_dilation: int = 2,\n",
    "        coord_norm: str = \"gn\",\n",
    "        coord_use_spatial_gate: bool = False,\n",
    "        coord_spatial_gate_beta: float = 0.35,\n",
    "        coord_scale_min: float = 0.6,\n",
    "        coord_scale_max: float = 1.6,\n",
    "        coord_head_init_std: float = 0.01,\n",
    "        residual: bool = True,\n",
    "        alpha_init: float = 0.75,\n",
    "        learnable_alpha: bool = False,\n",
    "        monitor: bool = False,\n",
    "        r_min: float = 0.45,\n",
    "        ema_momentum: float = 0.95,\n",
    "        min_rescue_ratio: float = 0.2,\n",
    "        alpha_eff_min: float = 0.2,\n",
    "        rescue_mode: str = \"ratio_floor\",\n",
    "        return_maps: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if channels < 1:\n",
    "            raise ValueError(\"channels >= 1 olmalı.\")\n",
    "        if not (0.0 < ema_momentum < 1.0):\n",
    "            raise ValueError(\"ema_momentum (0,1) aralığında olmalı.\")\n",
    "        if r_min <= 0:\n",
    "            raise ValueError(\"r_min pozitif olmalı.\")\n",
    "        if not (0.0 <= min_rescue_ratio <= 1.0):\n",
    "            raise ValueError(\"min_rescue_ratio [0,1] aralığında olmalı.\")\n",
    "        if not (0.0 <= alpha_eff_min <= 1.0):\n",
    "            raise ValueError(\"alpha_eff_min [0,1] aralığında olmalı.\")\n",
    "        if rescue_mode not in (\"ratio_floor\", \"alpha_floor\"):\n",
    "            raise ValueError(\"rescue_mode 'ratio_floor' veya 'alpha_floor' olmalı.\")\n",
    "\n",
    "        self.return_maps = bool(return_maps)\n",
    "        self.residual = bool(residual)\n",
    "\n",
    "        self.monitor = bool(monitor)\n",
    "        self.r_min = float(r_min)\n",
    "        self.ema_m = float(ema_momentum)\n",
    "        self.min_rescue_ratio = float(min_rescue_ratio)\n",
    "        self.alpha_eff_min = float(alpha_eff_min)\n",
    "        self.rescue_mode = str(rescue_mode)\n",
    "\n",
    "        self.ca = ChannelAttentionFusionT(\n",
    "            channels=channels,\n",
    "            reduction=ca_reduction,\n",
    "            min_hidden=ca_min_hidden,\n",
    "            fusion=ca_fusion,\n",
    "            gate=ca_gate,\n",
    "            temperature=ca_temperature,\n",
    "            learnable_temperature=learnable_temperature,\n",
    "            eps=1e-6,\n",
    "            act=ca_act,\n",
    "            bias=True,\n",
    "            fusion_router_hidden=ca_fusion_router_hidden,\n",
    "            return_fusion_weights=self.return_maps,\n",
    "            t_min=ca_t_min,\n",
    "            t_max=ca_t_max,\n",
    "            router_temperature=ca_router_temperature,\n",
    "            beta_ca=beta_ca,\n",
    "        )\n",
    "\n",
    "        self.coord = CoordinateAttPlus(\n",
    "            in_channels=channels,\n",
    "            reduction=coord_reduction,\n",
    "            min_mid_channels=coord_min_mid,\n",
    "            act=coord_act,\n",
    "            init_alpha=coord_init_alpha,\n",
    "            learnable_alpha=coord_learnable_alpha,\n",
    "            beta=coord_beta,\n",
    "            dilation=coord_dilation,\n",
    "            norm=coord_norm,\n",
    "            use_spatial_gate=coord_use_spatial_gate,\n",
    "            spatial_gate_beta=coord_spatial_gate_beta,\n",
    "            scale_min=coord_scale_min,\n",
    "            scale_max=coord_scale_max,\n",
    "            head_init_std=coord_head_init_std,\n",
    "        )\n",
    "\n",
    "        if self.residual:\n",
    "            eps = 1e-6\n",
    "            a0 = float(alpha_init)\n",
    "            a0 = min(max(a0, eps), 1.0 - eps)\n",
    "            raw0 = torch.logit(torch.tensor(a0), eps=eps)\n",
    "            if learnable_alpha:\n",
    "                self.alpha_raw = nn.Parameter(raw0)\n",
    "            else:\n",
    "                self.register_buffer(\"alpha_raw\", raw0)\n",
    "\n",
    "        self.register_buffer(\"r_ema\", torch.tensor(1.0))\n",
    "\n",
    "    def _alpha(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if (not self.residual) or (not hasattr(self, \"alpha_raw\")):\n",
    "            return x.new_tensor(1.0)\n",
    "        return torch.sigmoid(self.alpha_raw).to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def _std_per_sample(x: torch.Tensor) -> torch.Tensor:\n",
    "        # Daha güvenli: unbiased=False (correction=0) ile uç durumlarda NaN riskini azaltır\n",
    "        return x.float().flatten(1).std(dim=1, unbiased=False).mean()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_r_ema(self, r_out: torch.Tensor):\n",
    "        r_det = r_out.detach().to(device=self.r_ema.device, dtype=self.r_ema.dtype)\n",
    "        self.r_ema.mul_(self.ema_m).add_((1.0 - self.ema_m) * r_det)\n",
    "\n",
    "    def _compute_alpha_eff(self, x: torch.Tensor, alpha: torch.Tensor) -> torch.Tensor:\n",
    "        ratio = (self.r_ema.detach() / max(self.r_min, 1e-12)).clamp(0.0, 1.0)\n",
    "        ratio = ratio.to(device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if self.rescue_mode == \"ratio_floor\":\n",
    "            ratio = ratio.clamp(self.min_rescue_ratio, 1.0)\n",
    "            return alpha * ratio\n",
    "\n",
    "        alpha_eff = alpha * ratio\n",
    "        return alpha_eff.clamp(self.alpha_eff_min, 1.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        monitor_stats = None\n",
    "\n",
    "        if self.return_maps:\n",
    "            y_ca, ca_map, fusion_w = self.ca(x)\n",
    "            y = self.coord(y_ca)\n",
    "\n",
    "            if not self.residual:\n",
    "                coord_stats = self.coord.last_mask_stats()\n",
    "                return y, ca_map, fusion_w, coord_stats, None\n",
    "\n",
    "            alpha = self._alpha(x)\n",
    "            alpha_eff = alpha\n",
    "\n",
    "            if self.training and self.monitor:\n",
    "                x_std = self._std_per_sample(x)\n",
    "                y_std = self._std_per_sample(y)\n",
    "\n",
    "                out_tmp = x + alpha * (y - x)\n",
    "                out_std = self._std_per_sample(out_tmp)\n",
    "\n",
    "                r_block = (y_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "                r_out = (out_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "\n",
    "                self._update_r_ema(r_out)\n",
    "                alpha_eff = self._compute_alpha_eff(x, alpha)\n",
    "\n",
    "                monitor_stats = {\n",
    "                    \"x_std\": float(x_std.detach()),\n",
    "                    \"y_std\": float(y_std.detach()),\n",
    "                    \"out_std_pre\": float(out_std.detach()),\n",
    "                    \"r_block\": float(r_block.detach()),\n",
    "                    \"r_out_pre\": float(r_out.detach()),\n",
    "                    \"r_ema\": float(self.r_ema.detach()),\n",
    "                    \"alpha\": float(alpha.detach()),\n",
    "                    \"alpha_eff\": float(alpha_eff.detach()),\n",
    "                    \"rescue_mode\": self.rescue_mode,\n",
    "                }\n",
    "\n",
    "            out = x + alpha_eff * (y - x)\n",
    "            coord_stats = self.coord.last_mask_stats()\n",
    "            return out, ca_map, fusion_w, coord_stats, monitor_stats\n",
    "\n",
    "        y_ca, _ = self.ca(x)\n",
    "        y = self.coord(y_ca)\n",
    "\n",
    "        if not self.residual:\n",
    "            return y\n",
    "\n",
    "        alpha = self._alpha(x)\n",
    "        alpha_eff = alpha\n",
    "\n",
    "        if self.training and self.monitor:\n",
    "            x_std = self._std_per_sample(x)\n",
    "            out_tmp = x + alpha * (y - x)\n",
    "            out_std = self._std_per_sample(out_tmp)\n",
    "            r_out = (out_std / (x_std + 1e-12)).clamp(0.0, 10.0)\n",
    "\n",
    "            self._update_r_ema(r_out)\n",
    "            alpha_eff = self._compute_alpha_eff(x, alpha)\n",
    "\n",
    "        out = x + alpha_eff * (y - x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5057b3",
   "metadata": {},
   "source": [
    "## 8) Hızlı test\n",
    "\n",
    "Aşağıdaki hücre:\n",
    "- Adım 2 bloğunu kurar\n",
    "- Dummy input ile forward çalıştırır\n",
    "- Dönen shape’leri ve monitor_stats anahtarlarını gösterir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74fd6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 64, 56, 56])\n",
      "out: torch.Size([2, 64, 56, 56])\n",
      "ca_map: torch.Size([2, 64, 1, 1])\n",
      "fusion_w: torch.Size([2, 2])\n",
      "coord_stats: {'a_h': {'min': 0.4740104675292969, 'mean': 0.49960988759994507, 'max': 0.5296449065208435, 'std': 0.004075914621353149}, 'a_w': {'min': 0.48295947909355164, 'mean': 0.5000577569007874, 'max': 0.5220022201538086, 'std': 0.0036030977498739958}}\n",
      "monitor_stats keys: ['x_std', 'y_std', 'out_std_pre', 'r_block', 'r_out_pre', 'r_ema', 'alpha', 'alpha_eff', 'rescue_mode']\n",
      "monitor_stats: {'x_std': 1.0005626678466797, 'y_std': 0.6620503664016724, 'out_std_pre': 0.7463032603263855, 'r_block': 0.6616780757904053, 'r_out_pre': 0.745883584022522, 'r_ema': 0.9872941970825195, 'alpha': 0.7500000596046448, 'alpha_eff': 0.7500000596046448, 'rescue_mode': 'ratio_floor'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "m = CBAMChannelPlusCoord(\n",
    "    channels=64,\n",
    "    return_maps=True,\n",
    "    residual=True,\n",
    "    learnable_temperature=True,\n",
    "    monitor=True,\n",
    "    rescue_mode=\"ratio_floor\",\n",
    "    min_rescue_ratio=0.2,\n",
    "    alpha_eff_min=0.2,\n",
    "    r_min=0.45,\n",
    "    ema_momentum=0.95,\n",
    "    beta_ca=0.35,\n",
    "    ca_router_temperature=1.5,\n",
    "    ca_t_min=0.5,\n",
    "    ca_t_max=3.0,\n",
    "    coord_scale_min=0.6,\n",
    "    coord_scale_max=1.6,\n",
    ")\n",
    "\n",
    "m.train()\n",
    "out, ca_map, fusion_w, coord_stats, monitor_stats = m(x)\n",
    "\n",
    "print(\"x:\", x.shape)\n",
    "print(\"out:\", out.shape)\n",
    "print(\"ca_map:\", ca_map.shape)\n",
    "print(\"fusion_w:\", fusion_w.shape)\n",
    "print(\"coord_stats:\", coord_stats)\n",
    "print(\"monitor_stats keys:\", None if monitor_stats is None else list(monitor_stats.keys()))\n",
    "print(\"monitor_stats:\", monitor_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054e216",
   "metadata": {},
   "source": [
    "## 9) Çıktıları okuma notları\n",
    "\n",
    "- `ca_map (B,C,1,1)`: kanal maskesi (CA)\n",
    "- `fusion_w (B,2)`: avg/max birleşim ağırlıkları\n",
    "- `coord_stats`: Coord H/W maskelerinin dağılımı (min/mean/max/std)\n",
    "- `monitor_stats` (sadece training+monitor):\n",
    "  - `r_out_pre`: residual karışım uygulanmadan önce “out_tmp / x”\n",
    "  - `r_ema`: bunun EMA hali\n",
    "  - `alpha_eff`: rescue sonrası etkin karışım\n",
    "\n",
    "YOLO tarafında pratikte:\n",
    "- `r_out_pre` çok düşüyorsa (ör. 0.35–0.45 altı), blok bastırıyor olabilir.\n",
    "- `alpha_eff` düşmeye başladıysa rescue devreye giriyor demektir.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
