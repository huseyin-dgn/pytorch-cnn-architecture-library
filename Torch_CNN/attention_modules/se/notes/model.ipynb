{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9688cc",
   "metadata": {},
   "source": [
    "## Adım 0 — SE’nin yaptığı şey (tek cümle)\n",
    "\n",
    "* SE, her kanal için 0–1 arası bir ağırlık üretir ve feature map’i kanala göre çarpar.\n",
    "* Girdi/Çıktı: (B,C,H,W) -> (B,C,H,W) (shape değişmez)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c608b",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Adım 1 — En saf SE (sadece tanım)\n",
    "\n",
    "Klasik SE: GlobalAvgPool -> FC -> ReLU -> FC -> Sigmoid -> Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e8f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SeBlock(nn.Module):\n",
    "    def __init__(self, channels:int , reduction : int = 16):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden = max(1,channels//reduction)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels,hidden,kernel_size=1,bias=True)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden,channels,kernel_size=1,bias=True)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x:torch.Tensor) -> torch.Tensor:\n",
    "        w = self.pool(x)\n",
    "        w = self.fc1(w)\n",
    "        w = self.act(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.gate(w)        # (B,C,1,1) in [0,1]\n",
    "        return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5aa8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 56, 56]) torch.Size([2, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 64, 56, 56)\n",
    "se = SeBlock(64, reduction=16)\n",
    "y = se(x)\n",
    "print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e09c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Adım 2 — SE’yi bir Conv bloğa tak (gerçek kullanım)\n",
    "\n",
    "SE tek başına değil, genelde bir Conv bloğun sonuna konur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396f86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self,cin,cout,k=3,s=1,p=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(cin,cout,k,stride=s,padding=p,bias=False),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "    def forward(self,x) : return self.net(x)\n",
    "\n",
    "class ConvSEBlock(nn.Module):\n",
    "    def __init__(self, cin,cout,reductions = 16):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBNAct(cin,cout,3,1,1)\n",
    "        self.se = SeBlock(cout,reduction=reductions)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.se(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec311ed",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Adım 3 — “Daha iyi” SE: aktivasyonu SiLU yap (stabil + pratik)\n",
    "\n",
    "SE içindeki ReLU yerine SiLU/Swish çoğu modern modelde daha iyi davranır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94adbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEv2(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(1, channels // reduction)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1  = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act  = nn.SiLU(inplace=True)   # değişti\n",
    "        self.fc2  = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.pool(x)\n",
    "        w = self.fc1(w)\n",
    "        w = self.act(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.gate(w)\n",
    "        return x * w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4021073",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Adım 4 — “Daha iyi” SE: AvgPool + MaxPool (birleşik squeeze)\n",
    "\n",
    "Bazı görevlerde max bilgisi de yararlı olur. CBAM gibi düşün: squeeze’i güçlendir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d7b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_AvgMax(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(1, channels // reduction)\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1  = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act  = nn.ReLU(inplace=True)\n",
    "        self.fc2  = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.avg(x) + self.max(x)  # squeeze güçlendi\n",
    "        w = self.fc1(s)\n",
    "        w = self.act(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.gate(w)\n",
    "        return x * w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cae3be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Adım 5 — “Daha pratik” SE: reduction’u otomatik seç (küçük kanalda bozulmasın)\n",
    "\n",
    "Küçük C değerlerinde C//16 çok küçük kalabiliyor. Bu sürüm hidden’ı güvenli seçer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673f6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_Safe(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, min_hidden: int = 4):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, hidden, 1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden, channels, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.fc(self.pool(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb45c5",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34847b1a",
   "metadata": {},
   "source": [
    "## Şimdi son 4 adımı inceleyelim.Daha net ve daha işlevsel hale getirelim.\n",
    "\n",
    "----\n",
    "-----\n",
    "----\n",
    "\n",
    "# 1) Kapasite ve stabilite: Safe hidden + SiLU\n",
    "\n",
    "#### Ne değişti?\n",
    "\n",
    "* **hidden = max(min_hidden, C // reduction) (küçük kanalda kapasite ölmesin)**\n",
    "\n",
    "* ReLU → SiLU (daha stabil/pratik)\n",
    "\n",
    "Kod farkı (kritik satırlar):\n",
    "```python \n",
    "hidden = max(min_hidden, channels // reduction)   # yeni\n",
    "self.act = nn.SiLU(inplace=True)                  # ReLU yerine\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8c9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SE_Stable(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, min_hidden: int = 4):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)   # CHANGED\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1  = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act  = nn.SiLU(inplace=True)                # CHANGED\n",
    "        self.fc2  = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.pool(x)\n",
    "        w = self.fc1(w)\n",
    "        w = self.act(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.gate(w)\n",
    "        return x * w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51eabe",
   "metadata": {},
   "source": [
    "# 2) Squeeze’i güçlendir: AvgPool + MaxPool\n",
    "\n",
    "### Ne değişti?\n",
    "\n",
    "* Sadece avgpool(x) yerine avg + max kullanıyoruz.\n",
    "\n",
    "* Bu, kanal özetini daha güçlü yapar.\n",
    "\n",
    "Kod farkı (kritik satırlar):\n",
    "\n",
    "```python\n",
    "self.avg = nn.AdaptiveAvgPool2d(1)   # yeni\n",
    "self.mx  = nn.AdaptiveMaxPool2d(1)   # yeni\n",
    "s = self.avg(x) + self.mx(x)         # CHANGED\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392bec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_AvgMax(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, min_hidden: int = 4):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)   # ADDED\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)   # ADDED\n",
    "\n",
    "        self.fc1  = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act  = nn.SiLU(inplace=True)\n",
    "        self.fc2  = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.avg(x) + self.mx(x)         # CHANGED (squeeze güçlendi)\n",
    "        w = self.fc1(s)\n",
    "        w = self.act(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.gate(w)\n",
    "        return x * w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbc10a",
   "metadata": {},
   "source": [
    "# 3) Gate’i pratikleştir: Sigmoid ↔ Hardsigmoid seçilebilir\n",
    "\n",
    "### Ne değişti?\n",
    "\n",
    "Gate fonksiyonunu parametre yaptık:\n",
    "\n",
    "* sigmoid: klasik\n",
    "\n",
    "* hardsigmoid: genelde daha hızlı (özellikle CPU/edge)\n",
    "\n",
    "Kod farkı (kritik satırlar):\n",
    "```python\n",
    "if gate == \"sigmoid\": self.gate = nn.Sigmoid()\n",
    "elif gate == \"hardsigmoid\": self.gate = nn.Hardsigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a65426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_Gated(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 16, min_hidden: int = 4, gate: str = \"sigmoid\"):\n",
    "        super().__init__()\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate = nn.Sigmoid()\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate = nn.Hardsigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.avg(x) + self.mx(x)\n",
    "        w = self.fc1(s)\n",
    "        w = self.act(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.gate(w)\n",
    "        return x * w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6ef99",
   "metadata": {},
   "source": [
    "# 4) Entegrasyon ve kontrol: Squeeze türünü aç/kapat (ablation-friendly)\n",
    "\n",
    "### Ne değişti?\n",
    "\n",
    "* use_avg, use_max ile squeeze türlerini kontrol ediyoruz.\n",
    "\n",
    "Bu sayede:\n",
    "\n",
    "* classification: sadece avg çoğu zaman yeter\n",
    "\n",
    "* detection/seg: avg+max bazen daha iyi\n",
    "\n",
    "Kod farkı (kritik satırlar):\n",
    "```python\n",
    "assert use_avg or use_max\n",
    "s = 0.0\n",
    "if use_avg: s += avg(x)\n",
    "if use_max: s += max(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97d975",
   "metadata": {},
   "source": [
    "# Hepsini birleştiren “SEPlusPlus” (Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEPlusPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        gate: str = \"sigmoid\",      # \"sigmoid\" | \"hardsigmoid\"\n",
    "        use_avg: bool = True,\n",
    "        use_max: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert use_avg or use_max, \"En az bir squeeze türü açık olmalı.\"\n",
    "\n",
    "        # (1) Safe hidden\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        # (4) Ablation-friendly squeeze\n",
    "        self.use_avg = use_avg\n",
    "        self.use_max = use_max\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx  = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # (1) SiLU\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, 1, bias=True)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, 1, bias=True)\n",
    "\n",
    "        # (3) Gate choice\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate = nn.Sigmoid()\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate = nn.Hardsigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (2) Avg + Max squeeze (opsiyonel)\n",
    "        s = 0.0\n",
    "        if self.use_avg:\n",
    "            s = s + self.avg(x)\n",
    "        if self.use_max:\n",
    "            s = s + self.mx(x)\n",
    "\n",
    "        w = self.fc1(s)\n",
    "        w = self.act(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.gate(w)\n",
    "        return x * w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5ab5a",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "## Gelin son dokunuşlarımızı yapalım.\n",
    "---\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c0df2",
   "metadata": {},
   "source": [
    "# SE++ (Geliştirme Planı)\n",
    "\n",
    "Bu hücrede, mevcut SEPlusPlus modülünü daha stabil ve ablation-friendly hale getirmek için 3 iyileştirme ekleyeceğiz:\n",
    "\n",
    "## (A) Avg/Max Squeeze Birleştirme: Öğrenilebilir Ağırlıklar\n",
    "- `avg(x)` ve `max(x)` çıktılarını sabit toplamak yerine,\n",
    "- `w_avg` ve `w_max` ile **öğrenilebilir** birleştireceğiz.\n",
    "- Ağırlıkları **softmax** ile normalize edeceğiz: `softmax([w_avg, w_max])`\n",
    "- Böylece gate'in saturate olma ihtimali azalır ve model hangi squeeze'in daha faydalı olduğunu öğrenir.\n",
    "\n",
    "## (B) Temperature (T) ile Gate Saturasyonunu Kontrol\n",
    "- Sigmoid girişini yumuşatmak için: `gate_input / T`\n",
    "- `T > 1` daha yumuşak, `T < 1` daha keskin gate üretir.\n",
    "- Varsayılan olarak `T=1.0` kullanıp ablation ile deneyebiliriz.\n",
    "- İster sabit ister öğrenilebilir yapılabilir (burada opsiyon sunacağız).\n",
    "\n",
    "## (C) Residual Gating (Feature Öldürmeyi Azaltır)\n",
    "- Klasik SE: `y = x * gate`\n",
    "- Residual SE: `y = x * (1 + alpha * gate)`\n",
    "- `alpha` öğrenilebilir olursa model residual etkisini kendisi ayarlar.\n",
    "- Bu, özellikle detection/segmentation tarafında stabiliteye yardımcı olabilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407c891",
   "metadata": {},
   "source": [
    "# Ablation Checklist\n",
    "\n",
    "Denenecek varyantlar:\n",
    "1) Squeeze:\n",
    "   - sadece avg\n",
    "   - sadece max\n",
    "   - avg+max (öğrenilebilir fusion)\n",
    "\n",
    "2) Gate:\n",
    "   - sigmoid\n",
    "   - hardsigmoid\n",
    "\n",
    "3) Temperature:\n",
    "   - sabit T=1.0\n",
    "   - sabit T=2.0\n",
    "   - learnable T (pozitif olacak şekilde)\n",
    "\n",
    "4) Residual:\n",
    "   - kapalı: y = x * gate\n",
    "   - açık:  y = x * (1 + alpha * gate)\n",
    "     - alpha sabit (örn. 1.0)\n",
    "     - alpha learnable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc52c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SEPlusPlusV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        reduction: int = 16,\n",
    "        min_hidden: int = 4,\n",
    "        gate: str = \"sigmoid\",          # \"sigmoid\" | \"hardsigmoid\"\n",
    "        use_avg: bool = True,\n",
    "        use_max: bool = True,\n",
    "        fusion: str = \"softmax\",        # \"sum\" | \"softmax\"\n",
    "        temperature: float = 1.0,\n",
    "        learnable_temperature: bool = False,\n",
    "        residual: bool = False,\n",
    "        alpha_init: float = 1.0,\n",
    "        learnable_alpha: bool = False,\n",
    "        bias: bool = True,\n",
    "        eps: float = 1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert use_avg or use_max, \"En az bir squeeze türü açık olmalı.\"\n",
    "        assert fusion in (\"sum\", \"softmax\"), \"fusion 'sum' veya 'softmax' olmalı.\"\n",
    "        assert temperature > 0, \"temperature pozitif olmalı.\"\n",
    "\n",
    "        self.channels = channels\n",
    "        self.use_avg = use_avg\n",
    "        self.use_max = use_max\n",
    "        self.fusion = fusion\n",
    "        self.residual = residual\n",
    "        self.eps = eps\n",
    "\n",
    "        # Safe hidden\n",
    "        hidden = max(min_hidden, channels // reduction)\n",
    "\n",
    "        # Squeeze ops\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mx = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # Learnable fusion weights (only meaningful if both enabled)\n",
    "        # Start from equal importance.\n",
    "        if self.use_avg and self.use_max:\n",
    "            self.fusion_logits = nn.Parameter(torch.zeros(2))  # [0,0] -> softmax = [0.5,0.5]\n",
    "        else:\n",
    "            self.fusion_logits = None\n",
    "\n",
    "        # Excitation MLP (1x1 conv)\n",
    "        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1, bias=bias)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        # Gate choice\n",
    "        g = gate.lower()\n",
    "        if g == \"sigmoid\":\n",
    "            self.gate_fn = torch.sigmoid\n",
    "        elif g == \"hardsigmoid\":\n",
    "            self.gate_fn = F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(\"gate 'sigmoid' veya 'hardsigmoid' olmalı.\")\n",
    "\n",
    "        # Temperature: fixed or learnable (positive)\n",
    "        self.learnable_temperature = learnable_temperature\n",
    "        if learnable_temperature:\n",
    "            # parametrize T as softplus(t_raw) + eps to keep strictly positive\n",
    "            # init so that softplus(t_raw) ~= temperature\n",
    "            t_raw = torch.tensor(float(temperature))\n",
    "            # inverse softplus approx: log(exp(x)-1)\n",
    "            t_inv = torch.log(torch.exp(t_raw) - 1.0 + eps)\n",
    "            self.t_raw = nn.Parameter(t_inv)\n",
    "        else:\n",
    "            self.register_buffer(\"T\", torch.tensor(float(temperature)))\n",
    "\n",
    "        # Residual alpha\n",
    "        self.learnable_alpha = learnable_alpha\n",
    "        if residual:\n",
    "            if learnable_alpha:\n",
    "                self.alpha = nn.Parameter(torch.tensor(float(alpha_init)))\n",
    "            else:\n",
    "                self.register_buffer(\"alpha\", torch.tensor(float(alpha_init)))\n",
    "\n",
    "    def _get_temperature(self) -> torch.Tensor:\n",
    "        if self.learnable_temperature:\n",
    "            return F.softplus(self.t_raw) + self.eps\n",
    "        return self.T\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Squeeze\n",
    "        s_list = []\n",
    "        if self.use_avg:\n",
    "            s_list.append(self.avg(x))\n",
    "        if self.use_max:\n",
    "            s_list.append(self.mx(x))\n",
    "\n",
    "        if len(s_list) == 1:\n",
    "            s = s_list[0]\n",
    "        else:\n",
    "            # two squeezes: avg + max\n",
    "            if self.fusion == \"sum\":\n",
    "                s = s_list[0] + s_list[1]\n",
    "            else:\n",
    "                # softmax fusion: a*avg + b*max\n",
    "                w = torch.softmax(self.fusion_logits, dim=0)  # shape (2,)\n",
    "                s = w[0] * s_list[0] + w[1] * s_list[1]\n",
    "\n",
    "        # Excite\n",
    "        w = self.fc1(s)\n",
    "        w = self.act(w)\n",
    "        w = self.fc2(w)\n",
    "\n",
    "        # Temperature + gate\n",
    "        T = self._get_temperature()\n",
    "        w = w / T\n",
    "        w = self.gate_fn(w)\n",
    "\n",
    "        # Apply\n",
    "        if self.residual:\n",
    "            return x * (1.0 + self.alpha * w)\n",
    "        return x * w\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
