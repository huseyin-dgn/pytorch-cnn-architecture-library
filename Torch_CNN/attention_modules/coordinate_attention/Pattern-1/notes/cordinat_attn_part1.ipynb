{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd97862",
   "metadata": {},
   "source": [
    "---\n",
    "## AdÄ±m 1 iÃ§in incelenecek kod aÅŸaÄŸÄ±dadÄ±r.Temel bir uygulamadÄ±r ve amaÃ§ neyle karÅŸÄ± karÅŸÄ±ya olduÄŸumuzu , amacÄ±mÄ±zÄ±n ne olduÄŸunu anlamaktÄ±r\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f00aa",
   "metadata": {},
   "source": [
    "**H ve W eksenleri boyunca ayrÄ± ayrÄ± pooling yaparak konum bilgisini korur; bu iki eksen Ã¶zetini ortak bir 1Ã—1 dÃ¶nÃ¼ÅŸÃ¼mle birleÅŸtirip, tekrar iki ayrÄ± attention (A_h ve A_w) Ã¼retir ve girdiyi X * A_h * A_w ile yeniden Ã¶lÃ§ekler. Kanal maliyetini de C â†’ C/r reduction ile sÄ±nÄ±rlar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f3889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HSwish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0  # x * (Relu6(x+3) / 6)\n",
    "# Swish in daha ucuz versiyonudur.[âˆ’3,+3] geÃ§iÅŸ (non-linear) bÃ¶lgesi. Bu aralÄ±kta eÄŸri â€œyumuÅŸakâ€ davranÄ±r; dÄ±ÅŸÄ±nda ya 0â€™a yapÄ±ÅŸÄ±r ya lineer olur.\n",
    "# .Matematiksel tanÄ±mÄ± :\n",
    "# ===>>> x * (Relu6(x+3) / 6) tÃ¼r.O yÃ¼zden yukarÄ±da relu6 dan HSwish fonksiyonunu Ã¼retilmiÅŸtir.\n",
    "\n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, # Kanal sayÄ±sÄ±\n",
    "        reduction=32, # Boottleneck iÃ§in sÄ±kÄ±ÅŸtÄ±rma sayÄ±sÄ±.Channels a bÃ¶lÃ¼necektir.\n",
    "        min_mid_channels=8, # Bottleneck iÃ§in oluÅŸturulan sÄ±kÄ±ÅŸtÄ±rma sayÄ±sÄ±nÄ±n kontrolÃ¼dÃ¼r.\n",
    "        act=\"hswish\",\n",
    "        alpha=1.0, # Attention Ä±n emliyet kemeri.Attentionun ne kadar baskÄ±n olacaÄŸÄ± kontrol edilir.\n",
    "        learnable_alpha=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mid = max(min_mid_channels, in_channels // reduction) # Ara katman hesaplamasÄ±\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid) \n",
    "        # BN burada 1Ã—1 conv Ã§Ä±ktÄ±sÄ±nÄ± stabilize eder, sigmoid saturasyonunu azaltÄ±r, \n",
    "        # H/W kaynaklarÄ±nÄ±n Ã¶lÃ§eklerini dengeler. \n",
    "\n",
    "        if act.lower() == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        elif act.lower() == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'hswish' or 'relu'\")\n",
    "\n",
    "        self.conv_h = nn.Conv2d(mid, in_channels, kernel_size=1, bias=True) \n",
    "        # Burdan kanal yada konum bilgisi iÅŸimize yarar mÄ± onu seÃ§iyoruz.\n",
    "        # Height ve width iÃ§in ayrÄ± 1Ã—1 headâ€™ler; mid temsilden \n",
    "        # C kanala projekte edip A_h ve A_w maskelerini Ã¼retir (Ã¶ÄŸrenilebilir).\n",
    "\n",
    "        self.conv_w = nn.Conv2d(mid, in_channels, kernel_size=1, bias=True)\n",
    "\n",
    "        if learnable_alpha:\n",
    "            self.alpha = nn.Parameter(torch.tensor(float(alpha)))\n",
    "        else:\n",
    "            self.register_buffer(\"alpha\", torch.tensor(float(alpha)))\n",
    "\n",
    "        self._last_ah = None # Debug iÃ§in...\n",
    "        self._last_aw = None # Debug iÃ§in...\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        # Klasik bir CNN girdisi.BunlarÄ±n dim karÅŸÄ±lÄ±ÄŸÄ± \n",
    "        # B = 0 ,C = 1 , H = 2 , W = 3 \n",
    "\n",
    "        x_h = x.mean(dim=3, keepdim=True)  \n",
    "        # Ä°ÅŸlemin W uzayÄ±nda yapÄ±lacaÄŸÄ±nÄ± sÃ¶yler.W boyutunda ortalama alÄ±r \n",
    "        # OrtalamasÄ± alÄ±nan W uzayÄ±nÄ± kaldÄ±rÄ±r ve 1 koyar.\n",
    "        # Son boyut = B,C,H,1\n",
    "\n",
    "        x_w = x.mean(dim=2, keepdim=True).permute(0, 1, 3, 2)\n",
    "        # Ä°ÅŸlemin H uzayÄ±nda yapÄ±lacaÄŸÄ±nÄ± belirtir.H uzanÄ±yÄ±n ortalamasÄ±nÄ± alÄ±r.\n",
    "        # OrtalamasÄ± alÄ±nan H uzayÄ±nÄ± kaldÄ±rÄ±r ve 1 koyar\n",
    "        # Son boyut burda ÅŸudur :: :: ( B , C, 1, W)\n",
    "        # Ä°ÅŸlemlerin doÄŸru olmasÄ± iÃ§in boyutlarÄ± eÅŸitlememiz gerekir.Bu iÅŸlemi permute yapar.\n",
    "        # BoyutlarÄ± indexlerine gÃ¶re sÄ±ralatÄ±r.Ve son durumda boyut :: ( B,C,W,1) olur\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        # x_h: (B,C,H,1) ve x_w: (B,C,W,1) tensÃ¶rlerini dim=2 boyunca uÃ§ uca ekler\n",
    "        # SonuÃ§: (B,C,H+W,1)  -> H ve W Ã¶zetleri tek eksende birleÅŸtirildi\n",
    "\n",
    "        y = self.act(self.bn1(self.conv1(y))) \n",
    "        # cat sonrasÄ± 1Ã—1 conv + BN + activation, height ve width eksenlerinden gelen konumsal \n",
    "        # Ã¶zetlerin ayrÄ±lmadan Ã¶nce ortak bir latent uzayda birlikte Ã¶ÄŸrenilmesini saÄŸlar.\n",
    "\n",
    "        y_h, y_w = torch.split(y, [h, w], dim=2)\n",
    "        # YukarÄ±da concat + shared conv/BN/act sonrasÄ± oluÅŸan ortak temsilden geÃ§en mimari burda ayÄ±rÄ±lma iÅŸlemine geÃ§er.\n",
    "        # y_h deÄŸeri h , y_w deÄŸeri ise w yi temsil eder.\n",
    "        # Bu iÅŸlemler yapÄ±lÄ±rken boyut :: ( B ,C ,H+W ,1) olduÄŸundan dim = 2 seÃ§ilir.\n",
    "        # y_h: (B, mid, H, 1)   (height tarafÄ±)\n",
    "        # y_w: (B, mid, W, 1)   (width tarafÄ±)\n",
    "\n",
    "        y_w = y_w.permute(0, 1, 3, 2)\n",
    "        # Yukardaki 62.satÄ±rda yapÄ±lan permute iÅŸlemi tekrarlanÄ±r.\n",
    "        # y_w deÄŸeri (B,mid,1,W) haline getirilir.\n",
    "        # Bu noktadan sonra iki tensor ayrÄ± kola gider.\n",
    "        # Birisi W diÄŸeri ise H ekseninde iÅŸlem yapar.Zaten cordinat attn de bunu ister.\n",
    "        # y_h â†’ H ekseninde deÄŸiÅŸen maske\n",
    "        # y_w â†’ W ekseninde deÄŸiÅŸen maske\n",
    "\n",
    "        a_h = torch.sigmoid(self.conv_h(y_h))\n",
    "        # H eksenindeki kanal burda aÄŸÄ±rlÄ±klandÄ±rÄ±lÄ±r\n",
    "        # Height (H) ekseni boyunca kanal bazlÄ± attention Ã¼retir.\n",
    "        # conv_h: (B, mid, H, 1) -> (B, C, H, 1)\n",
    "        # sigmoid ile aÄŸÄ±rlÄ±klar 0-1 aralÄ±ÄŸÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±lÄ±r (gate/Ã¶lÃ§ek katsayÄ±sÄ±).\n",
    "        \n",
    "        a_w = torch.sigmoid(self.conv_w(y_w))\n",
    "        # YukarÄ±daki aÃ§Ä±klamanÄ±n aynÄ±sÄ± burda da H ekseni iÃ§in yapÄ±lÄ±r.\n",
    "\n",
    "        self._last_ah = a_h # DEBUG iÃ§in deÄŸerler saklanÄ±r\n",
    "        self._last_aw = a_w # DEBUG iÃ§in deÄŸerler saklanÄ±r\n",
    "\n",
    "        att = a_h * a_w # Ã‡Ä±kan deÄŸerler Ã§arpÄ±lÄ±r ve attention artÄ±k hazÄ±r olur.\n",
    "\n",
    "        scale = (1.0 - self.alpha) + self.alpha * att\n",
    "        # Attention burda Ã¶lÃ§eklenir.Ne kadar sert yada hafif olacaÄŸÄ±nÄ± ayarlar\n",
    "        # AynÄ± zamanda attention burda kontrol edilir.Alpha deÄŸeri emniyet kemeri gÃ¶revi gÃ¶rÃ¼r.\n",
    "        # EÄŸer alpha 0.8 ise :: 0.2 + 0.8*att deÄŸerinden attention ifadesi iÅŸleme alÄ±nÄ±r\n",
    "\n",
    "        return x * scale # Modelin ileriye doÄŸru hareketi iÃ§in x ve Ã¶lÃ§eklendirme Ã§arpÄ±lÄ±r.\n",
    "\n",
    "    @torch.no_grad()\n",
    "# last_mask_stats(), son forwardâ€™daki a_h ve a_w attention maskelerinin \n",
    "# daÄŸÄ±lÄ±mÄ±nÄ± (min/mean/max/std) gÃ¶rÃ¼p saturasyon veya aÅŸÄ±rÄ± bastÄ±rma gibi sorunlarÄ± \n",
    "# erken yakalamak iÃ§in kullanÄ±lan debug yardÄ±mcÄ± fonksiyonudur.\n",
    "    def last_mask_stats(self):\n",
    "        if self._last_ah is None or self._last_aw is None:\n",
    "            return None\n",
    "        ah = self._last_ah\n",
    "        aw = self._last_aw\n",
    "        return {\n",
    "            \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "            \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "        }\n",
    "\n",
    "class ConvCoordAttBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        k=3,\n",
    "        s=1,\n",
    "        p=None,\n",
    "        groups=1,\n",
    "        norm=\"bn\",\n",
    "        act=\"silu\",\n",
    "        coord_reduction=32,\n",
    "        coord_min_mid=8,\n",
    "        coord_act=\"hswish\",\n",
    "        alpha=1.0,\n",
    "        learnable_alpha=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = k // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=k, stride=s, padding=p, groups=groups, bias=False)\n",
    "\n",
    "        if norm == \"bn\":\n",
    "            self.norm = nn.BatchNorm2d(out_channels)\n",
    "        elif norm == \"gn\":\n",
    "            g = 8\n",
    "            while out_channels % g != 0 and g > 1:\n",
    "                g //= 2\n",
    "            self.norm = nn.GroupNorm(g, out_channels)\n",
    "        elif norm == \"none\":\n",
    "            self.norm = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(\"norm must be 'bn', 'gn', or 'none'\")\n",
    "\n",
    "        if act == \"silu\":\n",
    "            self.act = nn.SiLU(inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'silu', 'relu', or 'hswish'\")\n",
    "\n",
    "        self.att = CoordAtt(\n",
    "            in_channels=out_channels,\n",
    "            reduction=coord_reduction,\n",
    "            min_mid_channels=coord_min_mid,\n",
    "            act=coord_act,\n",
    "            alpha=alpha,\n",
    "            learnable_alpha=learnable_alpha,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return self.att(x)\n",
    "\n",
    "@torch.no_grad()\n",
    "def coordatt_debug_pass(module, x, name=\"CoordAttDebug\"):\n",
    "    module.eval()\n",
    "    out = module(x)\n",
    "\n",
    "    in_mean, in_std = float(x.mean()), float(x.std())\n",
    "    out_mean, out_std = float(out.mean()), float(out.std())\n",
    "    ratio = out_std / (in_std + 1e-12)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"in shape :\", tuple(x.shape))\n",
    "    print(\"out shape:\", tuple(out.shape))\n",
    "    print(f\"in  mean/std : {in_mean:.6f} / {in_std:.6f}\")\n",
    "    print(f\"out mean/std : {out_mean:.6f} / {out_std:.6f}\")\n",
    "    print(f\"std_ratio(out/in): {ratio:.6f}\")\n",
    "\n",
    "    if hasattr(module, \"last_mask_stats\"):\n",
    "        stats = module.last_mask_stats()\n",
    "        if stats is not None:\n",
    "            print(\"a_h stats:\", stats[\"a_h\"])\n",
    "            print(\"a_w stats:\", stats[\"a_w\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "def quick_sanity():\n",
    "    torch.manual_seed(0)\n",
    "    x = torch.randn(2, 64, 56, 56)\n",
    "\n",
    "    att = CoordAtt(64, reduction=32, alpha=1.0, learnable_alpha=False)\n",
    "    coordatt_debug_pass(att, x, \"Only CoordAtt\")\n",
    "\n",
    "    blk = ConvCoordAttBlock(64, 64, k=3, s=1, norm=\"bn\", act=\"silu\", coord_reduction=32, alpha=1.0)\n",
    "    coordatt_debug_pass(blk, x, \"Conv + CoordAtt Block\")\n",
    "\n",
    "    blk.train()\n",
    "    y = blk(x)\n",
    "    loss = y.mean()\n",
    "    loss.backward()\n",
    "    print(\"\\nBackward OK (loss:\", float(loss), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff35f3",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Åimdi biraz bu kodun aÃ§Ä±klamasÄ±na ve detaylarÄ±na inelim.Kodun Ã¼stÃ¼nde yorum satÄ±rlarÄ± zaten vardÄ±r.Ama daha detaylÄ± anlatÄ±mlarÄ± aÅŸaÄŸÄ±da bulabilirsiniz.\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b5a0e",
   "metadata": {},
   "source": [
    "# HSwish (Hard-Swish) Aktivasyon Fonksiyonu\n",
    "\n",
    "```python\n",
    "class HSwish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0\n",
    "```\n",
    "\n",
    "\n",
    "**HSwish, Swish aktivasyon fonksiyonunun hesaplama aÃ§Ä±sÄ±ndan hafifletilmiÅŸ (approximation) halidir. Ã–zellikle mobil ve hafif CNN mimarileri iÃ§in tasarlanmÄ±ÅŸtÄ±r.**\n",
    "\n",
    "### TanÄ±m\n",
    "\n",
    "##### Swish: Swish(x)=xâ‹…Ïƒ(x)\n",
    "\n",
    "##### HSwish: HSwish(x)=xâ‹…(ReLU6(x+3)â€‹) / 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdce075",
   "metadata": {},
   "source": [
    "### Kod KarÅŸÄ±lÄ±ÄŸÄ±\n",
    "```python\n",
    "x * relu6(x + 3) / 6\n",
    "```\n",
    "\n",
    "\n",
    "Bu ifade:\n",
    "\n",
    "* Sigmoid kullanmaz\n",
    "\n",
    "* ParÃ§alÄ± doÄŸrusal (piecewise linear) bir fonksiyondur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8006c",
   "metadata": {},
   "source": [
    "## Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09632e7c",
   "metadata": {},
   "source": [
    "| x aralÄ±ÄŸÄ±  | DavranÄ±ÅŸ     |\n",
    "| ---------- | ------------ |\n",
    "| x â‰¤ -3     | Ã§Ä±ktÄ± = 0    |\n",
    "| -3 < x < 3 | lineer geÃ§iÅŸ |\n",
    "| x â‰¥ 3      | Ã§Ä±ktÄ± = x    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32d4d9",
   "metadata": {},
   "source": [
    "* BÃ¼yÃ¼k pozitif deÄŸerleri aynen geÃ§irir\n",
    "\n",
    "* BÃ¼yÃ¼k negatifleri bastÄ±rÄ±r\n",
    "\n",
    "* Orta bÃ¶lgede yumuÅŸak geÃ§iÅŸ saÄŸlar\n",
    "\n",
    "## ReLU ile FarkÄ±\n",
    "\n",
    "| Ã–zellik                  | ReLU      | HSwish       |\n",
    "| ------------------------ | --------- | ------------ |\n",
    "| Negatif bÃ¶lgede gradient | 0         | KÄ±smen var   |\n",
    "| YumuÅŸaklÄ±k               | Keskin    | Daha yumuÅŸak |\n",
    "| Hesap maliyeti           | Ã‡ok dÃ¼ÅŸÃ¼k | DÃ¼ÅŸÃ¼k        |\n",
    "| Mobil uyum               | Ä°yi       | **Ã‡ok iyi**  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c525a15",
   "metadata": {},
   "source": [
    "## CoordAtt Ä°Ã§inde Neden Tercih Edilir?\n",
    "\n",
    "Coordinate Attention:\n",
    "\n",
    "* Kanal + konum bilgisini birlikte iÅŸler\n",
    "\n",
    "* Kanal sÄ±kÄ±ÅŸtÄ±rma sonrasÄ± non-linearity gerektirir\n",
    "\n",
    "HSwish:\n",
    "\n",
    "* Stabil gradient saÄŸlar\n",
    "\n",
    "* Mobil dostudur\n",
    "\n",
    "* 1Ã—1 conv sonrasÄ± iyi Ã§alÄ±ÅŸÄ±r\n",
    "\n",
    "Bu nedenle CoordAtt ve MobileNetV3 tabanlÄ± yapÄ±larda varsayÄ±lan aktivasyon olarak kullanÄ±lÄ±r.\n",
    "\n",
    "#### Ã–zet olarak : **HSwish, Swishâ€™in sigmoid iÃ§ermeyen, ReLU6 tabanlÄ± ve mobil iÃ§in optimize edilmiÅŸ bir yaklaÅŸÄ±k versiyonudur.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c7531",
   "metadata": {},
   "source": [
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344143a",
   "metadata": {},
   "source": [
    "# mid ne iÅŸe yarÄ±yor?\n",
    "```python\n",
    "        mid = max(min_mid_channels, in_channels // reduction)\n",
    "```\n",
    "\n",
    "\n",
    "mid, ÅŸu iki iÅŸi aynÄ± anda yapar:\n",
    "\n",
    "### Kanal sÄ±kÄ±ÅŸtÄ±rma (parametre ve FLOP kontrolÃ¼)\n",
    "* GiriÅŸ kanalÄ±n C iken, ortak dÃ¶nÃ¼ÅŸÃ¼mde C â†’ mid yaparsÄ±n.\n",
    "* Bu sayede 1Ã—1 convâ€™larÄ±n maliyeti dÃ¼ÅŸer.\n",
    "\n",
    "### Ortak bir gizli temsil (shared embedding) Ã¼retmek\n",
    "* CoordAttâ€™te iki eksenden gelen Ã¶zetler (x_h ve x_w) birleÅŸtiriliyor.\n",
    "* Bu birleÅŸik bilgi Ã¶nce tek bir ortak projeksiyondan geÃ§iyor:\n",
    "\n",
    "**concat([x_h, x_w]) â†’ shape (B, C, H+W, 1)**\n",
    "\n",
    "**Conv1Ã—1: C â†’ mid â†’ shape (B, mid, H+W, 1)**\n",
    "\n",
    "* Bu mid uzayÄ±, â€œyÃ¼kseklik + geniÅŸlikâ€ bilgisini tek bir latent uzayda karÄ±ÅŸtÄ±rÄ±p temsil ediyor. Sonra bu latent temsil iki ayrÄ± head ile tekrar C kanala geniÅŸletiliyor:\n",
    "\n",
    "Conv_h: mid â†’ C\n",
    "\n",
    "Conv_w: mid â†’ C\n",
    "\n",
    "### Yani mid tam olarak ÅŸu rolÃ¼ oynuyor:\n",
    "\n",
    "**(H ve W) eksen bilgisini birleÅŸtiren ortak latent alan + maliyet dÃ¼ÅŸÃ¼ren bottleneck.**\n",
    "\n",
    "## CBAMâ€™deki hidden ile birebir benzerlik\n",
    "\n",
    "CBAM-Channel (SE benzeri) iÃ§inde hidden ÅŸunun iÃ§indi:\n",
    "\n",
    "* Kanal istatistiÄŸini (C) dÃ¼ÅŸÃ¼k boyuta indirip iliÅŸki Ã¶ÄŸrenmek\n",
    "\n",
    "CoordAttâ€™te mid aynÄ± ÅŸeyi yapÄ±yor ama ek olarak:\n",
    "\n",
    "* Girdi artÄ±k sadece C vektÃ¶rÃ¼ deÄŸil,\n",
    "\n",
    "* (H+W) boyunca uzanan koordinat-encode edilmiÅŸ bir sinyal.\n",
    "**Bu yÃ¼zden mid, hem kanal iliÅŸkisi hem de eksensel konum baÄŸlamÄ±nÄ± taÅŸÄ±yan ortak temsil oluyor.**\n",
    "\n",
    "### Pratik sonuÃ§: mid neyi deÄŸiÅŸtirir?\n",
    "\n",
    "* mid kÃ¼Ã§Ã¼kse: daha hafif, ama kapasite azalÄ±r â†’ maskeler zayÄ±flayabilir\n",
    "\n",
    "* mid bÃ¼yÃ¼kse: daha gÃ¼Ã§lÃ¼, ama maliyet artar â†’ overfit/over-suppress riski artabilir\n",
    "\n",
    "**Bu yÃ¼zden mid = max(min_mid, C // reduction) dengesi standarttÄ±r.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e093ed",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927375ff",
   "metadata": {},
   "source": [
    "# 1Ã—1 Conv sonrasÄ± neden normalization (BN) var?\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f9b90",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ 1Ã—1 Conv burada ne yaptÄ±?\n",
    "\n",
    "Bu convâ€™un yaptÄ±ÄŸÄ± ÅŸey ÅŸu:\n",
    "\n",
    "* Girdi: (B, C, H+W, 1)\n",
    "\n",
    "* Ã‡Ä±ktÄ±: (B, mid, H+W, 1)\n",
    "\n",
    "Yani:\n",
    "\n",
    "* Kanal sayÄ±sÄ±nÄ± C â†’ mid indiriyor\n",
    "\n",
    "* Height + Width eksenlerinden gelen bilgiyi lineer olarak karÄ±ÅŸtÄ±rÄ±yor\n",
    "\n",
    "Ama bu karÄ±ÅŸtÄ±rma kontrolsÃ¼z.\n",
    "\n",
    "1Ã—1 conv ÅŸunu yapar:\n",
    "\n",
    "* ğ‘¦=Wx\n",
    "\n",
    "Burada:\n",
    "\n",
    "* x: concat edilmiÅŸ, istatistiÄŸi bozuk bir tensor\n",
    "\n",
    "* W: rastgele baÅŸlatÄ±lmÄ±ÅŸ aÄŸÄ±rlÄ±klar\n",
    "\n",
    "SonuÃ§:\n",
    "\n",
    "* Ã‡Ä±ktÄ±nÄ±n Ã¶lÃ§eÄŸi garanti altÄ±nda deÄŸil\n",
    "\n",
    "## 2ï¸âƒ£ 1Ã—1 Conv Ã§Ä±kÄ±ÅŸÄ± nasÄ±l bir ÅŸey olur? (BN YOKSA)\n",
    "\n",
    "BN yoksa ÅŸu problemler Ã§Ä±kar:\n",
    "\n",
    "#### âŒ Problem 1: Ã–lÃ§ek patlar ya da Ã§Ã¶ker\n",
    "\n",
    "Conv Ã§Ä±kÄ±ÅŸÄ±:\n",
    "\n",
    "* Ã‡ok bÃ¼yÃ¼k deÄŸerlere gidebilir\n",
    "\n",
    "* Ya da Ã§ok kÃ¼Ã§Ã¼k deÄŸerlere sÄ±kÄ±ÅŸabilir\n",
    "\n",
    "* Bu tensor birazdan sigmoidâ€™e gidecek attentionâ€™Ä±n atasÄ±.\n",
    "\n",
    "BN yoksa:\n",
    "\n",
    "* Attention maskeleri Ã§ok erken saturate olur\n",
    "\n",
    "Model daha Ã¶ÄŸrenmenin baÅŸÄ±nda:\n",
    "\n",
    "* ya her ÅŸeyi aÃ§ar , ya her ÅŸeyi kapatÄ±r\n",
    "\n",
    "**Bu attentionâ€™Ä±n agresifleÅŸmesi dediÄŸimiz durum.**\n",
    "\n",
    "#### âŒ Problem 2: H ve W eksenlerinden biri domine eder\n",
    "\n",
    "Unutma:\n",
    "\n",
    "* x_h ve x_w farklÄ± istatistiklere sahip\n",
    "\n",
    "* Concat sonrasÄ± conv bunlarÄ± karÄ±ÅŸtÄ±rÄ±yor\n",
    "\n",
    "BN yoksa:\n",
    "\n",
    "* Conv, Ã¶rneÄŸin x_h tarafÄ±nÄ± Ã§ok bÃ¼yÃ¼tÃ¼p\n",
    "\n",
    "* x_w tarafÄ±nÄ± bastÄ±rabilir (veya tam tersi)\n",
    "\n",
    "SonuÃ§:\n",
    "\n",
    "* Attention tek eksene kilitlenir\n",
    "\n",
    "* CoordAttâ€™in temel fikri bozulur\n",
    "\n",
    "#### âŒ Problem 3: Aktivasyon (HSwish) anlamsÄ±zlaÅŸÄ±r\n",
    "\n",
    "HSwishâ€™in Ã§alÄ±ÅŸma aralÄ±ÄŸÄ± Ã¶nemli:\n",
    "\n",
    "* Etkili olduÄŸu bÃ¶lge â‰ˆ [-3, 3]\n",
    "\n",
    "BN yoksa:\n",
    "\n",
    "* Conv Ã§Ä±kÄ±ÅŸÄ± bu aralÄ±ÄŸÄ±n Ã§ok dÄ±ÅŸÄ±na Ã§Ä±kar\n",
    "\n",
    "HSwish ya:\n",
    "\n",
    "* tamamen lineer Ã§alÄ±ÅŸÄ±r\n",
    "\n",
    "* ya tamamen sÄ±fÄ±r Ã¼retir\n",
    "\n",
    "**Yani HSwishâ€™i koymuÅŸ olsak bile fiilen ReLU gibi davranÄ±r.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e600dd",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "# Neden 1Ã—1 convâ€™dan iki tane var? Neden biri H, biri W? AmaÃ§ ne?\n",
    "```python\n",
    "self.conv_h = nn.Conv2d(mid, in_channels, kernel_size=1, bias=True)\n",
    "self.conv_w = nn.Conv2d(mid, in_channels, kernel_size=1, bias=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33347514",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Bu noktaya gelene kadar elimizde ne var?\n",
    "\n",
    "Åu anda elimizde:\n",
    "```bash\n",
    "y âˆˆ R^{B Ã— mid Ã— (H+W) Ã— 1}\n",
    "```\n",
    "Bu tensor:\n",
    "\n",
    "* Height ve Width bilgisini birlikte taÅŸÄ±yor\n",
    "\n",
    "* Ortak bir latent uzayda (mid)\n",
    "\n",
    "* Ama henÃ¼z attention deÄŸil\n",
    "\n",
    "Yani:\n",
    "\n",
    "**â€œBu kanal + konum bilgisi Ã¶nemli mi?â€**\n",
    "sorusunun ham cevabÄ±.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e24305",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a0b93",
   "metadata": {},
   "source": [
    "## SonrasÄ±nda Ã¶ÄŸrenilebilir alpha koyduk.Bu alphayÄ± parameter ile direkt modele verdik.EÄŸer false ise modele girmesin etkisi modele deÄŸimesin diye register buffer koyduk.Sonra da ÅŸu 2 deÄŸeri oluÅŸturduk.Bunlar ne ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35c2a7",
   "metadata": {},
   "source": [
    "```python\n",
    "self._last_ah = None \n",
    "self._last_aw = None\n",
    "```\n",
    "**self._last_ah ve self._last_aw tamamen debug/izleme amaÃ§lÄ± iki deÄŸiÅŸken. Modelin â€œÃ¶ÄŸrenenâ€ kÄ±smÄ± deÄŸiller; sadece son forwardâ€™da Ã¼retilen attention maskelerini sÄ±nÄ±f iÃ§inde saklamak iÃ§in varlar.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543596eb",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfcde4",
   "metadata": {},
   "source": [
    "# Åimdi ise forward kÄ±smÄ±na geÃ§elim.Bu satÄ±rlar ne iÅŸe yarÄ±yor ?\n",
    "```python\n",
    "x_h = x.mean(dim=3, keepdim=True) \n",
    "x_w = x.mean(dim=2, keepdim=True).permute(0, 1, 3, 2) \n",
    "```\n",
    "**Bu iki satÄ±r, Coordinate Attentionâ€™Ä±n en temel hamlesi: 2D uzayÄ± tek seferde yok etmek yerine, H ve W eksenlerini ayrÄ± ayrÄ± Ã¶zetlemek.**\n",
    "\n",
    "PyTorchâ€™ta boyut indeksleri:\n",
    "\n",
    "* dim=0 â†’ B\n",
    "\n",
    "* dim=1 â†’ C\n",
    "\n",
    "* dim=2 â†’ H\n",
    "\n",
    "* dim=3 â†’ W\n",
    "\n",
    "## 1) x_h = x.mean(dim=3, keepdim=True)\n",
    "\n",
    "Ne yapÄ±yor?\n",
    "* GeniÅŸlik ekseni (W, dim=3) boyunca ortalama alÄ±yor.\n",
    "\n",
    "AnlamÄ±:\n",
    "\n",
    "* Her yÃ¼kseklik satÄ±rÄ± iÃ§in (h),\n",
    "\n",
    "* o satÄ±rdaki W boyunca ortalama aktivasyon.\n",
    "\n",
    "**Yani: â€œBu kanal, dikey eksende nerelerde gÃ¼Ã§lÃ¼?â€**\n",
    "\n",
    "## 2) x_w = x.mean(dim=2, keepdim=True).permute(0, 1, 3, 2)\n",
    "\n",
    "Ä°ki adÄ±m var:\n",
    "### 2.a) x.mean(dim=2, keepdim=True)\n",
    "\n",
    "* YÃ¼kseklik ekseni (H, dim=2) boyunca ortalama alÄ±yor.\n",
    "* Bu aÅŸamada shape: (B,C,1,W)\n",
    "\n",
    "AnlamÄ±:\n",
    "\n",
    "* Her geniÅŸlik sÃ¼tunu iÃ§in (w),\n",
    "\n",
    "* H boyunca ortalama aktivasyon.\n",
    "\n",
    "**Yani: â€œBu kanal, yatay eksende nerelerde gÃ¼Ã§lÃ¼?â€**\n",
    "\n",
    "### 2.b) .permute(0, 1, 3, 2)\n",
    "\n",
    "Bu, boyutlarÄ±n yerini deÄŸiÅŸtiriyor:\n",
    "\n",
    "- Ã–nce: (B, C, 1, W)\n",
    "\n",
    "- Sonra: (B, C, W, 1)\n",
    "\n",
    "## Neden?\n",
    "Ã‡Ã¼nkÃ¼ birazdan x_h ile concat yapacaÄŸÄ±z:\n",
    "\n",
    "* x_h: (B, C, H, 1)\n",
    "\n",
    "* x_w (permute sonrasÄ±): (B, C, W, 1)\n",
    "\n",
    "Bu ikisi artÄ±k aynÄ± formatta:\n",
    "\n",
    "* Son boyut 1\n",
    "\n",
    "* ÃœÃ§Ã¼ncÃ¼ boyut â€œuzunlukâ€ (H veya W)\n",
    "\n",
    "### Yani kÄ±saca::\n",
    ">x_h: W boyunca ortalama â†’ (B,C,H,1) â†’ height yÃ¶nlÃ¼ Ã¶zet\n",
    "\n",
    ">x_w: H boyunca ortalama â†’ (B,C,1,W) sonra permute â†’ (B,C,W,1) â†’ width yÃ¶nlÃ¼ Ã¶zet\n",
    "\n",
    ">permute: concat iÃ§in format eÅŸleme (H ve Wâ€™yi aynÄ± eksende yan yana koyabilmek iÃ§in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0a855",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# 1.) cat burada ne yapÄ±yor?\n",
    "```python\n",
    "y = torch.cat([x_h, x_w], dim=2)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.cat, iki tensoru belirttiÄŸin eksen (dim) boyunca uÃ§ uca ekler. Yani toplama deÄŸil; birleÅŸtirme.\n",
    "\n",
    "Bizim elimizde iki tensor var:\n",
    "\n",
    "* x_h shape: (B, C, H, 1)\n",
    "\n",
    "* x_w shape: (B, C, W, 1) (permute sonrasÄ±)\n",
    "\n",
    "Bu ikisini cat edebilmek iÃ§in cat yapÄ±lmayan tÃ¼m boyutlar aynÄ± olmalÄ±:\n",
    "\n",
    "* B aynÄ± âœ…\n",
    "\n",
    "* C aynÄ± âœ…\n",
    "\n",
    "* son boyut 1 aynÄ± âœ…\n",
    "\n",
    "**sadece birleÅŸtireceÄŸimiz boyut farklÄ±: H ve W âœ…**.DolayÄ±sÄ±yla bu kod sayesinde :\n",
    ">Yani x_h ve x_w Ã¼Ã§Ã¼ncÃ¼ boyutta yan yana dizildi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280acad",
   "metadata": {},
   "source": [
    "# 2.) dim=2 ne iÅŸe yarÄ±yor?\n",
    "\n",
    "* dim=2 demek: 3. boyut (H ekseni slotu) boyunca birleÅŸtir demek.\n",
    "\n",
    "PyTorchâ€™ta indeksler:\n",
    "\n",
    "* dim=0 â†’ B\n",
    "\n",
    "* dim=1 â†’ C\n",
    "\n",
    "* dim=2 â†’ H (bizim burada â€œuzunluk ekseniâ€ gibi kullanÄ±yoruz)\n",
    "\n",
    "* dim=3 â†’ W veya 1\n",
    "\n",
    "Biz x_wâ€™yi permute ederek zaten (B,C,W,1) yaptÄ±k ki:\n",
    "\n",
    "* dim=2 artÄ±k x_h iÃ§in H\n",
    "\n",
    "* dim=2 artÄ±k x_w iÃ§in W\n",
    "\n",
    "olacak ÅŸekilde aynÄ± yere gelsin.\n",
    "\n",
    "### **Neden dim=2 seÃ§ildi?**\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ amacÄ±mÄ±z ÅŸuydu:\n",
    "\n",
    "* Height ve Width Ã¶zetlerini tek bir â€œsequenceâ€ gibi aynÄ± eksende toplamak.\n",
    "\n",
    "Bu yÃ¼zden:\n",
    "\n",
    "* x_hâ€™nin H uzunluÄŸunu\n",
    "\n",
    "- x_wâ€™nin W uzunluÄŸunu\n",
    "\n",
    "aynÄ± eksende topladÄ±k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c46540",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Åimdi cat sonrasÄ± ifadeyi inceleyelim.\n",
    "```python \n",
    "y_h, y_w = torch.split(y, [h, w], dim=2) \n",
    "y_w = y_w.permute(0, 1, 3, 2)\n",
    "```\n",
    "\n",
    "### 1) y_h, y_w = torch.split(y, [h, w], dim=2)\n",
    "\n",
    "Ne yapÄ±yor :: dim=2 eksenini (Ã¼Ã§Ã¼ncÃ¼ boyutu) iki parÃ§aya bÃ¶lÃ¼yor:\n",
    "* y_h = ilk h uzunluk\n",
    "\n",
    "* y_w = sonraki w uzunluk\n",
    "\n",
    "Yani:\n",
    "\n",
    "* y_h artÄ±k height iÃ§in ayrÄ±lmÄ±ÅŸ latent temsil\n",
    "\n",
    "* y_w artÄ±k width iÃ§in ayrÄ±lmÄ±ÅŸ latent temsil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221dd835",
   "metadata": {},
   "source": [
    "### 2) y_w = y_w.permute(0, 1, 3, 2)\n",
    "Åu anda y_w ÅŸu formatta:\n",
    "\n",
    "* ywâ€‹ âˆˆR BÃ—midÃ—WÃ—1\n",
    "\n",
    "Ama width attention Ã¼retmek iÃ§in bizim hedef formatÄ±mÄ±z:\n",
    "\n",
    "* (BÃ—midÃ—1Ã—W)\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ birazdan ÅŸunu yapacaÄŸÄ±z:\n",
    "```python\n",
    "a_w = sigmoid(conv_w(y_w))\n",
    "```\n",
    "* ve bu a_wâ€™nin ÅŸekli (B, C, 1, W) olmalÄ± ki girdiyle broadcast dÃ¼zgÃ¼n Ã§alÄ±ÅŸsÄ±n.\n",
    "\n",
    "Bu yÃ¼zden permute(0,1,3,2) yapÄ±yoruz:\n",
    "\n",
    "* Ã–nce: (B, mid, W, 1)\n",
    "\n",
    "* Sonra: (B, mid, 1, W)\n",
    "\n",
    "Yani son iki boyut yer deÄŸiÅŸtiriyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cf87b",
   "metadata": {},
   "source": [
    "### Neden sadece y_w permute ediliyor?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼:\n",
    "\n",
    "* y_h zaten (B, mid, H, 1) formatÄ±nda ve height attention head bunu bekler.\n",
    "\n",
    "* y_w ise concat Ã¶ncesinde width tarafÄ±nÄ± (B, C, W, 1) formatÄ±na Ã§evirmiÅŸtik.\n",
    "* Åimdi tekrar width baÅŸlÄ±ÄŸÄ±nÄ±n beklediÄŸi (B, mid, 1, W) formatÄ±na dÃ¶nmemiz gerekiyor.\n",
    "\n",
    "#### Genel Ã¶zet olarak ::\n",
    "\n",
    "**torch.split(y, [h, w], dim=2) iÅŸlemi, concat sonrasÄ± oluÅŸan (H+W) uzunluk eksenini ilk H (height) ve sonraki W (width) parÃ§alarÄ±na ayÄ±rarak y_h âˆˆ R^{BÃ—midÃ—HÃ—1} ve y_w âˆˆ R^{BÃ—midÃ—WÃ—1} Ã¼retir. ArdÄ±ndan y_w.permute(0,1,3,2) ile y_w (BÃ—midÃ—1Ã—W) formatÄ±na Ã§evrilir; bÃ¶ylece width attention maskesi (BÃ—CÃ—1Ã—W) Ã¼retmek ve girdiye doÄŸru broadcast etmek mÃ¼mkÃ¼n olur.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6056d8",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af1468",
   "metadata": {},
   "source": [
    "# Neden sigmoid?\n",
    "```python\n",
    "a_h = sigmoid(conv_h(y_h))  # (B, C, H, 1)\n",
    "a_w = sigmoid(conv_w(y_w))  # (B, C, 1, W)  (Ã¶ncesinde permute ile formatlanÄ±yor)\n",
    "```\n",
    "\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ sigmoid Ã§Ä±ktÄ±sÄ± 0 ile 1 arasÄ±ndadÄ±r:\n",
    "\n",
    "* 0â€™a yakÄ±nsa â†’ kapat (suppress)\n",
    "\n",
    "* 1â€™e yakÄ±nsa â†’ geÃ§ir (keep)\n",
    "\n",
    "Bu maskeler sonra girdiye Ã§arpÄ±lÄ±yor:\n",
    "\n",
    "* out = x * a_h * a_w\n",
    "\n",
    "----\n",
    "Buraya bir parantez aÃ§mak istiyorum.Permute iÅŸlemlerini daha net anlatabilmek iÃ§in ufak bir notlama koyuyorum.Buna hem bu dosya iÃ§erisinden hemde aynÄ± klasÃ¶rdeki permute.png gÃ¶rselinden ulaÅŸabilirsiniz.\n",
    "\n",
    "* x_h = mean over W â†’ (B,C,H,1) (W silinir)\n",
    "\n",
    "* x_w = mean over H â†’ (B,C,1,W) (H silinir)\n",
    "\n",
    "* x_w permute â†’ (B,C,W,1) (cat iÃ§in format)\n",
    "\n",
    "* cat dim=2 â†’ (B,C,H+W,1)\n",
    "\n",
    "* split [H,W] â†’ y_h:(B,mid,H,1), y_w:(B,mid,W,1)\n",
    "\n",
    "* y_w permute back â†’ (B,mid,1,W)\n",
    "\n",
    "* conv_h/conv_w + sigmoid â†’ a_h:(B,C,H,1), a_w:(B,C,1,W)\n",
    "\n",
    "* out = x * a_h * a_w (broadcast)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53cb39d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# ArtÄ±k son kÄ±sÄ±mdayÄ±z.(gating/mixing)\n",
    "```python\n",
    "att = a_h * a_w\n",
    "scale = (1.0 - self.alpha) + self.alpha * att\n",
    "return x * scale\n",
    "```\n",
    "### 1) att = a_h * a_w ne?\n",
    "\n",
    "* a_h shape: (B, C, H, 1)\n",
    "\n",
    "* a_w shape: (B, C, 1, W)\n",
    "\n",
    "Broadcast ile Ã§arpÄ±lÄ±nca: BÃ—CÃ—HÃ—W\n",
    "\n",
    "**Bu att bir attention maskesi: her kanal ve her konum iÃ§in 0â€“1 arasÄ± (sigmoidâ€™den geldiÄŸi iÃ§in).**\n",
    "\n",
    "### 2) scale neden bÃ¶yle?\n",
    "* scale=(1âˆ’Î±)+Î±â‹…att\n",
    "\n",
    "Bu ÅŸu demek:\n",
    "\n",
    "* alpha = 1 ise: scale = att â†’ klasik attention (direkt maske)\n",
    "\n",
    "* alpha = 0 ise: scale = 1 â†’ attention devre dÄ±ÅŸÄ± (Ã§Ä±ktÄ± = x)\n",
    "\n",
    "Yani bu formÃ¼l aslÄ±nda ÅŸunun aynÄ±sÄ±:\n",
    "\n",
    "* scale=1+Î±â‹…(attâˆ’1)\n",
    "\n",
    "**Attentionâ€™Ä±n â€œ1â€™den ne kadar saptÄ±ÄŸÄ±nÄ±â€ alpha ile ayarlÄ±yoruz.**\n",
    "\n",
    "### 3) Bu alpha niye kondu?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ attention bloklarÄ± bazen:\n",
    "\n",
    "* Ã§ok agresif davranÄ±r\n",
    "\n",
    "* feature stdâ€™sini dÃ¼ÅŸÃ¼rÃ¼r\n",
    "\n",
    "* erken saturate edip Ã¶ÄŸrenmeyi zorlaÅŸtÄ±rÄ±r\n",
    "\n",
    "Alpha:\n",
    "\n",
    "* â€œattention gÃ¼cÃ¼â€ iÃ§in emniyet kemeri.\n",
    "\n",
    "Bu, residual kapÄ± mantÄ±ÄŸÄ±yla aynÄ±:\n",
    "\n",
    "* alpha dÃ¼ÅŸÃ¼k â†’ daha residual, daha gÃ¼venli\n",
    "\n",
    "* alpha yÃ¼ksek â†’ daha attention odaklÄ±, daha riskli ama daha etkili olabilir\n",
    "\n",
    "### KÃ¼Ã§Ã¼k bir Ã¶zet iÃ§in :: :: :: \n",
    "\n",
    "**scale = (1-Î±) + Î±Â·att formÃ¼lÃ¼ attentionâ€™Ä± residual bir karÄ±ÅŸÄ±m olarak uygular; Î±=0 attentionâ€™Ä± kapatÄ±r, Î±=1 tam uygular, 0<Î±<1 etkisini yumuÅŸatÄ±p aÅŸÄ±rÄ± bastÄ±rma riskini azaltÄ±r (Î±>1 ise negatif Ã¶lÃ§ek riski doÄŸurur).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65345533",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "YukarÄ±da bulunan ÅŸu fonksiyon debug izlemek amacÄ±yla yapÄ±lÄ±r.\n",
    "\n",
    "```python \n",
    "@torch.no_grad()\n",
    "def last_mask_stats(self):\n",
    "    if self._last_ah is None or self._last_aw is None:\n",
    "        return None\n",
    "    ah = self._last_ah\n",
    "    aw = self._last_aw\n",
    "    return {\n",
    "        \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "        \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "Bu fonksiyon :: :: :: Forward iÃ§inde Ã¼retilen:\n",
    "\n",
    "* a_h (B,C,H,1)\n",
    "\n",
    "* a_w (B,C,1,W)\n",
    "\n",
    "maskeleri self._last_ah ve self._last_aw iÃ§ine kaydedilmiÅŸti.\n",
    "\n",
    "**Bu fonksiyon da o maskelerin: min,mean,max,std istatistiklerini dÃ¶ndÃ¼rÃ¼yor.**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f640cb9",
   "metadata": {},
   "source": [
    "DiÄŸer bir debug ve test fonksiyonu ise ÅŸudur:\n",
    "```python \n",
    "@torch.no_grad()\n",
    "def coordatt_debug_pass(module, x, name=\"CoordAttDebug\"):\n",
    "    module.eval()\n",
    "    out = module(x)\n",
    "\n",
    "    in_mean, in_std = float(x.mean()), float(x.std())\n",
    "    out_mean, out_std = float(out.mean()), float(out.std())\n",
    "    ratio = out_std / (in_std + 1e-12)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"in shape :\", tuple(x.shape))\n",
    "    print(\"out shape:\", tuple(out.shape))\n",
    "    print(f\"in  mean/std : {in_mean:.6f} / {in_std:.6f}\")\n",
    "    print(f\"out mean/std : {out_mean:.6f} / {out_std:.6f}\")\n",
    "    print(f\"std_ratio(out/in): {ratio:.6f}\")\n",
    "\n",
    "    if hasattr(module, \"last_mask_stats\"):\n",
    "        stats = module.last_mask_stats()\n",
    "        if stats is not None:\n",
    "            print(\"a_h stats:\", stats[\"a_h\"])\n",
    "            print(\"a_w stats:\", stats[\"a_w\"])\n",
    "\n",
    "    return out;\n",
    "```\n",
    "**Coordinate Attention bloÄŸunun davranÄ±ÅŸÄ±nÄ± gÃ¶zlemlemek ve doÄŸrulamak iÃ§in yazÄ±lmÄ±ÅŸ bir yardÄ±mcÄ± debug aracÄ±dÄ±r. EÄŸitim sÃ¼recinin parÃ§asÄ± deÄŸildir. AmaÃ§, bloÄŸun giriÅŸe kÄ±yasla Ã§Ä±kÄ±ÅŸÄ± ne kadar bastÄ±rdÄ±ÄŸÄ±nÄ±, boyutlarÄ± koruyup korumadÄ±ÄŸÄ±nÄ± ve Ã¼rettiÄŸi attention maskelerinin saÄŸlÄ±klÄ± daÄŸÄ±lÄ±mlara sahip olup olmadÄ±ÄŸÄ±nÄ± hÄ±zlÄ±ca kontrol etmektir.**\n",
    "\n",
    "**Fonksiyon, verilen bir girdiyi modÃ¼lden geÃ§irir ve giriÅŸâ€“Ã§Ä±kÄ±ÅŸ arasÄ±ndaki temel istatistikleri (ortalama, standart sapma) hesaplar. Ã–zellikle Ã§Ä±kÄ±ÅŸ standart sapmasÄ±nÄ±n giriÅŸe oranÄ± (std_ratio), attentionâ€™Ä±n sinyali aÅŸÄ±rÄ± bastÄ±rÄ±p bastÄ±rmadÄ±ÄŸÄ±nÄ± anlamak iÃ§in kullanÄ±lÄ±r. Buna ek olarak, modÃ¼l last_mask_stats metodunu destekliyorsa, height ve width yÃ¶nlÃ¼ attention maskelerinin minimum, ortalama, maksimum ve standart sapma deÄŸerlerini raporlar.**\n",
    "\n",
    "**Ã–zetle bu fonksiyon, Coordinate Attention bloÄŸunun stabil, aÅŸÄ±rÄ± agresif olmayan ve beklenen ÅŸekilde Ã§alÄ±ÅŸan bir attention Ã¼retip Ã¼retmediÄŸini geliÅŸtirme ve test aÅŸamasÄ±nda doÄŸrulamak iÃ§in kullanÄ±lan saf bir debug yardÄ±mcÄ± fonksiyonudur.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dcc476",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7f90b96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb83bec5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
