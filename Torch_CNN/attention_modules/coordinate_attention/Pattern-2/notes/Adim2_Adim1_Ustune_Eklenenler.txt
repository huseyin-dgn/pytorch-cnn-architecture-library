ADIM-2 (CoordinateAttPlus) — ADIM-1 ÜSTÜNE EKLENENLER (Özet .txt)
===============================================================

Bu dosya, Adım-1’deki klasik Coordinate Attention akışının üstüne eklenen geliştirmeleri ve nedenlerini özetler.

1) Mean + Max ile Profil Çıkarma (H_profile / W_profile)
--------------------------------------------------------
NE EKLENDİ?
- Klasik CA çoğu zaman yalnızca ortalama (mean pooling) ile H/W profili çıkarır.
- Adım-2’de profil şu şekilde güçlendirildi:
  - H_profili: W boyunca mean + max
  - W_profili: H boyunca mean + max
  - (mean + max) * 0.5 ile birleştirme

NEDEN EKLENDİ?
- Mean: genel dağılımı ve “ortalama enerji”yi taşır.
- Max: en baskın aktivasyonu (salient bölgeyi) taşır.
- İkisini birlikte kullanmak, profil sinyalini daha bilgilendirici yapar ve maskeyi “daha keskin” üretebilir.

2) H ve W için Yönlü Depthwise Konv (Yerel Bağlam)
-------------------------------------------------
NE EKLENDİ?
- H yolu için depthwise: kernel (3×1), padding (1,0), groups=C
- W yolu için depthwise: kernel (1×3), padding (0,1), groups=C

NEDEN EKLENDİ?
- Pooling tek başına kaba özet çıkarır.
- Yönlü (anisotropic) conv ile:
  - H yolunda satır komşulukları,
  - W yolunda sütun komşulukları,
  düşük maliyetle (depthwise) sinyale ek bağlam kazandırır.
- groups=C: her kanal kendi filtresiyle işlenir → attention tarafında “kanal başına ince ayar” yapılır.

3) Dilated Depthwise (Geniş Bağlam / Multi-Scale)
-------------------------------------------------
NE EKLENDİ?
- H için dilated depthwise: (3×1), dilation=(d,1), padding=(d,0)
- W için dilated depthwise: (1×3), dilation=(1,d), padding=(0,d)

NEDEN EKLENDİ?
- Amaç “kanal sayısını artırmak” değil; receptive field’i büyütmektir.
- d=2 iken (3) kernelin efektif boyutu 5’e çıkar:
  etkin = (k−1)*d + 1
- Yerel DW (ince detay) + Dilated DW (uzun menzil) birlikte:
  “hem yakın hem uzak bağlam” yakalanır.

4) Multi-Scale Toplama + 1×1 Channel Mixer (merge)
--------------------------------------------------
NE EKLENDİ?
- Aynı eksende local_dw + dilated_dw çıktıları toplanır.
- Ardından 1×1 conv ile kanal karıştırma yapılır:
  - h_channel_mixer (1×1)
  - w_channel_mixer (1×1)

NEDEN EKLENDİ?
- Depthwise conv kanalları karıştırmaz.
- 1×1 mixer, kanallar arası etkileşim ekler; iki ölçeğin birleşimini “kanal uzayında” yeniden düzenler.
- Bu, maskeye gidecek temsilin ifade gücünü artırır.

5) Shared Bottleneck’i Derinleştirme (C→mid→mid)
------------------------------------------------
NE EKLENDİ?
- Klasik CA’daki tek bottleneck adımı, iki aşamaya çıkarıldı:
  - shared_bottleneck_proj: 1×1, C→mid + norm + act
  - shared_bottleneck_refine: 1×1, mid→mid + norm + act

NEDEN EKLENDİ?
- H ve W concat sonrası ortak temsil (H+W şeridi) daha güçlü işlenir.
- mid uzayı:
  - maliyeti düşürür,
  - karar mekanizmasını stabilize eder,
  - head’lere daha “temiz” ve ayrıştırıcı sinyal verir.
- İki katman + nonlinearity, maskenin üretim kapasitesini artırır.

6) Head Katmanları (mid→C) + hardsigmoid ile Maske
--------------------------------------------------
NE EKLENDİ?
- H head: 1×1, mid→C + hardsigmoid → attn_H (B,C,H,1)
- W head: 1×1, mid→C + hardsigmoid → attn_W (B,C,1,W)

NEDEN EKLENDİ?
- Bottleneck’te karar “mid” kanalda alınır.
- Maske uygulanacağı için tekrar C kanala projeksiyon gerekir.
- hardsigmoid:
  - 0–1 benzeri aralıkta maske üretir,
  - sigmoid’e göre daha ucuz/stabil olabilir.

7) Öğrenilebilir Güç Düğmesi: alpha_h / alpha_w (logit parametre)
-----------------------------------------------------------------
NE EKLENDİ?
- alpha değerleri doğrudan değil “raw/logit” olarak tutulur:
  - raw0 = log(a0/(1-a0)) (sigmoid’in tersi / logit)
  - alpha = sigmoid(alpha_raw) ∈ (0,1)
- learnable_alpha=True ise Parameter; değilse buffer.

NEDEN EKLENDİ?
- Maskeyi direkt uygulamak bazen agresif bastırma yapar.
- alpha ile ölçek şöyle karıştırılır:
  scale = (1−α) + α·attn
- α=0 → identity (1.0), α=1 → tam attention.
- Eğitim başlangıcında stabilite + “attention gücünü öğrenerek açma” sağlar.

8) Global Yumuşatma: beta
-------------------------
NE EKLENDİ?
- Ölçek haritası (scale_hw) 1’e doğru çekilir:
  scale_hw = 1 + beta*(scale_hw−1)

NEDEN EKLENDİ?
- CA maskesi (scale_hw) bazen fazla agresif olabilir.
- beta, tüm attention’ın “sesini kısma/açma” düğmesi gibi çalışır.
- 0<beta<1: daha güvenli, daha az over-suppression riski.

9) Opsiyonel Spatial Gate (ek 2D lokal gate) — use_spatial_gate
---------------------------------------------------------------
NE EKLENDİ?
- Çıkış üstüne ek bir 2D kapı:
  spatial_gate_dw: 3×3 depthwise
  spatial_gate_pw: 1×1
  gate = sigmoid(...)
  gate = 1 + spatial_gate_beta*(gate−1)
  out = out * gate

NEDEN EKLENDİ?
- CA koordinat temelli (H ve W ayrı) maske üretir.
- Spatial gate, klasik “spatial attention” gibi tam 2D (H,W) lokal kontrol ekler.
- Ancak risk: “double-gating” ile over-suppression ihtimali artar.
- Bu yüzden varsayılan kapalı tutulması mantıklıdır.

Kısa Net Özet
-------------
Adım-2, Adım-1’in üstüne şunları ekleyerek maskeyi daha güçlü ve daha kontrollü üretir:
- Profil sinyali güçlendirme (mean+max)
- Yönlü multi-scale (local + dilated) depthwise
- Kanal karıştırma (1×1 mixer)
- Daha güçlü shared bottleneck (C→mid→mid)
- Maskenin şiddetini öğrenilebilir hale getiren alpha (logit) ve global beta
- İsteğe bağlı ek 2D spatial gate

