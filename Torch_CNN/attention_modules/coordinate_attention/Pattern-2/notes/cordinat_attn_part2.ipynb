{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f148a4",
   "metadata": {},
   "source": [
    "---\n",
    "## Bu .ipynb dosyasÄ± 2.adÄ±m iÅŸlemidir.Burda yapÄ±lan iÅŸlemler adÄ±m-1 in Ã¼stÃ¼ne koyularak yapÄ±lacaktÄ±r.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b8cb7",
   "metadata": {},
   "source": [
    "### Ã–nce Kod :: :: AdÄ±m 1'in Ã¼stÃ¼ne neler eklediÄŸimize bu klasÃ¶rden ulaÅŸabilirsiniz. :\n",
    "\n",
    "**\"Attention MekanizmalarÄ±\\Cordinat Attention (3.1)\\AdÄ±m-2\\Bu_adÄ±mda_neler_ekledik.ipynb\"**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class HSwish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3.0, inplace=True) / 6.0\n",
    "# AdÄ±m 1 de olan adÄ±mÄ±n aynÄ±sÄ± uygulandÄ±.Relu6 fonksiyonundan HSwish e uyarlama yaptÄ±k.Daha ucuz ve daha yumuÅŸak davranÄ±r.\n",
    "# Matematik denklemi :: :: :: x * relu(x+3) / 6\n",
    "\n",
    "def make_norm(norm: str, ch: int):\n",
    "    if norm == \"bn\":\n",
    "        return nn.BatchNorm2d(ch)\n",
    "\n",
    "    if norm == \"gn\":\n",
    "        g = min(32, ch)  # GN'de num_groups <= num_channels olmalÄ±. Bu yÃ¼zden en fazla ch, tercihen 32 ile baÅŸlÄ±yoruz.\n",
    "\n",
    "        # GN ÅŸartÄ±: ch % g == 0 olmalÄ± (kanallar gruplara eÅŸit bÃ¶lÃ¼nsÃ¼n).\n",
    "        # Uymuyorsa g'yi ikiye bÃ¶lerek 32â†’16â†’8â†’4â†’2 ÅŸeklinde dÃ¼ÅŸÃ¼rÃ¼p uygun bir bÃ¶len arÄ±yoruz.\n",
    "        while ch % g != 0 and g > 2:\n",
    "            g //= 2  # g'yi yarÄ±ya indir (tam sayÄ± bÃ¶lmesi)\n",
    "\n",
    "        # EÄŸer hÃ¢lÃ¢ bÃ¶lÃ¼nmÃ¼yorsa (Ã¶zellikle tek kanal sayÄ±larÄ±nda), son Ã§are:\n",
    "        # ch Ã§iftse 2 gruba bÃ¶l, tekse 1 grup (tÃ¼m kanallar tek grupta) kullan.\n",
    "        if ch % g != 0:\n",
    "            g = 2 if (ch % 2 == 0) else 1\n",
    "\n",
    "        return nn.GroupNorm(g, ch)\n",
    "\n",
    "    if norm == \"none\":\n",
    "        return nn.Identity()\n",
    "\n",
    "    raise ValueError(\"norm must be 'bn', 'gn', or 'none'\")\n",
    "\n",
    "\n",
    "class CoordinateAttPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        reduction: int = 32,\n",
    "        min_mid_channels: int = 8,\n",
    "        act: str = \"hswish\",\n",
    "        init_alpha: float = 0.7,\n",
    "        learnable_alpha: bool = True,\n",
    "        beta: float = 0.35,\n",
    "        dilation: int = 2,\n",
    "        norm: str = \"gn\",\n",
    "        use_spatial_gate: bool = False,\n",
    "        spatial_gate_beta: float = 0.35,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mid = max(min_mid_channels, in_channels // reduction) \n",
    "        ## BurasÄ± bizim bottleneck ifademiz.AslÄ±nda burda yapÄ±lan ÅŸey;\n",
    "            # H+W birleÅŸik ÅŸerit Ã¼zerinde ortak karar uzayÄ±: Câ†’mid sÄ±kÄ±ÅŸtÄ±rma + midâ†’mid refine.â€\n",
    "\n",
    "        self.shared_bottleneck_proj = nn.Conv2d(in_channels, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_norm = make_norm(norm, mid)\n",
    "        self.shared_bottleneck_refine = nn.Conv2d(mid, mid, 1, bias=False)\n",
    "        self.shared_bottleneck_refine_norm = make_norm(norm, mid)\n",
    "        ## BurasÄ± bottlenecek mimarisi.Åu an katman karÅŸÄ±lÄ±ÄŸÄ±\n",
    "            # in_channels -> mid -> mid \n",
    "                # DÄ°KKAT :: Burda henÃ¼z in_channels ile bir baÄŸlantÄ± yapmadÄ±k.Bu sonraki adÄ±m.O adÄ±mÄ± aÅŸaÄŸÄ±da attention mekanizmasÄ±nda gÃ¶receÄŸiz\n",
    "\n",
    "        if act.lower() == \"hswish\":\n",
    "            self.act = HSwish()\n",
    "        elif act.lower() == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'hswish' or 'relu'\")\n",
    "        \n",
    "        self.h_local_dw = nn.Conv2d(in_channels, in_channels, (3, 1), padding=(1, 0), groups=in_channels, bias=False)\n",
    "        ## H iÃ§in yapÄ±lan depthwise conv iÅŸlemi.YalnÄ±ca H olduÄŸu iÃ§in :\n",
    "            # Kernel (3,1) :: Yani H kanalÄ±na dikkat Ã§ektiriyoruz.DiÄŸer taraftan:\n",
    "                # Padding ifadesinde ise yalnzÄ±ca H eksenine padding yapÄ±yoruz.W ekseni gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi 0.\n",
    "\n",
    "        self.w_local_dw = nn.Conv2d(in_channels, in_channels, (1, 3), padding=(0, 1), groups=in_channels, bias=False)\n",
    "        ## W iÃ§in yapÄ±lan depthwise conv iÅŸlemidir.YalnÄ±zca W ye uygulanÄ±r :\n",
    "            # Kernel (1,3) olduÄŸundan H eksenine deÄŸil W eksenine odaklanÄ±r.\n",
    "                # Padding de aynÄ± ÅŸekilde.YalnÄ±zca W eksenine yapÄ±lÄ±r.\n",
    "\n",
    "\n",
    "        d = int(dilation)\n",
    "        self.h_dilated_dw = nn.Conv2d(in_channels,in_channels,(3, 1),padding=(d, 0),dilation=(d, 1),groups=in_channels,bias=False,)\n",
    "        # H iÃ§in yapÄ±lan dilated convdur.YapÄ± ÅŸudur:\n",
    "            # Yine 3,1 kernel_size kullanÄ±lÄ±yor.DiÄŸer Tarafdan pad deÄŸeri dilation ile aynÄ±.\n",
    "                # Dilation da ise H deÄŸerine d veriyoruz.W boyutuna ise 1.Yani amacÄ±mÄ±z sadece H'a uygulamak\n",
    "\n",
    "        \n",
    "        self.w_dilated_dw = nn.Conv2d(in_channels,in_channels,(1, 3),padding=(0, d),dilation=(1, d),groups=in_channels,bias=False,)\n",
    "        # YukarÄ±da olduÄŸu gibi.W iÃ§in yapÄ±lan iÅŸlemdir.\n",
    "\n",
    "        self.h_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        # H kanalÄ±ndan gelen dilated ve depthwise iÅŸlemini burda 1x1 e sokuyoruz.\n",
    "            # AmaÃ§ dilated ve depthwise dan gelen kanallarÄ± 1x1 conv'a koymak.\n",
    "                # Depthwise Ã§Ä±ktÄ±larÄ±ndan sonra channel mixing (kanallar arasÄ± etkileÅŸim) eklemek iÃ§in 1Ã—1.\n",
    "\n",
    "        self.w_channel_mixer = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        # W kanalÄ±ndan gelen bilgilerin de aynÄ±sÄ± burda iÅŸlenir.\n",
    "\n",
    "        self.h_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        # H ekseninine attention Ä±n asÄ±l uygulandÄ±ÄŸÄ± katmandÄ±r.Yani aslÄ±nda dikkat kÄ±smÄ±nÄ± bÃ¼tÃ¼n channels'e gÃ¶nderiyoruz\n",
    "            # Yani Ã¶rnekle mid -> channels === 8 -> 256 \n",
    "                # AslÄ±nda karar mekanizmasÄ±na ÅŸunu dedik. \n",
    "                    # Bu headâ€™ler mid â†’ C projeksiyonu yapar.Sonra hardsigmoid ile maskeyi Ã¼retir.\n",
    "\n",
    "        self.w_attention_head = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        # W eksenindeki iÅŸlemi de burda yapÄ±yoruz.\n",
    "\n",
    "        self.beta = float(beta)\n",
    "        # Attention Ä±n 2.kapÄ±sÄ±dÄ±r.Yine attention kontrol edilir.Fakat eksen olarak deÄŸil bÃ¼tÃ¼n attention kontrol edilir.\n",
    "        # beta, final scale_hwâ€™yi 1â€™e yaklaÅŸtÄ±rarak agresifliÄŸi kÄ±sar.Yani â€œglobal gain / dampingâ€\n",
    "        \n",
    "        eps = 1e-6\n",
    "        a0 = float(init_alpha)\n",
    "            # --- alpha baÅŸlangÄ±cÄ± (logit uzayÄ±) ---\n",
    "        # AmaÃ§: alpha deÄŸerini (0,1) aralÄ±ÄŸÄ±nda tutmak ama Ã¶ÄŸrenmeyi serbest bÄ±rakmak.\n",
    "        # Bunun iÃ§in alpha'yÄ± doÄŸrudan Ã¶ÄŸrenmek yerine \"alpha_raw\" (logit) parametresini Ã¶ÄŸreniyoruz:\n",
    "        #   alpha = sigmoid(alpha_raw)  -> (0,1)\n",
    "        # init_alpha baÅŸlangÄ±Ã§ gÃ¼cÃ¼dÃ¼r (Ã¶r. 0.7). 0 veya 1 olursa logit patlar, o yÃ¼zden eps ile clamp ediyoruz.\n",
    "        #   a0 âˆˆ (eps, 1-eps)\n",
    "        # raw0 = log(a0 / (1-a0))  -> sigmoid(raw0) = a0\n",
    "        # Yani alpha_raw'Ä± raw0 ile baÅŸlatÄ±nca, ileri geÃ§iÅŸte alpha ilk baÅŸta tam init_alpha olur.\n",
    "        a0 = min(max(a0, eps), 1.0 - eps)\n",
    "        t = torch.tensor(a0)\n",
    "        raw0 = torch.log(t / (1.0 - t))\n",
    "\n",
    "        # learnable_alpha=True ise alpha_raw Ã¶ÄŸrenilir (Parameter).\n",
    "        # False ise sabit tutulur (buffer): eÄŸitim boyunca deÄŸiÅŸmez, sadece kayÄ±t/cihaz taÅŸÄ±nÄ±r.\n",
    "        if learnable_alpha:\n",
    "            self.alpha_h_raw = nn.Parameter(raw0.clone())\n",
    "            self.alpha_w_raw = nn.Parameter(raw0.clone())\n",
    "        else:\n",
    "            self.register_buffer(\"alpha_h_raw\", raw0.clone())\n",
    "            self.register_buffer(\"alpha_w_raw\", raw0.clone())\n",
    "\n",
    "\n",
    "        self.use_spatial_gate = bool(use_spatial_gate)\n",
    "        self.spatial_gate_beta = float(spatial_gate_beta)\n",
    "        if self.use_spatial_gate:\n",
    "            self.spatial_gate_dw = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False)\n",
    "            self.spatial_gate_pw = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        # --- Opsiyonel Spatial Gate (ek 2D lokal kapÄ±) ---\n",
    "        # use_spatial_gate=True ise, CA ile Ã¶lÃ§eklenmiÅŸ Ã§Ä±ktÄ± Ã¼zerine bir de 2D (H,W) tabanlÄ± gate ekleriz.\n",
    "        # spatial_gate_dw: 3x3 depthwise -> lokal komÅŸuluk bilgisi (kanal kanal)\n",
    "        # spatial_gate_pw: 1x1 pointwise -> kanal karÄ±ÅŸtÄ±rma / yeniden Ã¶lÃ§ekleme\n",
    "        # Bu gate sigmoid ile (0,1) aralÄ±ÄŸÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±lÄ±p beta ile 1'e yaklaÅŸtÄ±rÄ±larak \"yumuÅŸatÄ±lÄ±r\".\n",
    "        # Not: Hata/bug deÄŸildir; bazÄ± senaryolarda fayda verir, bazÄ± senaryolarda double-gating yÃ¼zÃ¼nden over-suppression yapabilir.\n",
    "\n",
    "\n",
    "        self._last_ah = None\n",
    "        self._last_aw = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # GiriÅŸ shape'lerini al (B: batch, C: kanal, H: yÃ¼kseklik, W: geniÅŸlik)\n",
    "\n",
    "        H_profile = 0.5 * (x.mean(dim=3, keepdim=True) + x.amax(dim=3, keepdim=True))\n",
    "        # W eksenini (dim=3) squeeze ederek (B,C,H,1) H-profili Ã§Ä±kar: mean + max karÄ±ÅŸÄ±mÄ± (0.5 ile ortalama)\n",
    "\n",
    "        W_profile = 0.5 * (x.mean(dim=2, keepdim=True) + x.amax(dim=2, keepdim=True))\n",
    "        # H eksenini (dim=2) squeeze ederek (B,C,1,W) W-profili Ã§Ä±kar: mean + max karÄ±ÅŸÄ±mÄ± (0.5 ile ortalama)\n",
    "\n",
    "        H_multi_scale = self.h_channel_mixer(self.h_local_dw(H_profile) + self.h_dilated_dw(H_profile))\n",
    "        # H-profili Ã¼zerinde local DW + dilated DW Ã§Ä±ktÄ±sÄ±nÄ± topla (multi-scale), sonra 1x1 ile channel mixing yap -> (B,C,H,1)\n",
    "\n",
    "        W_multi_scale = self.w_local_dw(W_profile) + self.w_dilated_dw(W_profile)\n",
    "        # W-profili Ã¼zerinde local DW + dilated DW Ã§Ä±ktÄ±sÄ±nÄ± topla (multi-scale) -> (B,C,1,W)\n",
    "\n",
    "        W_multi_scale = self.w_channel_mixer(W_multi_scale)\n",
    "        # Depthwise sonrasÄ± kanallarÄ± karÄ±ÅŸtÄ±rmak iÃ§in 1x1 channel mixer uygula -> (B,C,1,W)\n",
    "\n",
    "        W_multi_scale = W_multi_scale.permute(0, 1, 3, 2)\n",
    "        # Concat iÃ§in formatÄ± deÄŸiÅŸtir: (B,C,1,W) -> (B,C,W,1)\n",
    "\n",
    "        hw_concat = torch.cat([H_multi_scale, W_multi_scale], dim=2)\n",
    "        # H ve W ÅŸeritlerini dim=2 boyunca yan yana ekle: (B,C,H,1)+(B,C,W,1) -> (B,C,H+W,1)\n",
    "\n",
    "        shared_mid = self.act(self.shared_bottleneck_norm(self.shared_bottleneck_proj(hw_concat)))\n",
    "        # Shared bottleneck 1. adÄ±m: kanal projeksiyonu C->mid + norm + aktivasyon -> (B,mid,H+W,1)\n",
    "\n",
    "        shared_mid = self.act(self.shared_bottleneck_refine_norm(self.shared_bottleneck_refine(shared_mid)))\n",
    "        # Shared bottleneck 2. adÄ±m: mid->mid refine + norm + aktivasyon -> (B,mid,H+W,1)\n",
    "\n",
    "        mid_H, mid_W = torch.split(shared_mid, [H, W], dim=2)\n",
    "        # H+W uzunluÄŸunu tekrar ayÄ±r: ilk H parÃ§a (B,mid,H,1), ikinci W parÃ§a (B,mid,W,1)\n",
    "\n",
    "        mid_W = mid_W.permute(0, 1, 3, 2)\n",
    "        # W parÃ§asÄ±nÄ± head iÃ§in formata sok: (B,mid,W,1) -> (B,mid,1,W)\n",
    "\n",
    "        attn_H = F.hardsigmoid(self.h_attention_head(mid_H), inplace=False)# Bu, maskenin kendisini Ã¼retmek iÃ§in.\n",
    "        # H head: mid->C projeksiyonu ve hardsigmoid ile (0,1) benzeri H maskesi Ã¼ret -> (B,C,H,1)\n",
    "\n",
    "        attn_W = F.hardsigmoid(self.w_attention_head(mid_W), inplace=False)# Bu, maskenin kendisini Ã¼retmek iÃ§in.\n",
    "        # W head: mid->C projeksiyonu ve hardsigmoid ile (0,1) benzeri W maskesi Ã¼ret -> (B,C,1,W)\n",
    "\n",
    "        self._last_ah = attn_H.detach()\n",
    "        # Debug/istatistik iÃ§in H maskesinin kopyasÄ±nÄ± graph'tan ayÄ±rÄ±p sakla\n",
    "\n",
    "        self._last_aw = attn_W.detach()\n",
    "        # Debug/istatistik iÃ§in W maskesinin kopyasÄ±nÄ± graph'tan ayÄ±rÄ±p sakla\n",
    "\n",
    "\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw)# Bu ise maskeyi deÄŸil, maskenin gÃ¼cÃ¼nÃ¼ ayarlayan karÄ±ÅŸÄ±m katsayÄ±sÄ±nÄ± Ã¼retmek iÃ§in.\n",
    "        # Bu alpha deÄŸerleri normalde -inf , inf aralÄ±ÄŸÄ±ndadÄ±r.Biz sigmoid ile bunlarÄ± 0,1 aralÄ±ÄŸÄ±na sÄ±kÄ±ÅŸtÄ±rdÄ±k.                                                  \n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw)# Bu ise maskeyi deÄŸil, maskenin gÃ¼cÃ¼nÃ¼ ayarlayan karÄ±ÅŸÄ±m katsayÄ±sÄ±nÄ± Ã¼retmek iÃ§in.                                                        \n",
    "\n",
    "        scale_H = (1.0 - alpha_h) + alpha_h * attn_H  # Burda dierktr scale matematiÄŸini kulanadÄ±k :: scale=(1âˆ’Î±)+Î±â‹…attn                                                   \n",
    "        scale_W = (1.0 - alpha_w) + alpha_w * attn_W                                                         \n",
    "\n",
    "        scale_hw = scale_H * scale_W\n",
    "         # Bir nokta ancak hem satÄ±rÄ± hem sÃ¼tunu Ã¶nemliyse gÃ¼Ã§lÃ¼ kalÄ±r.â€œÄ°ki koÅŸul birlikte saÄŸlansÄ±nâ€ mantÄ±ÄŸÄ±.      \n",
    "         # (B,C,H,1) * (B,C,1,W) â†’ (B,C,H,W)\n",
    "                                                                       \n",
    "        scale_hw = 1.0 + self.beta * (scale_hw - 1.0)                                                        \n",
    "\n",
    "        out = x * scale_hw # her kanalÄ±n her pikseli, kendi Ã¶lÃ§eÄŸiyle Ã§arpÄ±lÄ±yor.                                                                               \n",
    "\n",
    "        if self.use_spatial_gate:# Bu blok, CAâ€™den sonra Ã§Ä±ktÄ±ya ek bir 2D lokal kapÄ± uygular; bazen faydalÄ±, bazen fazla bastÄ±rÄ±p performansÄ± bozabilir.EÄŸer kullanÄ±lacaksa dikkat edilmeli.\n",
    "            gate = torch.sigmoid(self.spatial_gate_pw(self.spatial_gate_dw(out)))                            \n",
    "            gate = 1.0 + self.spatial_gate_beta * (gate - 1.0)                                              \n",
    "            out = out * gate                                                                                 \n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def last_mask_stats(self):\n",
    "        if self._last_ah is None or self._last_aw is None:\n",
    "            return None\n",
    "        ah = self._last_ah\n",
    "        aw = self._last_aw\n",
    "        return {\n",
    "            \"a_h\": {\"min\": float(ah.min()), \"mean\": float(ah.mean()), \"max\": float(ah.max()), \"std\": float(ah.std())},\n",
    "            \"a_w\": {\"min\": float(aw.min()), \"mean\": float(aw.mean()), \"max\": float(aw.max()), \"std\": float(aw.std())},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4dc69",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Kodun iÃ§erisindeki yorum satÄ±rlarÄ±nÄ± okuyunuz.EÄŸer anlam karmaÅŸasÄ± yaÅŸarsanÄ±z aÅŸaÄŸÄ±daki ifadelerden daha net anlam Ã§Ä±kartabilirsiniz.DeÄŸiÅŸken isimleri deÄŸiÅŸebilir.Ama siz ana iÅŸleme odaklanÄ±nÄ±z....\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c9e06",
   "metadata": {},
   "source": [
    "# `make_norm()` Ne YapÄ±yor?\n",
    "\n",
    "Bu fonksiyonun tek amacÄ±: **Normalization katmanÄ±nÄ± seÃ§mek**.\n",
    "\n",
    "- `norm=\"bn\"` â†’ `BatchNorm2d(ch)`\n",
    "- `norm=\"gn\"` â†’ `GroupNorm(g, ch)` (g otomatik seÃ§ilir)\n",
    "- `norm=\"none\"` â†’ `Identity()` (hiÃ§ normalize etmez)\n",
    "\n",
    "\n",
    "\n",
    "## BatchNorm (BN) vs GroupNorm (GN) â€” Temel Fark\n",
    "\n",
    "### BatchNorm2d (BN)\n",
    "**NasÄ±l Ã§alÄ±ÅŸÄ±r?**\n",
    "- Normalize iÅŸlemini **batch boyutunu da dahil ederek** yapar.\n",
    "- Her kanal iÃ§in istatistikleri (mean/var) **batch Ã¼zerinden** hesaplar.\n",
    "\n",
    "**ArtÄ±larÄ±**\n",
    "- BÃ¼yÃ¼k batch ile genellikle iyi Ã§alÄ±ÅŸÄ±r.\n",
    "- EÄŸitimde hÄ±zlÄ± ve stabil olabilir (uygun batch boyutunda).\n",
    "\n",
    "**Eksileri**\n",
    "- **Small batch** (Ã¶zellikle detection / segmentation gibi) durumlarda istatistikler â€œgÃ¼rÃ¼ltÃ¼lÃ¼â€ olur â†’ performans ve stabilite dÃ¼ÅŸebilir.\n",
    "- Train/test farkÄ±: trainâ€™de batch istatistiÄŸi, testâ€™te running mean/var kullanÄ±r.\n",
    "\n",
    "**Ne zaman mantÄ±klÄ±?**\n",
    "- Batch boyutu yeterince bÃ¼yÃ¼kse (Ã¶r. classification eÄŸitimleri).\n",
    "\n",
    "\n",
    "### GroupNorm (GN)\n",
    "**NasÄ±l Ã§alÄ±ÅŸÄ±r?**\n",
    "- Normalize iÅŸlemini **batchâ€™e bakmadan** yapar.\n",
    "- KanallarÄ± `g` adet gruba bÃ¶ler, her grup kendi iÃ§inde normalize edilir.\n",
    "- Bu yÃ¼zden **batch boyutundan baÄŸÄ±msÄ±z** daha stabil davranÄ±r.\n",
    "\n",
    "**ArtÄ±larÄ±**\n",
    "- Small batchâ€™te bile stabil.\n",
    "- Detection gibi kÃ¼Ã§Ã¼k batch kullanÄ±lan iÅŸlerde sÄ±k tercih edilir.\n",
    "\n",
    "**Eksileri**\n",
    "- BazÄ± durumlarda BN kadar hÄ±zlÄ± â€œoturmayabilirâ€ (tamamen probleme baÄŸlÄ±).\n",
    "\n",
    "**Ne zaman mantÄ±klÄ±?**\n",
    "- Batch kÃ¼Ã§Ã¼kse veya batch deÄŸiÅŸkenliÄŸi fazlaysa (Ã¶zellikle detection).\n",
    "\n",
    "\n",
    "## GroupNormâ€™da Kritik Åart: `ch % g == 0`\n",
    "\n",
    "GroupNorm ÅŸu ÅŸekilde tanÄ±mlanÄ±r:\n",
    "\n",
    "- `GroupNorm(num_groups=g, num_channels=ch)`\n",
    "\n",
    "Ama **kanallar gruplara eÅŸit bÃ¶lÃ¼nebilmek zorunda** olduÄŸu iÃ§in:\n",
    "\n",
    "> `ch % g == 0` olmalÄ±.\n",
    "\n",
    "Yani `g`, `ch`â€™nin tam bÃ¶leni olmalÄ±.\n",
    "\n",
    "\n",
    "## `gn` DalÄ±: `g` NasÄ±l SeÃ§iliyor?\n",
    "\n",
    "Kod:\n",
    "\n",
    "```python\n",
    "g = min(32, ch)\n",
    "while ch % g != 0 and g > 2:\n",
    "    g //= 2\n",
    "if ch % g != 0:\n",
    "    g = 2 if (ch % 2 == 0) else 1\n",
    "return nn.GroupNorm(g, ch)\n",
    "```\n",
    "\n",
    "### Mini Ã–rnekler\n",
    "\n",
    "* ch = 120\n",
    "\n",
    "* g=32 â†’ 120%32â‰ 0 â†’ g=16 â†’ 120%16â‰ 0 â†’ g=8 â†’ 120%8=0\n",
    "\n",
    "* SonuÃ§: GroupNorm(8, 120)\n",
    "\n",
    "--##--##--##--##--\n",
    "\n",
    "* ch = 122 (Ã§ift ama 8â€™e bÃ¶lÃ¼nmez)\n",
    "\n",
    "* g=32 â†’ 122%32â‰ 0 â†’ g=16 â†’ 122%16â‰ 0 â†’ g=8 â†’ 122%8â‰ 0 â†’ g=4 â†’ 122%4â‰ 0 â†’ g=2\n",
    "\n",
    "* 122%2=0, if Ã§alÄ±ÅŸmaz\n",
    "\n",
    "* SonuÃ§: GroupNorm(2, 122)\n",
    "\n",
    "--##--##--##--##--\n",
    "\n",
    "- ch = 121 (tek sayÄ±)\n",
    "\n",
    "* g=32 â†’ 16 â†’ 8 â†’ 4 â†’ 2 (while biter)\n",
    "\n",
    "* 121%2â‰ 0 â†’ if devreye girer â†’ tek olduÄŸu iÃ§in g=1\n",
    "\n",
    "- SonuÃ§: GroupNorm(1, 121)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc9510",
   "metadata": {},
   "source": [
    "--------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab19f347",
   "metadata": {},
   "source": [
    "# Shared Bottleneck Nedir? (Bu KÄ±sÄ±mda Ne YapÄ±lÄ±yor?)\n",
    "\n",
    "```python \n",
    "        self.conv1a = nn.Conv2d(in_channels, mid, 1, bias=False)\n",
    "        self.norm1a = make_norm(norm, mid)\n",
    "        self.conv1b = nn.Conv2d(mid, mid, 1, bias=False)\n",
    "        self.norm1b = make_norm(norm, mid)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "AÅŸaÄŸÄ±daki yapÄ±, **Coordinate Attention iÃ§indeki ortak (shared) bottleneck** kÄ±smÄ±dÄ±r:\n",
    "\n",
    "- Bu katmanlar **attention maskesi Ã¼retilmeden Ã¶nce**\n",
    "- H (yÃ¼kseklik) ve W (geniÅŸlik) bilgisini **birlikte iÅŸleyen**\n",
    "- Maskenin neye bakacaÄŸÄ±nÄ± belirleyen **en kritik karar bloÄŸudur**\n",
    "\n",
    "\n",
    "\n",
    "## Klasik Coordinate Attentionâ€™da Bottleneck MantÄ±ÄŸÄ±\n",
    "\n",
    "Standart CA tasarÄ±mÄ±nda genellikle ÅŸu vardÄ±r:\n",
    "\n",
    "- BirleÅŸtirilmiÅŸ H+W bilgisi\n",
    "- **Tek bir 1Ã—1 convolution**\n",
    "- Aktivasyon\n",
    "- Sonra H ve W iÃ§in ayrÄ± headâ€™ler\n",
    "\n",
    "Bu yapÄ±:\n",
    "- Hafiftir\n",
    "- Ama temsil gÃ¼cÃ¼ (capacity) sÄ±nÄ±rlÄ±dÄ±r\n",
    "\n",
    "Yani:\n",
    "> â€œMaskeyi Ã¼retmeden Ã¶nce Ã¶zellikleri tek bir doÄŸrusal dÃ¶nÃ¼ÅŸÃ¼mden geÃ§iririm.â€\n",
    "\n",
    "\n",
    "## Bu Kodda YapÄ±lan Åey: Bottleneckâ€™i DerinleÅŸtirmek\n",
    "\n",
    "Bu yapÄ±da bottleneck **tek katman deÄŸil**, iki katmanlÄ±dÄ±r:\n",
    "\n",
    "- Ä°lk katman: kanalÄ± dÃ¼ÅŸÃ¼rÃ¼r (`C â†’ mid`)\n",
    "- Ä°kinci katman: `mid` uzayÄ±nda tekrar iÅŸler (`mid â†’ mid`)\n",
    "- Her iki katmandan sonra:\n",
    "  - Normalization\n",
    "  - Nonlinearity (HSwish / ReLU)\n",
    "\n",
    "Bu tasarÄ±m ÅŸunu saÄŸlar:\n",
    "\n",
    "> Attention maskesi Ã¼retilmeden Ã¶nce, daha zengin ve daha ayrÄ±ÅŸtÄ±rÄ±cÄ± bir ortak temsil Ã¶ÄŸrenilir.\n",
    "\n",
    "\n",
    "## Neden â€œSharedâ€ Bottleneck Deniyor?\n",
    "\n",
    "- H ve W yollarÄ± **ayrÄ±lmadan Ã¶nce**\n",
    "- AynÄ± dÃ¶nÃ¼ÅŸÃ¼mlerden geÃ§er\n",
    "- Yani:\n",
    "  - YÃ¼kseklik bilgisi ve geniÅŸlik bilgisi\n",
    "  - AynÄ± â€œkarar mekanizmasÄ±â€ tarafÄ±ndan iÅŸlenir\n",
    "\n",
    "Bu, Coordinate Attentionâ€™Ä±n temel fikrine sadÄ±ktÄ±r:\n",
    "> â€œKoordinatlar ayrÄ±, ama karar ortak.â€\n",
    "\n",
    "\n",
    "## Bu Neden Daha GÃ¼Ã§lÃ¼?\n",
    "\n",
    "### 1) Daha Fazla Nonlinearity\n",
    "- Tek katman â†’ tek karar yÃ¼zeyi\n",
    "- Ä°ki katman â†’ daha karmaÅŸÄ±k iliÅŸkiler Ã¶ÄŸrenilebilir\n",
    "\n",
    "Bu Ã¶zellikle ÅŸurada iÅŸe yarar:\n",
    "- Mean + max pooling\n",
    "- Depthwise + dilated conv\n",
    "gibi zengin ama karmaÅŸÄ±k sinyallerden anlamlÄ± maske Ã¼retirken.\n",
    "\n",
    "\n",
    "\n",
    "### 2) Mid UzayÄ±nda Ek Ä°ÅŸleme\n",
    "- `mid` kanalÄ± bir â€œara temsilâ€dir\n",
    "- Bu uzayda ekstra dÃ¶nÃ¼ÅŸÃ¼m yapmak:\n",
    "  - GÃ¼rÃ¼ltÃ¼yÃ¼ ayÄ±klamaya\n",
    "  - H/W bilgisini daha anlamlÄ± sÄ±kÄ±ÅŸtÄ±rmaya\n",
    "yardÄ±mcÄ± olur\n",
    "\n",
    "\n",
    "### 3) Headâ€™ler Ä°Ã§in Daha Temiz Girdi\n",
    "Sonraki adÄ±mda:\n",
    "- H ve W tekrar ayrÄ±lÄ±r\n",
    "- AyrÄ± headâ€™lerle kanal sayÄ±sÄ±na geri projekte edilir\n",
    "\n",
    "Bu bottleneck ne kadar iyi olursa:\n",
    "- `a_h` ve `a_w` maskeleri o kadar anlamlÄ± olur\n",
    "\n",
    "## KÄ±saca Ã–zet\n",
    "\n",
    "Bu blok:\n",
    "\n",
    "- Convolutionâ€™larÄ± â€œdaha iyiâ€ yapmak iÃ§in deÄŸil\n",
    "- **Attention maskesini Ã¼retmeden Ã¶nceki ortak karar mekanizmasÄ±nÄ± gÃ¼Ã§lendirmek** iÃ§in var\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc0c7e",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Bu Katmanlar Ne Ä°ÅŸe YarÄ±yor? (Depthwise â€œYÃ¶nlÃ¼â€ KonvolÃ¼syon)\n",
    "\n",
    "```python\n",
    "        self.dw_h_d1 = nn.Conv2d(in_channels, in_channels, (3, 1), padding=(1, 0), groups=in_channels, bias=False)\n",
    "        self.dw_w_d1 = nn.Conv2d(in_channels, in_channels, (1, 3), padding=(0, 1), groups=in_channels, bias=False)\n",
    "```\n",
    "\n",
    "\n",
    "Bu iki katman, Coordinate Attentionâ€™Ä±n **H (yÃ¼kseklik)** ve **W (geniÅŸlik)** yÃ¶nlerinden Ã§Ä±kardÄ±ÄŸÄ± Ã¶zet sinyali daha â€œanlamlÄ±â€ hale getirmek iÃ§in var.\n",
    "\n",
    "Buradaki temel fikir ÅŸu:\n",
    "\n",
    "> â€œPooling ile elde ettiÄŸim 1D (tek eksenli) sinyali, kÃ¼Ã§Ã¼k bir konvolÃ¼syonla iÅŸleyip yerel baÄŸlam (local context) ekleyeyim.â€\n",
    "\n",
    "\n",
    "\n",
    "## Neden `(3,1)` ve `(1,3)`?\n",
    "\n",
    "Bu Ã§ekirdekler **yÃ¶nlÃ¼ (anisotropic) filtrelerdir**:\n",
    "\n",
    "### `(3,1)` â†’ Dikey yÃ¶nde (H boyunca) bakar\n",
    "- YÃ¼kseklik ekseninde komÅŸu satÄ±rlarÄ± gÃ¶rÃ¼r\n",
    "- GeniÅŸlik ekseninde ise tek kolonda kaldÄ±ÄŸÄ± iÃ§in yatay karÄ±ÅŸma yapmaz\n",
    "\n",
    "Bu yÃ¼zden:\n",
    "- **H yolundaki sinyali** â€œdikey komÅŸulukâ€ bilgisiyle zenginleÅŸtirir\n",
    "\n",
    "\n",
    "\n",
    "### `(1,3)` â†’ Yatay yÃ¶nde (W boyunca) bakar\n",
    "- GeniÅŸlik ekseninde komÅŸu sÃ¼tunlarÄ± gÃ¶rÃ¼r\n",
    "- YÃ¼kseklik ekseninde tek satÄ±rda kaldÄ±ÄŸÄ± iÃ§in dikey karÄ±ÅŸma yapmaz\n",
    "\n",
    "Bu yÃ¼zden:\n",
    "- **W yolundaki sinyali** â€œyatay komÅŸulukâ€ bilgisiyle zenginleÅŸtirir\n",
    "\n",
    "\n",
    "\n",
    "## Neden â€œDepthwiseâ€ (groups = in_channels)?\n",
    "\n",
    "Bu katmanlar **depthwise convolution** olarak Ã§alÄ±ÅŸÄ±r:\n",
    "\n",
    "- Her kanal **kendi baÅŸÄ±na** filtrelenir\n",
    "- Kanallar birbirine karÄ±ÅŸmaz\n",
    "\n",
    "Bu ÅŸu anlama gelir:\n",
    "\n",
    "### ArtÄ± tarafÄ±\n",
    "- Ã‡ok ucuz (parametre ve FLOP dÃ¼ÅŸÃ¼k)\n",
    "- Kanal baÅŸÄ±na â€œince ayarâ€ yapar\n",
    "- Attention tarafÄ±nda gÃ¼rÃ¼ltÃ¼ eklemeden sinyali yumuÅŸatÄ±r / ÅŸekillendirir\n",
    "\n",
    "### Eksi tarafÄ±\n",
    "- Kanal karÄ±ÅŸÄ±mÄ± yoktur  \n",
    "  (Bu eksik daha sonra genelde 1Ã—1 â€œmergeâ€ ile kapatÄ±lÄ±r.)\n",
    "\n",
    "\n",
    "## Neden Padding Var?\n",
    "\n",
    "- `(3,1)` iÃ§in padding `(1,0)`:\n",
    "  - H boyutu korunur (3â€™lÃ¼k filtre Hâ€™de kÃ¼Ã§Ã¼ltmesin diye)\n",
    "- `(1,3)` iÃ§in padding `(0,1)`:\n",
    "  - W boyutu korunur\n",
    "\n",
    "AmaÃ§:\n",
    "> Bu iÅŸlemlerden sonra H/W Ã¶zetlerinin boyutu deÄŸiÅŸmesin, aynÄ± uzunlukta kalsÄ±n.\n",
    "\n",
    "\n",
    "\n",
    "## Bu TasarÄ±mÄ±n Coordinate Attentionâ€™a KatkÄ±sÄ±\n",
    "\n",
    "Klasik CAâ€™da H ve W Ã¶zetleri Ã§oÄŸu zaman sadece poolingâ€™den gelir.  \n",
    "Pooling tek baÅŸÄ±na bazen â€œfazla kabaâ€ kalÄ±r.\n",
    "\n",
    "Burada yapÄ±lan geliÅŸtirme:\n",
    "\n",
    "- Pooling ile gelen **1D Ã¶zet sinyal**\n",
    "- kÃ¼Ã§Ã¼k, yÃ¶nlÃ¼, depthwise filtreyle iÅŸlenir\n",
    "- bÃ¶ylece â€œyakÄ±n komÅŸuluk iliÅŸkileriâ€ maskeye yansÄ±r\n",
    "\n",
    "Ã–zetle:\n",
    "\n",
    "> â€œMaskeyi Ã¼retmeden Ã¶nce H ve W sinyalini kanal bazÄ±nda, dÃ¼ÅŸÃ¼k maliyetle, yÃ¶nlÃ¼ olarak zenginleÅŸtiriyoruz.â€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60951c99",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aacec3",
   "metadata": {},
   "source": [
    "# Burada Ne YapÄ±yoruz?\n",
    "\n",
    "```python\n",
    "        d = int(dilation)\n",
    "        self.dw_h_d2 = nn.Conv2d(in_channels, in_channels, (3, 1), padding=(d, 0), dilation=(d, 1), groups=in_channels, bias=False)\n",
    "        self.dw_w_d2 = nn.Conv2d(in_channels, in_channels, (1, 3), padding=(0, d), dilation=(1, d), groups=in_channels, bias=False)\n",
    "```\n",
    "\n",
    "- Zaten `(3,1)` ve `(1,3)` ile **yÃ¶nlÃ¼ depthwise** filtreler vardÄ± (yakÄ±n komÅŸuluk).\n",
    "- Åimdi bunlarÄ±n bir de **dilationâ€™lÄ±** versiyonunu ekliyoruz:\n",
    "  - H yÃ¶nÃ¼ iÃ§in: `(3,1)` ama satÄ±rlar arasÄ±nda â€œatlayarakâ€ bakÄ±yor\n",
    "  - W yÃ¶nÃ¼ iÃ§in: `(1,3)` ama sÃ¼tunlar arasÄ±nda â€œatlayarakâ€ bakÄ±yor\n",
    "\n",
    "Bu yÃ¼zden toplamda 4 conv var:\n",
    "- H: normal depthwise + dilated depthwise\n",
    "- W: normal depthwise + dilated depthwise\n",
    "\n",
    "Ama amaÃ§ â€œkanalÄ± geniÅŸletmekâ€ deÄŸil.\n",
    "AmaÃ§: **receptive fieldâ€™i bÃ¼yÃ¼tmek** (yani daha geniÅŸ baÄŸlam gÃ¶rmek).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de681fe",
   "metadata": {},
   "source": [
    "## â€œ3Ã—1â€™i 5Ã—1 mi yaptÄ±k?â€ ifadesini inceleyelim\n",
    "\n",
    "KÄ±smen evet, ama doÄŸru ifade ÅŸu:\n",
    "\n",
    "**Dilated conv, kernel boyutunu bÃ¼yÃ¼tmez; kernelâ€™in Ã¶rnekleme aralÄ±ÄŸÄ±nÄ± bÃ¼yÃ¼tÃ¼r.\n",
    "SonuÃ§ olarak â€œetkin kernel boyutuâ€ bÃ¼yÃ¼r.**\n",
    "\n",
    "Etkin kernel boyutu formÃ¼lÃ¼\n",
    "\n",
    "> Etkin boyut = (k - 1) * dilation + 1\n",
    "\n",
    "Burada k=3:\n",
    "\n",
    "* d=1 â†’ etkin boyut: (3-1)*1 + 1 = 3 â†’ 3\n",
    "\n",
    "* d=2 â†’ etkin boyut: (3-1)*2 + 1 = 5 â†’ 5\n",
    "\n",
    "* d=3 â†’ etkin boyut: (3-1)*3 + 1 = 7 â†’ 7\n",
    "\n",
    "Yani dilation=2 iken:\n",
    "\n",
    "* (3,1) etkin olarak (5,1) gibi davranÄ±r\n",
    "\n",
    "* (1,3) etkin olarak (1,5) gibi davranÄ±r\n",
    "\n",
    "Ama Ã¶nemli nÃ¼ans:\n",
    "\n",
    "* GerÃ§ek kernel hÃ¢lÃ¢ 3 elemanlÄ±dÄ±r\n",
    "\n",
    "* Sadece aralarÄ±nda boÅŸluk (atlama) vardÄ±r\n",
    "\n",
    "## Neden hem normal hem dilated var?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ ikisi farklÄ± tÃ¼r bilgi toplar:\n",
    "\n",
    "* Normal depthwise: ince detay, yakÄ±n komÅŸuluk (local)\n",
    "\n",
    "* Dilated depthwise: daha geniÅŸ baÄŸlam, uzun menzil (context)\n",
    "\n",
    "Bu ikisini toplamak:\n",
    "\n",
    "**â€œHem yakÄ±n hem uzak sinyali aynÄ± anda yakalaâ€ demek.**\n",
    "\n",
    "Coordinate Attentionâ€™da bu Ã¶zellikle mantÄ±klÄ± Ã§Ã¼nkÃ¼:\n",
    "\n",
    "* H ve W yÃ¶nÃ¼nde Ã¼rettiÄŸin sinyal aslÄ±nda â€œ1D Ã¶zetâ€\n",
    "\n",
    "* Bu 1D Ã¶zetin hem kÄ±sa hem uzun desenleri olabilir (Ã¶r. uzun Ã§izgiler, ÅŸeritler, geniÅŸ objeler)\n",
    "\n",
    "\n",
    "### SonuÃ§\n",
    "\n",
    "* Evet: depthwiseâ€™Ä±n dilated versiyonunu ekledik.\n",
    "\n",
    "* Evet: toplam 4 conv oldu (H ve W iÃ§in iki Ã¶lÃ§ek).\n",
    "\n",
    "**Ama â€œkanalÄ± geniÅŸletmekâ€ deÄŸil; receptive fieldâ€™i geniÅŸletmek.**\n",
    "\n",
    "* dilation=2 ise (3,1) efektif olarak (5,1) gibi, (1,3) efektif olarak (1,5) gibi davranÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e130c14",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.merge_h = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "        self.merge_w = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "\n",
    "        self.head_h = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "        self.head_w = nn.Conv2d(mid, in_channels, 1, bias=True)\n",
    "```\n",
    "\n",
    "\n",
    "# `merge_h / merge_w` Ne YapÄ±yor? \n",
    "\n",
    "Bu iki katman, H ve W yollarÄ±nda **iki farklÄ± Ã¶lÃ§ekte Ã¼retilen sinyali** (normal depthwise + dilated depthwise) birleÅŸtirdikten sonra devreye giriyor.\n",
    "\n",
    "### Neyi â€œmergeâ€ ediyoruz?\n",
    "H yolunda ve W yolunda ÅŸu fikir var:\n",
    "\n",
    "- AynÄ± yÃ¶n iÃ§in iki farklÄ± konvolÃ¼syon Ã§Ä±ktÄ±sÄ± var:\n",
    "  - **yakÄ±n baÄŸlam** (normal depthwise)\n",
    "  - **uzak baÄŸlam** (dilated depthwise)\n",
    "\n",
    "Bu iki Ã§Ä±ktÄ± Ã¶nce toplanÄ±yor (multi-scale fusion).  \n",
    "Ama bu Ã§Ä±ktÄ±lar **depthwise** olduÄŸu iÃ§in:\n",
    "\n",
    "> Kanallar kendi iÃ§inde iÅŸlenmiÅŸ olsa da, kanallar arasÄ± etkileÅŸim yoktur.\n",
    "\n",
    "### 1Ã—1 â€œmergeâ€ neden gerekli?\n",
    "`merge_h` ve `merge_w` (1Ã—1 conv) ÅŸu iÅŸi yapar:\n",
    "\n",
    "- **kanallar arasÄ± karÄ±ÅŸÄ±m (channel mixing) saÄŸlar**\n",
    "- toplanan multi-scale sinyali â€œkanal boyutundaâ€ yeniden dÃ¼zenler\n",
    "- gerekiyorsa kÃ¼Ã§Ã¼k bir yeniden Ã¶lÃ§ekleme / kaydÄ±rma (bias) yapar\n",
    "\n",
    "Yani doÄŸru Ã¶zet cÃ¼mle:\n",
    "\n",
    "> â€œDepthwise + dilated depthwise ile elde edilen H/W sinyalini birleÅŸtiriyoruz; sonra 1Ã—1 ile kanallar arasÄ± etkileÅŸim ekleyip sinyali â€˜merge ediyoruzâ€™.â€\n",
    "\n",
    "Not: Burada â€œ2 kanaldan gelenâ€ demek yerine, daha doÄŸru ifade:\n",
    "- â€œaynÄ± kanal sayÄ±sÄ±nda iki farklÄ± dal/Ã¶lÃ§ekten gelen sinyaliâ€ merge ediyoruz.\n",
    "\n",
    "\n",
    "\n",
    "## `head_h / head_w` Ne YapÄ±yor?\n",
    "\n",
    "Bu iki katman Coordinate Attentionâ€™Ä±n klasik mantÄ±ÄŸÄ±na geri dÃ¶nÃ¼yor:\n",
    "\n",
    "- Shared bottleneck (mid kanal) Ã¼zerinden iÅŸlenmiÅŸ temsil var.\n",
    "- Bu temsil ikiye ayrÄ±lÄ±yor:\n",
    "  - H iÃ§in olan parÃ§a\n",
    "  - W iÃ§in olan parÃ§a\n",
    "\n",
    "Ama o parÃ§alarÄ±n kanal sayÄ±sÄ± `mid`.  \n",
    "Bizim nihai hedefimiz ise:\n",
    "\n",
    "> H ve W iÃ§in **C kanallÄ± attention maskesi** Ã¼retmek.\n",
    "\n",
    "### Headâ€™ler bunun iÃ§in var:\n",
    "- `head_h`: `mid â†’ in_channels (C)`\n",
    "- `head_w`: `mid â†’ in_channels (C)`\n",
    "\n",
    "### 1Ã—1 neden burada doÄŸru seÃ§im?\n",
    "Ã‡Ã¼nkÃ¼ headâ€™lerin amacÄ±:\n",
    "- uzamsal boyutu bÃ¼yÃ¼tmek/kÃ¼Ã§Ã¼ltmek deÄŸil\n",
    "- **kanal projeksiyonu yapmak** (mid uzayÄ±ndan tekrar C uzayÄ±na Ã§Ä±kmak)\n",
    "\n",
    "Yani 1Ã—1 burada ÅŸu anlama geliyor:\n",
    "\n",
    "> â€œHer uzamsal konum iÃ§in, kanallar Ã¼zerinde lineer bir dÃ¶nÃ¼ÅŸÃ¼m yapÄ±yorum.â€\n",
    "\n",
    "Bu sayede:\n",
    "- H maskesi: `(B, C, H, 1)`\n",
    "- W maskesi: `(B, C, 1, W)`\n",
    "ÅŸeklinde Ã§Ä±kabilir.\n",
    "\n",
    "\n",
    "## â€œUzamsal boyuta getirdikâ€ kÄ±smÄ±nÄ± dÃ¼zeltelim (Ã¶nemli nÃ¼ans)\n",
    "\n",
    "Senin cÃ¼mlende ÅŸÃ¶yle bir ifade var:\n",
    "> â€œHead tarafÄ±nda mid channelsâ€™a son Ã§Ä±ktÄ±yÄ± kernel_size=1 ile yine uzamsal boyuta getirdik.â€\n",
    "\n",
    "Burada kÃ¼Ã§Ã¼k ama Ã¶nemli bir dÃ¼zeltme var:\n",
    "\n",
    "- `kernel_size=1` uzamsal boyutu â€œgeri getirmezâ€.\n",
    "- Uzamsal boyut zaten korunur (H veya W uzunluÄŸu zaten var).\n",
    "- 1Ã—1â€™in yaptÄ±ÄŸÄ± ÅŸey **kanal boyutunu dÃ¶nÃ¼ÅŸtÃ¼rmek**tir.\n",
    "\n",
    "Daha doÄŸru cÃ¼mle:\n",
    "\n",
    "> â€œHead katmanlarÄ± 1Ã—1 conv ile, uzamsal boyutu sabit tutup kanal sayÄ±sÄ±nÄ± `mid â†’ C` projekte eder; bÃ¶ylece H ve W iÃ§in C-kanallÄ± maskeler Ã¼retilir.â€\n",
    "\n",
    "\n",
    "## KÄ±sa Ã–zet\n",
    "\n",
    "- **merge_h / merge_w**: Multi-scale (normal + dilated) depthwise Ã§Ä±ktÄ±yÄ± birleÅŸtirip, 1Ã—1 ile **kanal karÄ±ÅŸÄ±mÄ±** ekler.\n",
    "- **head_h / head_w**: Bottleneckâ€™ten gelen `mid` temsili 1Ã—1 ile **C kanala projekte eder**, H ve W attention maskelerini Ã¼retir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65dcdb",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Bu KÄ±sÄ±m Ne YapÄ±yor? (`alpha` iÃ§in gÃ¼venli ve Ã¶ÄŸrenilebilir baÅŸlangÄ±Ã§)\n",
    "\n",
    "```python\n",
    "        eps = 1e-6\n",
    "        a0 = float(init_alpha)\n",
    "        a0 = min(max(a0, eps), 1.0 - eps)\n",
    "        t = torch.tensor(a0)\n",
    "        raw0 = torch.log(t / (1.0 - t))\n",
    "```\n",
    "\n",
    "\n",
    "Bu blok **Ã§ok teknik ama Ã§ok Ã¶nemli** bir iÅŸi yapÄ±yor:\n",
    "> `alpha` deÄŸerini **Ã¶ÄŸrenilebilir** yapmak iÃ§in, onu **logit uzayÄ±nda gÃ¼venli ÅŸekilde baÅŸlatÄ±yor**.\n",
    "\n",
    "AdÄ±m adÄ±m, yavaÅŸ yavaÅŸ gidelim.\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ `eps = 1e-6` â€” Neden var?\n",
    "\n",
    "```python\n",
    "eps = 1e-6\n",
    "```\n",
    "Bu satÄ±rÄ±n tek amacÄ± var:\n",
    "\n",
    "* SayÄ±sal patlamayÄ± Ã¶nlemek.\n",
    "\n",
    "* Birazdan log(t / (1 - t)) hesaplanacak.\n",
    "\n",
    "EÄŸer:\n",
    "\n",
    "* t = 0 olursa â†’ log(0) âŒ\n",
    "\n",
    "* t = 1 olursa â†’ log(âˆ) âŒ\n",
    "\n",
    "Ä°ÅŸte eps, bu uÃ§ durumlarÄ± engellemek iÃ§in.\n",
    "\n",
    "## 2ï¸âƒ£ a0 = float(init_alpha)\n",
    "```python\n",
    "a0 = float(init_alpha)\n",
    "```\n",
    "\n",
    "\n",
    "* KullanÄ±cÄ±dan gelen init_alpha (Ã¶r. 0.7)\n",
    "\n",
    "* Python floatâ€™a Ã§evriliyor\n",
    "\n",
    "* HenÃ¼z Ã¶ÄŸrenilebilir deÄŸil, sadece baÅŸlangÄ±Ã§ deÄŸeri\n",
    "\n",
    "Bu ÅŸu demek:\n",
    "\n",
    "**â€œAttention baÅŸta ne kadar gÃ¼Ã§lÃ¼ olsun istiyoruz?â€**\n",
    "\n",
    "## 3ï¸âƒ£ a0 = min(max(a0, eps), 1.0 - eps)\n",
    "```python\n",
    "a0 = min(max(a0, eps), 1.0 - eps)\n",
    "```\n",
    "\n",
    "\n",
    "Bu satÄ±r clamp yapÄ±yor.\n",
    "\n",
    "Yani:\n",
    "\n",
    "* a0 en az eps\n",
    "\n",
    "* a0 en fazla 1 - eps\n",
    "\n",
    "BaÅŸka bir deyiÅŸle:\n",
    "\n",
    "**eps < a0 < 1 - eps**\n",
    "\n",
    "\n",
    "Sebep:\n",
    "\n",
    "* Birazdan log(t / (1 - t)) yapÄ±lacak\n",
    "\n",
    "* Bu formÃ¼l 0 ve 1â€™de tanÄ±msÄ±z\n",
    "\n",
    "Bu satÄ±r:\n",
    "\n",
    "**â€œNe olursa olsun sayÄ±sal olarak gÃ¼venli aralÄ±kta kal.â€**\n",
    "## 4ï¸âƒ£ t = torch.tensor(a0)\n",
    "```python\n",
    "t = torch.tensor(a0)\n",
    "```\n",
    "\n",
    "\n",
    "* Python float â†’ PyTorch tensor\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ birazdan:\n",
    "\n",
    "* gradient\n",
    "\n",
    "* parameter\n",
    "\n",
    "* buffer\n",
    "g\n",
    "ibi ÅŸeylerle Ã§alÄ±ÅŸacaÄŸÄ±z\n",
    "\n",
    "HenÃ¼z:\n",
    "\n",
    "* Parameter deÄŸil\n",
    "\n",
    "* Sadece tensor\n",
    "\n",
    "## 5ï¸âƒ£ AsÄ±l kritik satÄ±r: raw0 = log(t / (1 - t))\n",
    "```python\n",
    "raw0 = torch.log(t / (1.0 - t))\n",
    "```\n",
    "\n",
    "\n",
    "Burada yapÄ±lan ÅŸey ÅŸu:\n",
    "\n",
    "* Sigmoidâ€™in tersini (inverse / logit) almak\n",
    "\n",
    "HatÄ±rla:\n",
    "\n",
    "Sigmoid fonksiyonu:\n",
    "```bash\n",
    "sigmoid(x) = 1 / (1 + exp(-x))\n",
    "```\n",
    "\n",
    "\n",
    "Bunun tersi (logit):\n",
    "```bash\n",
    "logit(p) = log(p / (1 - p))\n",
    "```\n",
    "\n",
    "\n",
    "Yani:\n",
    "\n",
    "EÄŸer raw0â€™Ä± sigmoidâ€™den geÃ§irirsen:\n",
    "```bash\n",
    "sigmoid(raw0) = t = a0\n",
    "```\n",
    "\n",
    "\n",
    "ğŸ¯ Tam olarak istediÄŸimiz ÅŸey bu.\n",
    "\n",
    "## 6ï¸âƒ£ Neden bu dolambaÃ§lÄ± yol?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ biz ÅŸunu istiyoruz:\n",
    "\n",
    "* alpha 0 ile 1 arasÄ±nda kalsÄ±n\n",
    "\n",
    "* Ama Ã¶ÄŸrenilebilir (unconstrained) olsun\n",
    "\n",
    "Bunun Ã§Ã¶zÃ¼mÃ¼:\n",
    "\n",
    "* AsÄ±l parametreyi logit uzayÄ±nda tut\n",
    "\n",
    "* Ä°leri geÃ§iÅŸte sigmoid ile [0,1] aralÄ±ÄŸÄ±na map et\n",
    "\n",
    "Yani:\n",
    "\n",
    "* Ã–ÄŸrenilen ÅŸey: alpha_raw (âˆ’âˆ, +âˆ)\n",
    "\n",
    "* KullanÄ±lan ÅŸey: alpha = sigmoid(alpha_raw) âˆˆ (0,1)\n",
    "\n",
    "Bu industry standard bir tekniktir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e66eb",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230e862",
   "metadata": {},
   "source": [
    "# Bu Blok Ne YapÄ±yor? (Opsiyonel Spatial Gate)\n",
    "```python \n",
    "\n",
    "        self.use_spatial_gate = bool(use_spatial_gate)\n",
    "        self.spatial_gate_beta = float(spatial_gate_beta)\n",
    "        if self.use_spatial_gate:\n",
    "            self.spatial_dw = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels, bias=False)\n",
    "            self.spatial_pw = nn.Conv2d(in_channels, in_channels, 1, bias=True)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Bu kod, `use_spatial_gate=True` ise modele ek bir mekanizma ekler:\n",
    "\n",
    "- Ã‡Ä±ktÄ± feature map Ã¼zerinde **konum bazlÄ± (spatial)** bir maske Ã¼retir\n",
    "- Bu maskeyi `sigmoid` ile 0â€“1 aralÄ±ÄŸÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±r\n",
    "- Sonra bu maskeyi â€œyumuÅŸatÄ±pâ€ (beta ile) feature mapâ€™e Ã§arpar\n",
    "\n",
    "Ama ÅŸunu unutma:\n",
    "> Bu gate, CAâ€™den baÄŸÄ±msÄ±z ikinci bir bastÄ±rma katmanÄ± gibi Ã§alÄ±ÅŸÄ±r.\n",
    "\n",
    "## 1) `self.use_spatial_gate = bool(use_spatial_gate)`\n",
    "Bu bir anahtar:\n",
    "\n",
    "- `False` â†’ bu gate tamamen devre dÄ±ÅŸÄ± (hiÃ§ ek parametre yok)\n",
    "- `True`  â†’ aÅŸaÄŸÄ±daki convâ€™lar eklenir ve forwardâ€™da uygulanÄ±r\n",
    "\n",
    "\n",
    "## 2) `self.spatial_gate_beta = float(spatial_gate_beta)`\n",
    "Bu, gateâ€™in â€œÅŸiddetiniâ€ kontrol eden katsayÄ±dÄ±r.\n",
    "\n",
    "Tipik mantÄ±k ÅŸu:\n",
    "- gate sigmoidâ€™den Ã§Ä±kar â†’ (0,1)\n",
    "- sonra `1 + beta*(gate - 1)` gibi bir yumuÅŸatma ile 1â€™e yaklaÅŸtÄ±rÄ±lÄ±r  \n",
    "  (yani gate Ã§ok agresif olmasÄ±n)\n",
    "\n",
    "`beta` kÃ¼Ã§Ã¼kse gate etkisi az, bÃ¼yÃ¼kse etkisi artar.\n",
    "\n",
    "\n",
    "## 3) `self.spatial_dw` (3Ã—3 depthwise conv)\n",
    "Bu katman:\n",
    "\n",
    "- **Spatial (H,W)** baÄŸlamÄ± gÃ¶rÃ¼r (3Ã—3 komÅŸuluk)\n",
    "- **Depthwise** olduÄŸu iÃ§in her kanalÄ± ayrÄ± iÅŸler (kanallar karÄ±ÅŸmaz)\n",
    "- AmaÃ§: konumsal olarak â€œnereler Ã¶nemli / nereler deÄŸilâ€ sinyali Ã§Ä±karmak\n",
    "\n",
    "KÄ±saca:\n",
    "> â€œHer kanal iÃ§in, 3Ã—3 komÅŸuluktan bir spatial ipucu Ã§Ä±kar.â€\n",
    "\n",
    "\n",
    "## 4) `self.spatial_pw` (1Ã—1 pointwise conv)\n",
    "Depthwise conv kanallarÄ± karÄ±ÅŸtÄ±rmadÄ±ÄŸÄ± iÃ§in 1Ã—1 burada devreye girer:\n",
    "\n",
    "- Kanal bazÄ±nda yeniden Ã¶lÃ§ekleme / karÄ±ÅŸtÄ±rma saÄŸlar\n",
    "- â€œGateâ€ Ã¼retimini daha esnek yapar\n",
    "\n",
    "KÄ±saca:\n",
    "> â€œDepthwiseâ€™Ä±n Ã¼rettiÄŸi sinyali kanal uzayÄ±nda dÃ¼zenleyip gerÃ§ek gateâ€™e dÃ¶nÃ¼ÅŸtÃ¼r.â€\n",
    "\n",
    "\n",
    "## BÃ¼yÃ¼k Resim: Neden var?\n",
    "\n",
    "Coordinate Attention zaten:\n",
    "- H yÃ¶nÃ¼ maskesi (`a_h`)\n",
    "- W yÃ¶nÃ¼ maskesi (`a_w`)\n",
    "Ã¼retip bunlarÄ± birleÅŸtiriyor.\n",
    "\n",
    "Spatial gate ise ÅŸunu ekler:\n",
    "- DoÄŸrudan `(H,W)` Ã¼zerinde ek bir kapÄ±\n",
    "\n",
    "Yani:\n",
    "- CA: koordinat temelli (ayrÄ± H ve W)\n",
    "- Spatial gate: klasik â€œspatial attentionâ€ gibi (tam 2D)\n",
    "\n",
    "\n",
    "## Risk (Ã–nemli)\n",
    "Bu gate aÃ§Ä±lÄ±rsa:\n",
    "- AynÄ± feature map ikinci kez bastÄ±rÄ±labilir\n",
    "- BazÄ± durumlarda fayda verir\n",
    "- BazÄ± durumlarda over-suppression yapar (Ã¶zellikle small batch / detection)\n",
    "\n",
    "O yÃ¼zden default `False` mantÄ±klÄ±.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a76a9",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Bu Ä°ki SatÄ±r Ne YapÄ±yor?\n",
    "```python\n",
    "        x_h = 0.5 * (x.mean(dim=3, keepdim=True) + x.amax(dim=3, keepdim=True))    # (B,C,H,1)\n",
    "        x_w_1w = 0.5 * (x.mean(dim=2, keepdim=True) + x.amax(dim=2, keepdim=True)) # (B,C,1,W)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Elimizde giriÅŸ feature map var:\n",
    "\n",
    "- `x` ÅŸekli: **(B, C, H, W)**  \n",
    "  (Batch, Channel, Height, Width)\n",
    "\n",
    "Burada amaÃ§:\n",
    "> Her kanal iÃ§in, uzamsal bilgiyi iki farklÄ± â€œÅŸeritâ€ Ã¶zete Ã§evirmek:\n",
    "- **H yÃ¶nÃ¼ Ã¶zeti:** (B, C, H, 1)\n",
    "- **W yÃ¶nÃ¼ Ã¶zeti:** (B, C, 1, W)\n",
    "\n",
    "Bu, Coordinate Attentionâ€™Ä±n â€œkoordinat bilgisi kaybolmasÄ±nâ€ diye yaptÄ±ÄŸÄ± ana numara.\n",
    "\n",
    "\n",
    "\n",
    "# 1) `x_h`: W eksenini sÄ±kÄ±ÅŸtÄ±r, Hâ€™yi koru\n",
    "\n",
    "`dim=3` demek: **W ekseni** (geniÅŸlik).\n",
    "\n",
    "- `x.mean(dim=3, keepdim=True)`:\n",
    "  - Her (B,C,H) iÃ§in W boyunca **ortalama** alÄ±r.\n",
    "  - SonuÃ§: **(B, C, H, 1)**\n",
    "\n",
    "- `x.amax(dim=3, keepdim=True)`:\n",
    "  - Her (B,C,H) iÃ§in W boyunca **maksimum** alÄ±r.\n",
    "  - SonuÃ§: **(B, C, H, 1)**\n",
    "\n",
    "Sonra ikisini toplayÄ±p 0.5 ile ortalamasÄ±nÄ± alÄ±yorsun:\n",
    "\n",
    "> **W boyunca â€œmean + maxâ€ karÄ±ÅŸÄ±mÄ± ile bir H-profili Ã§Ä±karÄ±yorsun.**\n",
    "\n",
    "Yani `x_h` ÅŸunu temsil eder:\n",
    "> â€œBu kanalda, her yÃ¼kseklik satÄ±rÄ±nda aktivasyon ne durumda?â€  \n",
    "(Wâ€™deki tÃ¼m piksellerin Ã¶zetlenmiÅŸ hali)\n",
    "\n",
    "\n",
    "## 2) `x_w_1w`: H eksenini sÄ±kÄ±ÅŸtÄ±r, Wâ€™yi koru\n",
    "\n",
    "`dim=2` demek: **H ekseni** (yÃ¼kseklik).\n",
    "\n",
    "- `x.mean(dim=2, keepdim=True)`:\n",
    "  - H boyunca ortalama alÄ±r\n",
    "  - SonuÃ§: **(B, C, 1, W)**\n",
    "\n",
    "- `x.amax(dim=2, keepdim=True)`:\n",
    "  - H boyunca maksimum alÄ±r\n",
    "  - SonuÃ§: **(B, C, 1, W)**\n",
    "\n",
    "Sonra yine 0.5 ile karÄ±ÅŸtÄ±rÄ±yorsun:\n",
    "\n",
    "> **H boyunca â€œmean + maxâ€ karÄ±ÅŸÄ±mÄ± ile bir W-profili Ã§Ä±karÄ±yorsun.**\n",
    "\n",
    "Yani `x_w_1w` ÅŸunu temsil eder:\n",
    "> â€œBu kanalda, her geniÅŸlik sÃ¼tununda aktivasyon ne durumda?â€  \n",
    "(Hâ€™deki tÃ¼m piksellerin Ã¶zetlenmiÅŸ hali)\n",
    "\n",
    "\n",
    "\n",
    "## Neden Hem Mean Hem Max?\n",
    "\n",
    "Klasik CA Ã§oÄŸu zaman sadece **mean pooling** kullanÄ±r.\n",
    "Senin yaptÄ±ÄŸÄ±n geliÅŸtirme:\n",
    "\n",
    "- **Mean**: genel daÄŸÄ±lÄ±mÄ±, â€œortalama enerjiyiâ€ yakalar  \n",
    "- **Max**: en baskÄ±n/salient aktivasyonu yakalar (en gÃ¼Ã§lÃ¼ sinyal nerede?)\n",
    "\n",
    "Birlikte kullanÄ±nca:\n",
    "> Hem â€œgenel trendâ€ hem â€œen dikkat Ã§ekici bÃ¶lgeâ€ aynÄ± anda maskeye sinyal verir.\n",
    "\n",
    "Bu genelde attentionâ€™Ä± daha â€œkeskinâ€ ve daha bilgilendirici yapar.\n",
    "\n",
    "\n",
    "\n",
    "## `keepdim=True` neden Ã¶nemli?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ boyutu koruyup:\n",
    "- `H` profilini **(B,C,H,1)** olarak,\n",
    "- `W` profilini **(B,C,1,W)** olarak\n",
    "tutmak istiyoruz.\n",
    "\n",
    "Bu ÅŸekiller, biraz sonra concat/permute ile CAâ€™nin standart akÄ±ÅŸÄ±na girecek.\n",
    "\n",
    "\n",
    "## Tek cÃ¼mlelik Ã¶zet\n",
    "\n",
    "> Bu iki satÄ±r, feature mapâ€™i kanal bazÄ±nda iki adet 1D koordinat Ã¶zetine Ã§evirir; biri satÄ±r (H) profili, biri sÃ¼tun (W) profili; Ã¼stelik mean+max ile daha gÃ¼Ã§lÃ¼ bir Ã¶zet Ã¼retir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bfa94",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "# H ve W Multi-Scale Bloku (Local + Dilated + 1Ã—1 Merge)\n",
    "\n",
    "```python \n",
    "        H_multi_scale = self.h_channel_mixer(self.h_local_dw(H_profile) + self.h_dilated_dw(H_profile))  \n",
    "\n",
    "        W_multi_scale = self.w_local_dw(W_profile) + self.w_dilated_dw(W_profile)                        \n",
    "        W_multi_scale = self.w_channel_mixer(W_multi_scale)                                               \n",
    "        W_multi_scale = W_multi_scale.permute(0, 1, 3, 2) \n",
    "```\n",
    "\n",
    "\n",
    "Bu kÄ±sÄ±mda yaptÄ±ÄŸÄ±mÄ±z iÅŸin Ã¶zÃ¼ ÅŸudur:\n",
    "\n",
    "> H ve W iÃ§in oluÅŸturduÄŸumuz 1D profilleri (ÅŸeritleri) iki farklÄ± Ã¶lÃ§ekte (local + long-range) iÅŸleyip,\n",
    "> sonra 1Ã—1 ile kanal karÄ±ÅŸtÄ±rmasÄ± yaparak â€œtek bir gÃ¼Ã§lÃ¼ temsilâ€ haline getiriyoruz.\n",
    "\n",
    "\n",
    "\n",
    "## 1) `H_multi_scale` (Height profili iÃ§in)\n",
    "\n",
    "Elimizde ÅŸu var:\n",
    "- **H_profile:** `(B, C, H, 1)`  \n",
    "  (W ekseni sÄ±kÄ±ÅŸtÄ±rÄ±ldÄ±, H korunuyor)\n",
    "\n",
    "Bu profile iki ayrÄ± depthwise konvolÃ¼syon uygulanÄ±yor:\n",
    "\n",
    "### a) Local (yakÄ±n komÅŸuluk)\n",
    "- H boyunca 3 elemanlÄ±k pencereyle bakar.\n",
    "- â€œKÄ±sa menzil / lokal detayâ€ yakalar.\n",
    "\n",
    "### b) Dilated (uzak baÄŸlam)\n",
    "- Yine 3 elemanlÄ± pencere ama aralÄ±klÄ± Ã¶rnekleme (dilation) ile.\n",
    "- â€œUzun menzil / geniÅŸ baÄŸlamâ€ yakalar.\n",
    "\n",
    "Bu iki Ã§Ä±ktÄ±:\n",
    "\n",
    "- AynÄ± ÅŸekildedir `(B, C, H, 1)`\n",
    "- AynÄ± kanallara aittir (kanal sayÄ±sÄ± deÄŸiÅŸmez)\n",
    "- Bu yÃ¼zden toplanabilir.\n",
    "\n",
    "> Buradaki toplama â€œkanal toplamaâ€ deÄŸil, **iki Ã¶lÃ§eÄŸin (multi-scale) birleÅŸtirilmesidir**.\n",
    "\n",
    "### c) 1Ã—1 â€œchannel mixerâ€ (merge)\n",
    "Depthwise conv kanallarÄ± karÄ±ÅŸtÄ±rmadÄ±ÄŸÄ± iÃ§in, ardÄ±ndan 1Ã—1 gelir:\n",
    "\n",
    "- Kanallar arasÄ±nda etkileÅŸim saÄŸlar (channel mixing)\n",
    "- Multi-scale birleÅŸimini kanal uzayÄ±nda yeniden dÃ¼zenler\n",
    "- Her kanalÄ±n katkÄ±sÄ±nÄ± daha esnek hale getirir\n",
    "\n",
    "SonuÃ§:\n",
    "- **H_multi_scale:** `(B, C, H, 1)`\n",
    "\n",
    "\n",
    "\n",
    "## 2) `W_multi_scale` (Width profili iÃ§in)\n",
    "\n",
    "Elimizde:\n",
    "- **W_profile:** `(B, C, 1, W)`  \n",
    "  (H ekseni sÄ±kÄ±ÅŸtÄ±rÄ±ldÄ±, W korunuyor)\n",
    "\n",
    "### a) Local depthwise\n",
    "- W boyunca lokal komÅŸuluÄŸu yakalar.\n",
    "\n",
    "### b) Dilated depthwise\n",
    "- W boyunca daha geniÅŸ baÄŸlam yakalar (dilation ile).\n",
    "\n",
    "Ä°kisi yine aynÄ± ÅŸekildedir `(B, C, 1, W)` ve toplanÄ±r:\n",
    "\n",
    "> Bu da yine â€œkanal deÄŸil, Ã¶lÃ§ek birleÅŸtirmeâ€dir.\n",
    "\n",
    "### c) 1Ã—1 â€œchannel mixerâ€ (merge)\n",
    "Toplanan sinyal 1Ã—1â€™den geÃ§irilir:\n",
    "\n",
    "- Kanal karÄ±ÅŸÄ±mÄ± eklenir\n",
    "- W yÃ¶nÃ¼ndeki temsil daha ifade gÃ¼cÃ¼ kazanÄ±r\n",
    "\n",
    "SonuÃ§ hÃ¢lÃ¢:\n",
    "- `(B, C, 1, W)`\n",
    "\n",
    "### d) `permute` (format uyumu iÃ§in)\n",
    "Son adÄ±mda ÅŸekli ÅŸu hale Ã§eviriyoruz:\n",
    "- `(B, C, 1, W)` â†’ `(B, C, W, 1)`\n",
    "\n",
    "Bunun sebebi:\n",
    "> Biraz sonra H ve Wâ€™yi `dim=2` ekseninde `cat` edeceÄŸiz.\n",
    "> H tarafÄ± zaten `(B, C, H, 1)` formatÄ±nda.\n",
    "> W tarafÄ±nÄ± da `(B, C, W, 1)` yaparsak,\n",
    "> ikisini aynÄ± eksende yan yana koyup `(B, C, H+W, 1)` oluÅŸturabiliriz.\n",
    "\n",
    "\n",
    "\n",
    "## Tek CÃ¼mlelik Ã–zet\n",
    "\n",
    "- H ve W profillerini **local + dilated depthwise** ile iki Ã¶lÃ§ekte iÅŸleriz,\n",
    "- Ã§Ä±ktÄ±larÄ± **toplayÄ±p** multi-scale birleÅŸtiririz,\n",
    "- 1Ã—1 ile **kanal mixing** ekleriz,\n",
    "- W tarafÄ±nÄ± `permute` ile concat formatÄ±na hazÄ±rlarÄ±z.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4c900",
   "metadata": {},
   "source": [
    "# H + W Concat ve Shared Bottleneck BloÄŸu \n",
    "\n",
    "Bu bÃ¶lÃ¼mde artÄ±k **H ve W eksenlerinden gelen bilgiler ayrÄ± ayrÄ± deÄŸil**,  \n",
    "**ortak bir karar mekanizmasÄ±** iÃ§inde birlikte iÅŸlenir.\n",
    "\n",
    "```python\n",
    "        hw_concat = torch.cat([H_multi_scale, W_multi_scale], dim=2)                                       \n",
    "\n",
    "        shared_mid = self.act(self.shared_bottleneck_norm(self.shared_bottleneck_proj(hw_concat)))       \n",
    "        shared_mid = self.act(self.shared_bottleneck_refine_norm(self.shared_bottleneck_refine(shared_mid)))  \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 1ï¸âƒ£ H ve W Åeritlerinin BirleÅŸtirilmesi (Concat)\n",
    "\n",
    "Bu aÅŸamaya gelmeden Ã¶nce elimizde ÅŸunlar vardÄ±:\n",
    "\n",
    "- **H tarafÄ±**:  \n",
    "  YÃ¼kseklik boyunca anlam taÅŸÄ±yan, multi-scale iÅŸlenmiÅŸ bir ÅŸerit  \n",
    "  â†’ ÅŸekil olarak `(B, C, H, 1)`\n",
    "\n",
    "- **W tarafÄ±**:  \n",
    "  GeniÅŸlik boyunca anlam taÅŸÄ±yan, multi-scale iÅŸlenmiÅŸ bir ÅŸerit  \n",
    "  â†’ ÅŸekil olarak `(B, C, W, 1)`\n",
    "\n",
    "Bu iki ÅŸerit **toplanmaz**.  \n",
    "Ã‡Ã¼nkÃ¼ toplarsak bilgi Ã¼st Ã¼ste biner ve karÄ±ÅŸÄ±r.\n",
    "\n",
    "Bunun yerine:\n",
    "> H ÅŸeridi ile W ÅŸeridi **yan yana eklenir**.\n",
    "\n",
    "SonuÃ§:\n",
    "- Kanal sayÄ±sÄ± deÄŸiÅŸmez\n",
    "- Uzunluk artÄ±k **H + W** olur\n",
    "- Elimizde **tek bir uzun ÅŸerit** vardÄ±r\n",
    "\n",
    "Bu ÅŸerit ÅŸu an ÅŸunu temsil eder:\n",
    "> â€œBu tensorun ilk kÄ±smÄ± H bilgisi, devamÄ± W bilgisi.â€\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ Neden Shared (Ortak) Bottleneck?\n",
    "\n",
    "Buradaki temel fikir ÅŸudur:\n",
    "\n",
    "> H ve W ayrÄ± ayrÄ± karar vermesin.  \n",
    "> Ã–nce **aynÄ± masa etrafÄ±nda otursunlar**,  \n",
    "> sonra tekrar ayrÄ±lÄ±p kendi maskelerini Ã¼retsinler.\n",
    "\n",
    "Bu yÃ¼zden bu birleÅŸik ÅŸerit **shared bottleneck** iÃ§ine sokulur.\n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ Bottleneck â€“ 1. AÅŸama (C â†’ mid)\n",
    "\n",
    "Ä°lk bottleneck adÄ±mÄ±nda:\n",
    "\n",
    "- Kanal boyutu **Câ€™den midâ€™e dÃ¼ÅŸÃ¼rÃ¼lÃ¼r**\n",
    "- Uzamsal uzunluk (**H+W**) korunur\n",
    "\n",
    "AmaÃ§:\n",
    "- Hesap maliyetini dÃ¼ÅŸÃ¼rmek\n",
    "- Gereksiz kanal bilgisini elemek\n",
    "- Attention kararÄ±nÄ± daha sade bir uzayda vermek\n",
    "\n",
    "Bu aÅŸamadan sonra temsil:\n",
    "- Daha kompakt\n",
    "- Daha stabil\n",
    "- Karar vermeye daha uygun hÃ¢le gelir\n",
    "\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ Bottleneck â€“ 2. AÅŸama (mid â†’ mid)\n",
    "\n",
    "Ä°kinci aÅŸamada:\n",
    "\n",
    "- Kanal boyutu **mid olarak kalÄ±r**\n",
    "- Temsil bir kez daha iÅŸlenir (refine edilir)\n",
    "\n",
    "Bu adÄ±mÄ±n amacÄ±:\n",
    "> â€œTek katman yetmeyebilir;  \n",
    "> ortak karar uzayÄ±nda biraz daha dÃ¼ÅŸÃ¼nelim.â€\n",
    "\n",
    "Yani bu, bottleneckâ€™i **derinleÅŸtirme** adÄ±mÄ±dÄ±r.\n",
    "\n",
    "\n",
    "\n",
    "### 5ï¸âƒ£ Bu BloÄŸun Ã‡Ä±ktÄ±sÄ± Ne Anlama Geliyor?\n",
    "\n",
    "Bu bloÄŸun sonunda elimizde ÅŸu vardÄ±r:\n",
    "\n",
    "- H ve W bilgisi **tek bir ÅŸerit hÃ¢linde**\n",
    "- Ama artÄ±k ham deÄŸil\n",
    "- Ortak bir bottleneckâ€™ten geÃ§miÅŸ\n",
    "- Kanal boyutu kÃ¼Ã§Ã¼ltÃ¼lmÃ¼ÅŸ\n",
    "- â€œKarar vermeye hazÄ±râ€ bir temsil\n",
    "\n",
    "Bir sonraki adÄ±mda:\n",
    "- Bu uzun ÅŸerit tekrar **H ve W olarak ikiye ayrÄ±lacak**\n",
    "- Her biri kendi attention maskesini Ã¼retmek iÃ§in kullanÄ±lacak\n",
    "\n",
    "\n",
    "### ğŸ”‘ Tek CÃ¼mlelik Ã–zet\n",
    "\n",
    "H ve Wâ€™den gelen multi-scale ÅŸeritleri yan yana ekleyip,  \n",
    "kanal boyutunu dÃ¼ÅŸÃ¼ren shared bottleneckâ€™ten geÃ§irerek  \n",
    "iki eksen iÃ§in **ortak bir attention karar temsili** Ã¼retiyoruz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89eb45b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Split + Permute + Head (H ve W Maskelerini Ãœretme AÅŸamasÄ±)\n",
    "\n",
    "Bu blokta artÄ±k â€œortak karar ÅŸeridiâ€ olan `shared_mid` iÃ§inden  \n",
    "**H ve W parÃ§alarÄ±nÄ± geri ayÄ±rÄ±p**, her biri iÃ§in **attention maskesi** Ã¼retiyoruz.\n",
    "\n",
    "```python \n",
    "        mid_H, mid_W = torch.split(shared_mid, [H, W], dim=2)                                             \n",
    "        mid_W = mid_W.permute(0, 1, 3, 2)                                                                   \n",
    "\n",
    "        attn_H = F.hardsigmoid(self.h_attention_head(mid_H), inplace=False)                                 \n",
    "        attn_W = F.hardsigmoid(self.w_attention_head(mid_W), inplace=False) \n",
    "```\n",
    "\n",
    "\n",
    "### 1ï¸âƒ£ `torch.split(shared_mid, [H, W], dim=2)` ne yapÄ±yor?\n",
    "\n",
    "`shared_mid` ÅŸu formdaydÄ±:\n",
    "\n",
    "- **(B, mid, H+W, 1)**\n",
    "\n",
    "Burada `H+W` uzunluÄŸu **dim=2** ekseninde duruyor.\n",
    "\n",
    "Ã–nceden `cat` yaparken:\n",
    "- Ã¶nce H ÅŸeridi\n",
    "- sonra W ÅŸeridi\n",
    "ÅŸeklinde yan yana koymuÅŸtuk.\n",
    "\n",
    "Split burada ÅŸunu diyor:\n",
    "\n",
    "> â€œdim=2 eksenini, Ã¶nce **H uzunluÄŸunda**, sonra **W uzunluÄŸunda** iki parÃ§aya ayÄ±r.â€\n",
    "\n",
    "SonuÃ§lar:\n",
    "\n",
    "- **mid_H**: `(B, mid, H, 1)`  \n",
    "  â†’ H eksenine ait bottleneck temsili\n",
    "\n",
    "- **mid_W**: `(B, mid, W, 1)`  \n",
    "  â†’ W eksenine ait bottleneck temsili\n",
    "\n",
    "**Ã–nemli:** split yÃ¼zdeyle bÃ¶lmez; senin verdiÄŸin kesin uzunluklara gÃ¶re bÃ¶ler.\n",
    "\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ `mid_W = mid_W.permute(0, 1, 3, 2)` neden var?\n",
    "\n",
    "Split sonrasÄ± `mid_W` ÅŸu ÅŸekilde:\n",
    "\n",
    "- `(B, mid, W, 1)`\n",
    "\n",
    "Ama W iÃ§in maskeyi Ã¼retirken (head_w tarafÄ±nda) standart format ÅŸu olmalÄ±:\n",
    "\n",
    "- `(B, mid, 1, W)`\n",
    "\n",
    "Yani W uzunluÄŸunu **son eksene** taÅŸÄ±mamÄ±z gerekiyor.\n",
    "\n",
    "`permute(0,1,3,2)` ÅŸu deÄŸiÅŸimi yapar:\n",
    "\n",
    "- `(B, mid, W, 1)` â†’ `(B, mid, 1, W)`\n",
    "\n",
    "Bu tamamen **format uyumu** iÃ§indir; bilgi deÄŸiÅŸmiyor, sadece eksenler yer deÄŸiÅŸtiriyor.\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ `h_attention_head(mid_H)` ve `w_attention_head(mid_W)` ne yapÄ±yor?\n",
    "\n",
    "Åimdi iki parÃ§a da â€œmaskeye dÃ¶nÃ¼ÅŸmeâ€ aÅŸamasÄ±na geldi.\n",
    "\n",
    "Bu head katmanlarÄ±nÄ±n gÃ¶revi:\n",
    "\n",
    "> Bottleneckâ€™teki (mid kanallÄ±) temsili,  \n",
    "> tekrar **orijinal kanal sayÄ±sÄ±na (C)** projekte etmek.\n",
    "\n",
    "Yani:\n",
    "- **mid â†’ C** dÃ¶nÃ¼ÅŸÃ¼mÃ¼\n",
    "\n",
    "Bu yÃ¼zden headâ€™ler 1Ã—1 konv gibi davranÄ±r:\n",
    "- Uzamsal boyut korunur\n",
    "- Kanal boyutu deÄŸiÅŸtirilir\n",
    "\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ `F.hardsigmoid(..., inplace=False)` neden kullanÄ±lÄ±yor?\n",
    "\n",
    "Head Ã§Ä±ktÄ±sÄ± â€œmaskeye benzerâ€ bir ÅŸeye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmek istenir:\n",
    "- deÄŸerler kontrollÃ¼ olsun\n",
    "- aÅŸÄ±rÄ± bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k deÄŸerler taÅŸmasÄ±n\n",
    "\n",
    "`hardsigmoid` burada:\n",
    "- maskeyi **0â€“1 aralÄ±ÄŸÄ±na** sÄ±kÄ±ÅŸtÄ±rÄ±r (pratikte)\n",
    "- sigmoidâ€™e gÃ¶re daha ucuz ve stabil bir aktivasyon olabilir\n",
    "\n",
    "`inplace=False` seÃ§imi:\n",
    "- autograd ve ara tensÃ¶rlerin gÃ¼venliÄŸi aÃ§Ä±sÄ±ndan daha az riskli\n",
    "- inplace bazen gradient graphâ€™Ä±nÄ± bozabilir, bu tÃ¼r attention akÄ±ÅŸlarÄ±nda gereksiz risk\n",
    "\n",
    "\n",
    "### 5ï¸âƒ£ Bu bloÄŸun Ã§Ä±ktÄ±larÄ± ne?\n",
    "\n",
    "SonuÃ§ta elde ettiÄŸimiz iki maskedir:\n",
    "\n",
    "- **attn_H**: `(B, C, H, 1)`  \n",
    "  â†’ â€œH ekseninde, her kanal iÃ§in Ã¶lÃ§ekâ€\n",
    "\n",
    "- **attn_W**: `(B, C, 1, W)`  \n",
    "  â†’ â€œW ekseninde, her kanal iÃ§in Ã¶lÃ§ekâ€\n",
    "\n",
    "Bir sonraki aÅŸamada:\n",
    "- bu iki maske alpha/beta ile yumuÅŸatÄ±lacak\n",
    "- Ã§arpÄ±lÄ±p `(B,C,H,W)` tam 2D Ã¶lÃ§eÄŸe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lecek\n",
    "- ve giriÅŸ feature mapâ€™e uygulanacak\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”‘ Tek CÃ¼mlelik Ã–zet\n",
    "\n",
    "`shared_mid` iÃ§indeki H+W ÅŸeridini tekrar H ve W olarak ayÄ±rÄ±p,  \n",
    "Wâ€™yi formata sokup, iki head ile `mid â†’ C` projekte ederek  \n",
    "H ve W iÃ§in kanal bazlÄ± attention maskeleri Ã¼retiyoruz.\n",
    "\n",
    "---\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d1fbc",
   "metadata": {},
   "source": [
    "# `alpha` ve `scale_H / scale_W` BloÄŸu (KontrollÃ¼ Attention GÃ¼cÃ¼)\n",
    "\n",
    "Bu bÃ¶lÃ¼m, Ã¼retilen `attn_H` ve `attn_W` maskelerini **doÄŸrudan uygulamak yerine**\n",
    "Ã¶nce **â€œne kadar gÃ¼veneceÄŸiz?â€** sorusuna cevap veren bir karÄ±ÅŸÄ±m (mixing) uygular.\n",
    "\n",
    "```python\n",
    "        alpha_h = torch.sigmoid(self.alpha_h_raw)# Bu alpha deÄŸerleri                                                 \n",
    "        alpha_w = torch.sigmoid(self.alpha_w_raw)                                                          \n",
    "\n",
    "        scale_H = (1.0 - alpha_h) + alpha_h * attn_H  # Burda dierktr scale matematiÄŸini kulanadÄ±k :: scale=(1âˆ’Î±)+Î±â‹…attn                                                   \n",
    "        scale_W = (1.0 - alpha_w) + alpha_w * attn_W                                                         \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 1ï¸âƒ£ `alpha_h = sigmoid(alpha_h_raw)` ne demek?\n",
    "\n",
    "`alpha_h_raw` / `alpha_w_raw` aslÄ±nda **sÄ±nÄ±rsÄ±z** deÄŸer alabilen parametrelerdir:\n",
    "- teorik aralÄ±k: `(-âˆ, +âˆ)`\n",
    "\n",
    "Ama `alpha` deÄŸerinin anlamÄ± ÅŸudur:\n",
    "> â€œAttentionâ€™Ä± ne kadar aÃ§ayÄ±m?â€\n",
    "\n",
    "Bu da doÄŸal olarak **0 ile 1 arasÄ±nda** olmalÄ±dÄ±r.\n",
    "\n",
    "Bu yÃ¼zden `sigmoid` ile:\n",
    "- `alpha_h = sigmoid(alpha_h_raw)` â†’ `(0, 1)`\n",
    "- `alpha_w = sigmoid(alpha_w_raw)` â†’ `(0, 1)`\n",
    "\n",
    "\n",
    "\n",
    "> Not: `alpha_raw`â€™Ä±n â€œlogit uzayÄ±ndaâ€ tutulmasÄ±, Ã¶ÄŸrenmeyi daha rahat yapar;\n",
    "> Ã§Ã¼nkÃ¼ parametre serbestÃ§e hareket eder, ama `alpha` yine gÃ¼venli aralÄ±kta kalÄ±r.\n",
    "\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ `scale_H = (1-Î±) + Î±Â·attn_H` ne yapÄ±yor?\n",
    "\n",
    "Bu formÃ¼l, matematikte Ã§ok bilinen bir ÅŸeydir:\n",
    "> **Linear interpolation (lerp)**\n",
    "\n",
    "Yani `scale_H`, iki uÃ§ arasÄ±nda karÄ±ÅŸÄ±m yapar:\n",
    "- **1.0** (hiÃ§ dokunma / identity)\n",
    "- **attn_H** (tam attention)\n",
    "\n",
    "Bunu `Î±` kontrol eder:\n",
    "\n",
    "#### âœ… EÄŸer `Î± = 0` olursa\n",
    "\\[\n",
    "scale_H = (1-0) + 0 \\cdot attn_H = 1\n",
    "\\]\n",
    "â†’ Attention tamamen devre dÄ±ÅŸÄ±: **hiÃ§ Ã¶lÃ§ekleme yok**\n",
    "\n",
    "#### âœ… EÄŸer `Î± = 1` olursa\n",
    "\\[\n",
    "scale_H = (1-1) + 1 \\cdot attn_H = attn_H\n",
    "\\]\n",
    "â†’ Attention tam gÃ¼Ã§: **maskeyi olduÄŸu gibi uygula**\n",
    "\n",
    "#### âœ… EÄŸer `0 < Î± < 1` olursa\n",
    "\\[\n",
    "scale_H = 1 \\text{ ile } attn_H \\text{ arasÄ±nda bir deÄŸer}\n",
    "\\]\n",
    "â†’ Attention â€œyumuÅŸakâ€ uygulanÄ±r, agresiflik dÃ¼ÅŸer\n",
    "\n",
    "AynÄ± mantÄ±k `scale_W` iÃ§in de geÃ§erlidir.\n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ Neden bÃ¶yle bir karÄ±ÅŸÄ±m gerekli?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ `attn_H` / `attn_W` doÄŸrudan kullanÄ±lÄ±rsa:\n",
    "- bazÄ± durumlarda aÅŸÄ±rÄ± bastÄ±rma (over-suppression) olabilir\n",
    "- eÄŸitim baÅŸÄ±nda maskeler yanlÄ±ÅŸ kalibre olabilir\n",
    "- kÃ¼Ã§Ã¼k batch / gÃ¼rÃ¼ltÃ¼lÃ¼ featureâ€™larda stabilite dÃ¼ÅŸebilir\n",
    "\n",
    "Bu karÄ±ÅŸÄ±m sayesinde:\n",
    "> Model isterse attentionâ€™Ä± **zamanla aÃ§ar**,\n",
    "> isterse **daha kapalÄ±** tutar.\n",
    "\n",
    "Yani `alpha`, **Ã¶ÄŸrenilebilir bir â€œgÃ¼Ã§ dÃ¼ÄŸmesiâ€** gibi Ã§alÄ±ÅŸÄ±r.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”‘ Tek CÃ¼mlelik Ã–zet\n",
    "\n",
    "`alpha` ile maskeyi doÄŸrudan uygulamak yerine, maskeyi **1 ile attn arasÄ±nda karÄ±ÅŸtÄ±rarak**\n",
    "attentionâ€™Ä±n ÅŸiddetini Ã¶ÄŸrenilebilir ve stabil hale getiriyoruz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd8fb5",
   "metadata": {},
   "source": [
    "---\n",
    "----\n",
    "# Final Ã–lÃ§ekleme (`beta`) + Uygulama + Opsiyonel Spatial Gate\n",
    "\n",
    "Bu son blok, Ã¼retilen 2D Ã¶lÃ§eÄŸi (scale) **daha gÃ¼venli hale getirip** giriÅŸe uygular;  \n",
    "istersen de bunun Ã¼stÃ¼ne **ek bir lokal 2D gate** koyar.\n",
    "\n",
    "```python\n",
    "        out = x * scale_hw # her kanalÄ±n her pikseli, kendi Ã¶lÃ§eÄŸiyle Ã§arpÄ±lÄ±yor.                                                                               \n",
    "\n",
    "        if self.use_spatial_gate:# Bu blok, CAâ€™den sonra Ã§Ä±ktÄ±ya ek bir 2D lokal kapÄ± uygular; bazen faydalÄ±, bazen fazla bastÄ±rÄ±p performansÄ± bozabilir.EÄŸer kullanÄ±lacaksa dikkat edilmeli.\n",
    "            gate = torch.sigmoid(self.spatial_gate_pw(self.spatial_gate_dw(out)))                            \n",
    "            gate = 1.0 + self.spatial_gate_beta * (gate - 1.0)                                              \n",
    "            out = out * gate                                                                                 \n",
    "\n",
    "        return out\n",
    "```\n",
    "\n",
    "\n",
    "### 1ï¸âƒ£ `scale_hw = 1.0 + beta * (scale_hw - 1.0)` ne yapÄ±yor?\n",
    "\n",
    "Bu satÄ±r, `scale_hw` deÄŸerlerini **1â€™e doÄŸru Ã§eker**.\n",
    "\n",
    "- `scale_hw` aslÄ±nda H ve W maskelerinden tÃ¼retilen 2D Ã¶lÃ§ek haritasÄ±dÄ±r.\n",
    "- BazÄ± durumlarda bu Ã¶lÃ§ek fazla agresif olabilir (Ã§ok bastÄ±rma / fazla ÅŸiÅŸirme).\n",
    "\n",
    "Bu formÃ¼l ÅŸudur:\n",
    "\n",
    "> â€œScaleâ€™i olduÄŸu gibi kullanma, etkisini `beta` kadar uygula.â€\n",
    "\n",
    "#### Betaâ€™nÄ±n etkisi:\n",
    "- **beta = 0**  \n",
    "  â†’ `scale_hw = 1`  \n",
    "  â†’ tamamen identity (hiÃ§ attention yok gibi)\n",
    "\n",
    "- **beta = 1**  \n",
    "  â†’ `scale_hw` olduÄŸu gibi kullanÄ±lÄ±r (tam etki)\n",
    "\n",
    "- **0 < beta < 1**  \n",
    "  â†’ scale 1â€™e yaklaÅŸÄ±r  \n",
    "  â†’ bastÄ±rma/ÅŸiÅŸirme yumuÅŸar, stabilite artar\n",
    "\n",
    "Bunu pratikte ÅŸÃ¶yle dÃ¼ÅŸÃ¼n:\n",
    "> `beta` = attentionâ€™Ä±n â€œgenel sesini kÄ±sma dÃ¼ÄŸmesiâ€.\n",
    "\n",
    "### 2ï¸âƒ£ `out = x * scale_hw` ne demek?\n",
    "\n",
    "Bu, attentionâ€™Ä±n uygulandÄ±ÄŸÄ± yer:\n",
    "\n",
    "- `x`: orijinal feature map `(B, C, H, W)`\n",
    "- `scale_hw`: her kanal ve her piksel iÃ§in Ã¶lÃ§ek `(B, C, H, W)`\n",
    "\n",
    "Ã‡arpÄ±m ÅŸu anlama gelir:\n",
    "\n",
    "> â€œHer kanalÄ±n her konumu, kendi Ã¶lÃ§eÄŸi kadar bÃ¼yÃ¼tÃ¼lÃ¼r veya kÃ¼Ã§Ã¼ltÃ¼lÃ¼r.â€\n",
    "\n",
    "Bu noktada attention artÄ±k **tamamen uygulanmÄ±ÅŸ** olur.\n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ Opsiyonel: `use_spatial_gate` bloÄŸu ne yapar?\n",
    "\n",
    "Bu kÄ±sÄ±m aÃ§Ä±lÄ±rsa (True):\n",
    "\n",
    "> CAâ€™den Ã§Ä±kmÄ±ÅŸ `out` Ã¼zerinde **ek bir 2D lokal kapÄ± (gate)** Ã¼retilir ve tekrar Ã§arpÄ±lÄ±r.\n",
    "\n",
    "Bu gate Ã¼retimi:\n",
    "\n",
    "- `spatial_gate_dw`: 3Ã—3 depthwise  \n",
    "  â†’ lokal 2D komÅŸuluÄŸu gÃ¶rÃ¼r (H,W), kanal kanal iÅŸler\n",
    "- `spatial_gate_pw`: 1Ã—1  \n",
    "  â†’ kanal bazÄ±nda karÄ±ÅŸtÄ±rma / ayarlama\n",
    "- `sigmoid`: gateâ€™i 0â€“1 aralÄ±ÄŸÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±r\n",
    "\n",
    "Sonra gate yine `spatial_gate_beta` ile 1â€™e Ã§ekilerek yumuÅŸatÄ±lÄ±r:\n",
    "\n",
    "> â€œGate Ã§ok agresif olmasÄ±n, etkisini kontrollÃ¼ uygula.â€\n",
    "\n",
    "Son adÄ±m:\n",
    "- `out = out * gate`  \n",
    "  â†’ CAâ€™den geÃ§en Ã§Ä±ktÄ± bir kez daha â€œlokal 2D kontrolâ€ ile dÃ¼zenlenmiÅŸ olur\n",
    "\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ Neden â€œriskliâ€ diyorsun?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼ iki kez Ã¶lÃ§ekleme yapmÄ±ÅŸ oluyorsun:\n",
    "\n",
    "1) CA Ã¶lÃ§eÄŸi ile `x * scale_hw`\n",
    "2) Spatial gate ile `out * gate`\n",
    "\n",
    "Bu bazÄ± senaryolarda:\n",
    "- aÅŸÄ±rÄ± bastÄ±rma (over-suppression)\n",
    "- zayÄ±f gradient akÄ±ÅŸÄ±\n",
    "- detection + kÃ¼Ã§Ã¼k batchâ€™te dalgalanma\n",
    "gibi sorunlar Ã§Ä±karabilir.\n",
    "\n",
    "Bu yÃ¼zden:\n",
    "- Default kapalÄ± olmasÄ± mantÄ±klÄ±\n",
    "- AÃ§acaksan `spatial_gate_beta` genelde kÃ¼Ã§Ã¼k tutulmalÄ±\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”‘ Tek CÃ¼mlelik Ã–zet\n",
    "\n",
    "Ã–nce `beta` ile CA Ã¶lÃ§eÄŸini 1â€™e yaklaÅŸtÄ±rÄ±p agresifliÄŸi dÃ¼ÅŸÃ¼rÃ¼yoruz,  \n",
    "sonra feature mapâ€™e uyguluyoruz; opsiyonel spatial gate aÃ§Ä±ksa, Ã§Ä±ktÄ±ya bir de lokal 2D kapÄ± ekleyip ikinci bir kontrollÃ¼ Ã¶lÃ§ekleme yapÄ±yoruz.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
