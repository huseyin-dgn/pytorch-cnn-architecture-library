{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f3b6c4",
   "metadata": {},
   "source": [
    "\n",
    "# Bu soruya cevap arÄ±yoruz: Element-wise toplama (âŠ•) nasÄ±l gerÃ§ekleÅŸir? Nerelerde kullanÄ±lÄ±r?\n",
    "\n",
    "Bu notebookâ€™ta â€œelement-wise toplamaâ€yÄ± (eleman bazlÄ± toplama) **matematiksel olarak** ve **PyTorch pratikleriyle** netleÅŸtiriyoruz.\n",
    "\n",
    "Sorular:\n",
    "1) Element-wise toplama tam olarak ne demek?  \n",
    "2) TensÃ¶r boyutlarÄ± uymuyorsa ne olur? (shape/broadcast)  \n",
    "3) Derin Ã¶ÄŸrenmede nerelerde kullanÄ±lÄ±r? (Residual/skip, FPN/PAN, attention fusion, loss vb.)  \n",
    "4) Neden Ã¶zellikle residual aÄŸlarda kritik? (gradient akÄ±ÅŸÄ±)  \n",
    "5) Element-wise toplama ile concatenation arasÄ±ndaki fark nedir?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f7b92",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1) TanÄ±m: Element-wise toplama nedir?\n",
    "\n",
    "Ä°ki tensÃ¶rÃ¼n **aynÄ± konumdaki** elemanlarÄ±nÄ± toplamak demektir.\n",
    "\n",
    "EÄŸer \\(A\\) ve \\(B\\) aynÄ± ÅŸekle sahipse:\n",
    "\n",
    "\\[\n",
    "C = A \\oplus B,\\quad C_{i} = A_{i} + B_{i}\n",
    "\\]\n",
    "\n",
    "CNN aktivasyonu iÃ§in tipik ÅŸekil:\n",
    "- \\(A, B \\in \\mathbb{R}^{N \\times C \\times H \\times W}\\)\n",
    "\n",
    "Bu durumda:\n",
    "\n",
    "\\[\n",
    "C[n,c,h,w] = A[n,c,h,w] + B[n,c,h,w]\n",
    "\\]\n",
    "\n",
    "ğŸ“Œ Kritik nokta:\n",
    "> Element-wise toplama **shape (boyut) uyumu** ister (ya da broadcast edilebilir olmalÄ±).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca8b1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: torch.Size([2, 8, 16, 16]) B: torch.Size([2, 8, 16, 16]) C: torch.Size([2, 8, 16, 16])\n",
      "max|C-(A+B)|: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "A = torch.randn(2, 8, 16, 16)\n",
    "B = torch.randn(2, 8, 16, 16)\n",
    "\n",
    "C = A + B\n",
    "print(\"A:\", A.shape, \"B:\", B.shape, \"C:\", C.shape)\n",
    "print(\"max|C-(A+B)|:\", (C - (A+B)).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81da700",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2) Shape uyumu: Ne zaman Ã§alÄ±ÅŸÄ±r, ne zaman patlar?\n",
    "\n",
    "### 2.1) Tam eÅŸleÅŸme (en gÃ¼venlisi)\n",
    "- (N,C,H,W) bire bir aynÄ± â†’ sorunsuz\n",
    "\n",
    "### 2.2) Broadcast ile toplama (PyTorch/Numpy)\n",
    "BazÄ± boyutlar 1 ise otomatik geniÅŸletilebilir:\n",
    "\n",
    "Ã–rn:\n",
    "- \\(A\\): (N,C,H,W)\n",
    "- \\(b\\): (1,C,1,1)\n",
    "\n",
    "\\[\n",
    "C = A + b\n",
    "\\]\n",
    "\n",
    "Bu, kanal baÅŸÄ±na bias/offset gibi davranÄ±r.\n",
    "\n",
    "### 2.3) Ã‡alÄ±ÅŸmayacak durum\n",
    "- H/W farklÄ±ysa (Ã¶r. 16Ã—16 ile 8Ã—8) â†’ doÄŸrudan toplanmaz\n",
    "- C farklÄ±ysa â†’ doÄŸrudan toplanmaz\n",
    "\n",
    "Bu durumda:\n",
    "- ya **resize/upsample/downsample** yaparsÄ±n,\n",
    "- ya **1Ã—1 conv** ile kanal eÅŸlersin,\n",
    "- ya da **concat** + conv kullanÄ±rsÄ±n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0e9fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: torch.Size([2, 8, 16, 16]) b: torch.Size([1, 8, 1, 1]) C: torch.Size([2, 8, 16, 16])\n",
      "Beklenen hata: RuntimeError - The size of tensor a (16) must match the size of tensor b (8) at non-singleton dimension 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Broadcast Ã¶rneÄŸi: (N,C,H,W) + (1,C,1,1)\n",
    "\n",
    "A = torch.randn(2, 8, 16, 16)\n",
    "b = torch.randn(1, 8, 1, 1)\n",
    "\n",
    "C = A + b\n",
    "print(\"A:\", A.shape, \"b:\", b.shape, \"C:\", C.shape)\n",
    "\n",
    "# YanlÄ±ÅŸ shape Ã¶rneÄŸi (hata beklenir)\n",
    "try:\n",
    "    B_bad = torch.randn(2, 8, 8, 8)\n",
    "    _ = A + B_bad\n",
    "except Exception as e:\n",
    "    print(\"Beklenen hata:\", type(e).__name__, \"-\", str(e).split('\\n')[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cddcc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3) Nerelerde kullanÄ±lÄ±r?\n",
    "\n",
    "Element-wise toplama derin Ã¶ÄŸrenmede Ã§ok â€œomurgaâ€ bir operasyondur. En tipik kullanÄ±m alanlarÄ±:\n",
    "\n",
    "### 3.1) Residual baÄŸlantÄ±lar (ResNet)\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "- \\(x\\): skip (identity) yolu\n",
    "- \\(F(x)\\): conv/attention vb. ile iÅŸlenmiÅŸ yol\n",
    "\n",
    "Bu, derin aÄŸlarda gradient akÄ±ÅŸÄ±nÄ± iyileÅŸtirir.\n",
    "\n",
    "### 3.2) U-Net / skip connections\n",
    "Encoderâ€™dan gelen feature ile decoder featureâ€™Ä±:\n",
    "- ya concat edilir\n",
    "- ya da element-wise toplanÄ±r (Ã¶zellikle kanal eÅŸleÅŸiyorsa)\n",
    "\n",
    "### 3.3) FPN / PAN / Neck fusion (YOLO dahil)\n",
    "FarklÄ± Ã¶lÃ§ek featureâ€™lar birleÅŸtirilirken:\n",
    "- upsample + element-wise add (klasik FPN)\n",
    "- concat + conv (PAN/PAFPNâ€™de sÄ±k)\n",
    "\n",
    "FPNâ€™nin klasik formu:\n",
    "\\[\n",
    "P_l = Conv( C_l + Upsample(P_{l+1}) )\n",
    "\\]\n",
    "\n",
    "### 3.4) Attentionâ€™da â€œresidual attentionâ€\n",
    "Attention Ã§Ä±ktÄ±sÄ± doÄŸrudan featureâ€™a eklenebilir:\n",
    "\\[\n",
    "y = x + Attn(x)\n",
    "\\]\n",
    "\n",
    "### 3.5) Loss toplama (Ã§ok gÃ¶revli)\n",
    "Toplam kayÄ±p:\n",
    "\\[\n",
    "\\mathcal{L} = \\mathcal{L}_{cls} + \\lambda \\mathcal{L}_{box} + \\dots\n",
    "\\]\n",
    "Bu da â€œelement-wiseâ€ deÄŸil ama aynÄ± mantÄ±ÄŸÄ±n scalar versiyonudur: farklÄ± terimleri toplama.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5feb9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4) Residual toplama neden bu kadar Ã¶nemli? (Gradient sezgisi)\n",
    "\n",
    "Residual blok:\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "Geri yayÄ±lÄ±mda:\n",
    "\\[\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x} =\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y}\n",
    "\\left( I + \\frac{\\partial F(x)}{\\partial x} \\right)\n",
    "\\]\n",
    "\n",
    "Buradaki kritik ÅŸey **I (identity)** terimi.\n",
    "\n",
    "Bu ne saÄŸlar?\n",
    "- Gradientâ€™in â€œtamamen kaybolmasÄ±nÄ±â€ engellemeye yardÄ±mcÄ± olur\n",
    "- Ã‡ok derin aÄŸlarÄ±n eÄŸitilebilirliÄŸini artÄ±rÄ±r\n",
    "\n",
    "ğŸ“Œ Bu yÃ¼zden element-wise add:\n",
    "> â€œSadece basit bir iÅŸlemâ€ deÄŸil, derin aÄŸlarÄ±n eÄŸitim dinamiÄŸini deÄŸiÅŸtiren temel mekanizmadÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c38b7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 5) Element-wise toplama vs Concatenation (concat)\n",
    "\n",
    "Ä°kisi sÄ±k karÄ±ÅŸtÄ±rÄ±lÄ±r.\n",
    "\n",
    "### 5.1) Element-wise toplama\n",
    "- Åekiller uyumlu olmalÄ±\n",
    "- Ã‡Ä±kÄ±ÅŸ kanalÄ± sayÄ±sÄ± **aynÄ± kalÄ±r** (C deÄŸiÅŸmez)\n",
    "- Bilgi â€œÃ¼st Ã¼ste bindirilirâ€ (superposition)\n",
    "\n",
    "### 5.2) Concatenation (torch.cat)\n",
    "- Kanallardan birleÅŸtirirsin:\n",
    "  - (N,C,H,W) + (N,C,H,W) â†’ (N,2C,H,W)\n",
    "- Kanal sayÄ±sÄ± artar â†’ sonra genelde 1Ã—1 veya 3Ã—3 conv ile karÄ±ÅŸtÄ±rÄ±lÄ±r\n",
    "- Daha fazla kapasite ama daha fazla compute\n",
    "\n",
    "Pratik kural:\n",
    "- Hafif ve stabil: **add**\n",
    "- Daha fazla temsil gÃ¼cÃ¼: **concat + conv**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137e7ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_add: torch.Size([2, 8, 16, 16])\n",
      "y_cat: torch.Size([2, 16, 16, 16])\n",
      "y_mix: torch.Size([2, 8, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add vs Concat demo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x1 = torch.randn(2, 8, 16, 16)\n",
    "x2 = torch.randn(2, 8, 16, 16)\n",
    "\n",
    "y_add = x1 + x2\n",
    "y_cat = torch.cat([x1, x2], dim=1)  # kanal dim=1\n",
    "\n",
    "print(\"y_add:\", y_add.shape)\n",
    "print(\"y_cat:\", y_cat.shape)\n",
    "\n",
    "# concat sonrasÄ± karÄ±ÅŸtÄ±rma iÃ§in 1x1 conv\n",
    "mix = nn.Conv2d(16, 8, kernel_size=1)\n",
    "y_mix = mix(y_cat)\n",
    "print(\"y_mix:\", y_mix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7471b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6) CNN/YOLO pratiklerinde â€œshape eÅŸleÅŸtirmeâ€ nasÄ±l yapÄ±lÄ±r?\n",
    "\n",
    "Element-wise add yapmak istiyorsan iki koÅŸul var:\n",
    "- **H,W aynÄ±** olacak\n",
    "- **C aynÄ±** olacak\n",
    "\n",
    "EÄŸer farklÄ±ysa:\n",
    "\n",
    "### 6.1) H,W farklÄ±ysa\n",
    "- upsample (nearest/bilinear)\n",
    "- downsample (stride conv / pooling)\n",
    "\n",
    "### 6.2) C farklÄ±ysa\n",
    "- 1Ã—1 conv ile kanal projeksiyonu (projection)\n",
    "- veya linear layer (sequence modellerde)\n",
    "\n",
    "Bu, ResNetâ€™te â€œprojection shortcutâ€ olarak geÃ§er:\n",
    "\\[\n",
    "y = W_s x + F(x)\n",
    "\\]\n",
    "burada \\(W_s\\) genelde 1Ã—1 convâ€™dur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78b903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 8, 16, 16]) Fx: torch.Size([2, 16, 16, 16]) proj(x): torch.Size([2, 16, 16, 16]) y: torch.Size([2, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Projection shortcut demo: C farklÄ±ysa 1x1 conv ile eÅŸleÅŸtir\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn(2, 8, 16, 16)\n",
    "Fx = torch.randn(2, 16, 16, 16)   # F(x) kanal sayÄ±sÄ± farklÄ±\n",
    "\n",
    "proj = nn.Conv2d(8, 16, kernel_size=1, bias=False)\n",
    "y = proj(x) + Fx\n",
    "\n",
    "print(\"x:\", x.shape, \"Fx:\", Fx.shape, \"proj(x):\", proj(x).shape, \"y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d580f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 7) KapanÄ±ÅŸ: Net cevap\n",
    "\n",
    "- **Element-wise toplama**, iki tensÃ¶rÃ¼n aynÄ± indeksli elemanlarÄ±nÄ± toplar.\n",
    "- Pratikte ya **shape eÅŸitliÄŸi** ister ya da broadcast ile Ã§alÄ±ÅŸÄ±r.\n",
    "- En yaygÄ±n kullanÄ±m alanlarÄ±:\n",
    "  - Residual baÄŸlantÄ±lar (ResNet, modern backboneâ€™lar)\n",
    "  - U-Net / skip baÄŸlantÄ±larÄ±\n",
    "  - FPN/PAN gibi neck fusion yapÄ±larÄ± (YOLO dahil)\n",
    "  - Attention Ã§Ä±ktÄ±sÄ±nÄ±n residual eklenmesi\n",
    "- Neden Ã¶nemli? Ã‡Ã¼nkÃ¼ Ã¶zellikle residual yapÄ±da **gradient akÄ±ÅŸÄ±nÄ±** iyileÅŸtirir.\n",
    "- Add vs concat:\n",
    "  - add: hafif, kanal sabit\n",
    "  - concat: kanal artar, sonra conv ile karÄ±ÅŸtÄ±rÄ±lÄ±r (daha pahalÄ± ama kapasite yÃ¼ksek)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
