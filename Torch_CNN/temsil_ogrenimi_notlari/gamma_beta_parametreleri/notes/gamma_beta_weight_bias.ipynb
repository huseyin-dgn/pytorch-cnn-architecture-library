{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b2a078",
   "metadata": {},
   "source": [
    "\n",
    "# Bu soruya cevap arÄ±yoruz: **Î³ (gamma) ve Î² (beta)** neden var? (weight/bias ile iliÅŸkisi)\n",
    "\n",
    "GÃ¶rÃ¼ntÃ¼ iÅŸlemede ve genel derin Ã¶ÄŸrenmede sÄ±k gÃ¶rdÃ¼ÄŸÃ¼n iki tÃ¼r â€œayarlamaâ€ var:\n",
    "\n",
    "- **Norm katmanlarÄ±nda:** Ã¶ÄŸrenilebilir **Î³ (scale)** ve **Î² (shift)**\n",
    "- **Lineer/Conv katmanlarÄ±nda:** Ã¶ÄŸrenilebilir **weight (W)** ve **bias (b)**\n",
    "\n",
    "Bu notebook ÅŸunlarÄ± netleÅŸtirir:\n",
    "\n",
    "1) Î³/Î² tam olarak ne yapar?  \n",
    "2) Neden â€œnormalize edip sonra tekrar scale/shiftâ€ mantÄ±klÄ±?  \n",
    "3) Hangi problemi Ã§Ã¶zer?  \n",
    "4) Î³/Î² yoksa ne bozulur?  \n",
    "5) Î³/Î² ile W/bias arasÄ±ndaki iliÅŸki nedir?  \n",
    "6) CV pratiklerinde (BN/GN/LN, residual, attention) nerede kritikleÅŸir?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99c186",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1) En temel tanÄ±m: affine dÃ¶nÃ¼ÅŸÃ¼m\n",
    "\n",
    "Bir tensÃ¶re uygulanan en basit Ã¶ÄŸrenilebilir ayar:\n",
    "\n",
    "\\[\n",
    "y = a x + b\n",
    "\\]\n",
    "\n",
    "- \\(a\\): **scale** (Ã¶lÃ§ekleme)\n",
    "- \\(b\\): **shift** (kaydÄ±rma)\n",
    "\n",
    "Norm katmanlarÄ±ndaki Î³ ve Î² tam olarak budur:\n",
    "\n",
    "\\[\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "\\]\n",
    "\n",
    "Burada \\(\\hat{x}\\) normalize edilmiÅŸ aktivasyondur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a390b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2) â€œNormalize et â†’ sonra Î³/Î² ile geri koyâ€ neden var?\n",
    "\n",
    "Normalizasyon (Ã¶rn BatchNorm) ÅŸu iÅŸi yapar:\n",
    "\n",
    "\\[\n",
    "\\hat{x} = \\frac{x-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}\n",
    "\\]\n",
    "\n",
    "Bu iÅŸlem:\n",
    "- Ã¶lÃ§ek farklarÄ±nÄ± bastÄ±rÄ±r\n",
    "- daÄŸÄ±lÄ±mÄ± stabilize eder\n",
    "- optimizasyonu kolaylaÅŸtÄ±rÄ±r\n",
    "\n",
    "Ama tek baÅŸÄ±na ÅŸunu da yapar:\n",
    "- aÄŸÄ±n â€œistediÄŸiâ€ Ã¶lÃ§ek/ortalama bilgisini **zorla siler**\n",
    "\n",
    "Ä°ÅŸte Î³/Î² burada devreye girer:\n",
    "\n",
    "- **Î³**: aÄŸÄ±n istediÄŸi **varyansÄ±/Ã¶lÃ§eÄŸi** geri koymasÄ±na izin verir\n",
    "- **Î²**: aÄŸÄ±n istediÄŸi **ortalama/ofseti** geri koymasÄ±na izin verir\n",
    "\n",
    "ğŸ“Œ Kilit fikir:\n",
    "> Normalizasyon â€œoptimizasyon iÃ§inâ€ yapÄ±lÄ±r.  \n",
    "> Î³/Î² ise â€œtemsil gÃ¼cÃ¼ kaybÄ± olmasÄ±nâ€ diye eklenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0100a1a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3) Hangi problemi Ã§Ã¶zer?\n",
    "\n",
    "### 3.1) Temsil gÃ¼cÃ¼ kaybÄ±nÄ± engeller\n",
    "\n",
    "EÄŸer sadece normalize edersen, her kanal/feature zorla:\n",
    "- ortalama â‰ˆ 0\n",
    "- std â‰ˆ 1\n",
    "\n",
    "kalÄ±r.\n",
    "\n",
    "Bu durumda aÄŸÄ±n bazÄ± katmanlarda â€œbelli bir Ã¶lÃ§ek/offsetâ€ istemesi mÃ¼mkÃ¼n olabilir.\n",
    "Ã–rneÄŸin:\n",
    "- ReLU eÅŸiÄŸi\n",
    "- residual branch Ã¶lÃ§ek dengesi\n",
    "- attention gating iÃ§in uygun logit Ã¶lÃ§ekleri\n",
    "\n",
    "Î³/Î² **bu Ã¶zgÃ¼rlÃ¼ÄŸÃ¼ geri verir.**\n",
    "\n",
    "### 3.2) â€œNorm koyunca aÄŸ bozulmasÄ±nâ€ garantisi (identity mapping kapasitesi)\n",
    "\n",
    "Ã‡ok kritik: BN + Î³/Î² ile aÄŸ isterse:\n",
    "\n",
    "- \\(\\gamma=1, \\beta=0\\) seÃ§erek â€œstandart normalizeâ€ davranÄ±r\n",
    "- ya da \\(\\gamma,\\beta\\) Ã¶ÄŸrenerek belirli kanallarÄ± bÃ¼yÃ¼tÃ¼p kÃ¼Ã§Ã¼ltÃ¼r\n",
    "\n",
    "BazÄ± durumlarda Î³/Î², norm katmanÄ±nÄ±n â€œzararâ€ verme riskini azaltÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64365613",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4) Hangi sorunun olmamasÄ±na olanak tanÄ±r?\n",
    "\n",
    "### 4.1) â€œNormalize ettiÄŸim iÃ§in bilgi kaybettimâ€ sorunu\n",
    "\n",
    "Sadece normalize etsen:\n",
    "- Ã¶lÃ§ek ve ofset bilgisi â€œsabitlenmiÅŸâ€ olur\n",
    "- modelin ifade gÃ¼cÃ¼ sÄ±nÄ±rlanÄ±r\n",
    "\n",
    "Î³/Î² ile:\n",
    "- normalizasyonun getirdiÄŸi stabilite korunur\n",
    "- ama model â€œgerekenâ€ Ã¶lÃ§ek/ofseti geri Ã¶ÄŸrenir\n",
    "\n",
    "### 4.2) Aktivasyon fonksiyonlarÄ±yla uyum (Ã¶zellikle ReLU)\n",
    "\n",
    "ReLUâ€™da negatifler sÄ±fÄ±rlanÄ±r.\n",
    "EÄŸer Î² ile daÄŸÄ±lÄ±mÄ± kaydÄ±rmazsan bazÄ± kanallar gereksiz yere â€œÃ¶lÃ¼â€ kalabilir.\n",
    "Î², aktivasyonlarÄ±n eÅŸik etrafÄ±nda daha iyi konumlanmasÄ±na yardÄ±m eder.\n",
    "\n",
    "### 4.3) Residual toplama dengesi\n",
    "\n",
    "Residual blokta:\n",
    "\\[\n",
    "y = x + F(x)\n",
    "\\]\n",
    "\n",
    "F(x) dalÄ±nÄ±n Ã¶lÃ§eÄŸi xâ€™e gÃ¶re Ã§ok bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k olursa:\n",
    "- optimizasyon zorlaÅŸÄ±r\n",
    "- eÄŸitim dengesizleÅŸir\n",
    "\n",
    "Norm iÃ§indeki Î³, bu dalÄ±n Ã¶lÃ§eÄŸini ayarlamada kritik rol oynar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99767eb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 5) Î³/Î² ile weight/bias (W,b) arasÄ±ndaki iliÅŸki\n",
    "\n",
    "### 5.1) Conv/Linear:\n",
    "\\[\n",
    "y = Wx + b\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "- W: yÃ¶n/karÄ±ÅŸÄ±m Ã¶ÄŸrenir (feature mixing)\n",
    "- b: sabit ofset\n",
    "\n",
    "### 5.2) Norm + affine:\n",
    "\\[\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "- Î³: her kanal/feature iÃ§in ayrÄ± Ã¶lÃ§ek (diagonal scaling gibi)\n",
    "- Î²: her kanal/feature iÃ§in ofset\n",
    "\n",
    "Ã–nemli fark:\n",
    "- W â€œkanallar arasÄ± karÄ±ÅŸÄ±mâ€ yapabilir.\n",
    "- Î³ ise genelde â€œkanal baÅŸÄ±naâ€ Ã¶lÃ§ekler (daha sÄ±nÄ±rlÄ± ama stabil).\n",
    "\n",
    "ğŸ“Œ Pratik sonuÃ§:\n",
    "> Î³/Î², norm katmanÄ±nÄ± â€œÃ¶ÄŸrenilebilir ve zarar vermeyenâ€ hale getirir;  \n",
    "> W/b ise asÄ±l dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (feature karÄ±ÅŸÄ±mÄ±nÄ±) Ã¶ÄŸrenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e82ad8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6) BNâ€™de bias neden bazen kapatÄ±lÄ±r?\n",
    "\n",
    "SÄ±k gÃ¶rÃ¼len pratik:\n",
    "\n",
    "- Conv bias = False\n",
    "- Sonra BatchNorm gelir\n",
    "\n",
    "Neden?\n",
    "\n",
    "BatchNorm zaten Î² ile ofset eklediÄŸi iÃ§in, conv bias Ã§oÄŸu zaman redundant olur.\n",
    "AyrÄ±ca BN, conv Ã§Ä±kÄ±ÅŸÄ±nÄ± normalize ederken bias etkisini â€œabsorbeâ€ eder.\n",
    "\n",
    "Bu yÃ¼zden:\n",
    "> Conv bias + BN Î² aynÄ± iÅŸin iki kopyasÄ± gibi davranabilir.\n",
    "\n",
    "(Bu %100 her senaryoda deÄŸil ama genel pratik bÃ¶yle.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636022dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x mean/std: 0.8841065071429416 1.022668337083785\n",
      "x+b mean/std: 2.884106507142941 1.022668337083785\n",
      "normalize(x+b) mean/std: 4.718447854656915e-16 0.9999902217543507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basit bir gÃ¶sterim: BN'nin affine parametreleri conv bias'Ä± Ã§oÄŸu zaman gereksiz kÄ±lar.\n",
    "# Bu hÃ¼cre teorik: sayÄ±sal Ã¶rnekle \"bias + normalize\" etkisini gÃ¶sterir.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(8, 1)          # kÃ¼Ã§Ã¼k batch\n",
    "b = 2.0                            # conv bias gibi dÃ¼ÅŸÃ¼n\n",
    "x_b = x + b\n",
    "\n",
    "mu = x_b.mean()\n",
    "std = x_b.std() + 1e-5\n",
    "x_hat = (x_b - mu) / std\n",
    "\n",
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "y = gamma * x_hat + beta\n",
    "\n",
    "print(\"x mean/std:\", x.mean(), x.std())\n",
    "print(\"x+b mean/std:\", x_b.mean(), x_b.std())\n",
    "print(\"normalize(x+b) mean/std:\", y.mean(), y.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e94c5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 7) CVâ€™de Î³/Î² ne zaman â€œÃ¶zellikleâ€ Ã¶nem kazanÄ±r?\n",
    "\n",
    "### 7.1) Small batch / GN/LN kullanÄ±mÄ±\n",
    "BatchNorm yerine GN/LN kullanÄ±rken de affine (Î³/Î²) kritik; Ã§Ã¼nkÃ¼ norm â€œsabit bir standardizasyonâ€ gibi kalmasÄ±n.\n",
    "\n",
    "### 7.2) Attention gating logit Ã¶lÃ§eÄŸi\n",
    "Sigmoid/softmax Ã¶ncesi logit Ã¶lÃ§eÄŸi Ã§ok kÃ¼Ã§Ã¼k/bÃ¼yÃ¼k olursa saturasyon olur.\n",
    "Î³, bu Ã¶lÃ§eÄŸi ayarlamaya yardÄ±m eder.\n",
    "\n",
    "### 7.3) Residual branch stabilizasyonu\n",
    "BazÄ± mimarilerde Î³ baÅŸlangÄ±Ã§ta kÃ¼Ã§Ã¼k ayarlanÄ±r (Ã¶r. bazÄ± residual tasarÄ±mlarda son BN Î³=0 baÅŸlatma) â†’ baÅŸlangÄ±Ã§ta blok â€œidentityâ€ye yakÄ±n davranÄ±r â†’ stabil eÄŸitim.\n",
    "\n",
    "Bu, â€œÎ³ sadece Ã¶lÃ§ekâ€ deÄŸil, aynÄ± zamanda **eÄŸitim dinamiÄŸi kontrol knobâ€™u** demek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f952aa",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 8) KapanÄ±ÅŸ: net cevap\n",
    "\n",
    "- Normalizasyon, aktivasyonlarÄ± standardize ederek optimizasyonu kolaylaÅŸtÄ±rÄ±r.\n",
    "- Ama bu standardizasyon, aÄŸÄ±n istediÄŸi Ã¶lÃ§ek/ofseti zorla sabitleyebilir.\n",
    "- **Î³ (gamma) ve Î² (beta)** bu kaybÄ± telafi eder:\n",
    "  - Î³: Ã¶lÃ§eÄŸi geri koyar (scale)\n",
    "  - Î²: ortalamayÄ± geri koyar (shift)\n",
    "- BÃ¶ylece norm katmanÄ±:\n",
    "  - **stabil** (normalize)\n",
    "  - ama **esnek** (Ã¶ÄŸrenilebilir affine)\n",
    "  olur.\n",
    "\n",
    "Yani Î³/Î², normalizasyonun â€œstabiliteâ€ faydasÄ±nÄ± alÄ±rken â€œtemsil gÃ¼cÃ¼ kaybÄ±â€ problemini engeller.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
